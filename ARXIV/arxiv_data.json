[
    {
        "id": "2103.13192",
        "title": "on preference learning based on sequential bayesian optimization with   pairwise comparison",
        "abstract": "user preference learning is generally a hard problem. individual preferences are typically unknown even to users themselves, while the space of choices is infinite. here we study user preference learning from information-theoretic perspective. we model preference learning as a system with two interacting sub-systems, one representing a user with his/her preferences and another one representing an agent that has to learn these preferences. the user with his/her behaviour is modeled by a parametric preference function. to efficiently learn the preferences and reduce search space quickly, we propose the agent that interacts with the user to collect the most informative data for learning. the agent presents two proposals to the user for evaluation, and the user rates them based on his/her preference function. we show that the optimum agent strategy for data collection and preference learning is a result of maximin optimization of the normalized weighted kullback-leibler (kl) divergence between true and agent-assigned predictive user response distributions. the resulting value of kl-divergence, which we also call remaining system uncertainty (rsu), provides an efficient performance metric in the absence of the ground truth. this metric characterises how well the agent can predict user and, thus, the quality of the underlying learned user (preference) model. our proposed agent comprises sequential mechanisms for user model inference and proposal generation. to infer the user model (preference function), bayesian approximate inference is used in the agent. the data collection strategy is to generate proposals, responses to which help resolving uncertainty associated with prediction of the user responses the most. the efficiency of our approach is validated by numerical simulations. also a real-life example of preference learning application is provided.",
        "doi": "",
        "created": "2021-03-24",
        "url": "https://arxiv.org/abs/2103.13192",
        "authors": [
            "tanya ignatenko",
            "kirill kondrashov",
            "marco cox",
            "bert de vries"
        ]
    },
    {
        "id": "2107.02905",
        "title": "an in silico drug repurposing pipeline to identify drugs with the   potential to inhibit sars-cov-2 replication",
        "abstract": "drug repurposing provides an opportunity to redeploy drugs, which ideally are already approved for use in humans, for the treatment of other diseases. for example, the repurposing of dexamethasone and baricitinib has played a crucial role in saving patient lives during the ongoing sars-cov-2 pandemic. there remains a need to expand therapeutic approaches to prevent life-threatening complications in patients with covid-19. using an in silico approach based on structural similarity to drugs already in clinical trials for covid-19, potential drugs were predicted for repurposing. for a subset of identified drugs with different targets to their corresponding covid-19 clinical trial drug, a mechanism of action analysis was applied to establish whether they might have a role in inhibiting the replication of sars-cov-2. of sixty drugs predicted in this study, two with the potential to inhibit sars-cov-2 replication were identified using mechanism of action analysis. triamcinolone is a corticosteroid that is structurally similar to dexamethasone; gallopamil is a calcium channel blocker that is structurally similar to verapamil. in silico approaches indicate possible mechanisms of action for both drugs in inhibiting sars-cov-2 replication. the identification of these drugs as potentially useful for patients with covid-19 who are at a higher risk of developing severe disease supports the use of in silico approaches to facilitate quick and cost-effective drug repurposing. such drugs could expand the number of treatments available to patients who are not protected by vaccination.",
        "doi": "10.1016/j.imu.2023.101387",
        "created": "2021-07-05",
        "url": "https://arxiv.org/abs/2107.02905",
        "authors": [
            "m\u00e9abh macmahon",
            "woochang hwang",
            "soorin yim",
            "eoghan macmahon",
            "alexandre abraham",
            "justin barton",
            "mukunthan tharmakulasingam",
            "paul bilokon",
            "vasanthi priyadarshini gaddi",
            "namshik han"
        ]
    },
    {
        "id": "2112.08297",
        "title": "rethinking influence functions of neural networks in the   over-parameterized regime",
        "abstract": "understanding the black-box prediction for neural networks is challenging. to achieve this, early studies have designed influence function (if) to measure the effect of removing a single training point on neural networks. however, the classic implicit hessian-vector product (ihvp) method for calculating if is fragile, and theoretical analysis of if in the context of neural networks is still lacking. to this end, we utilize the neural tangent kernel (ntk) theory to calculate if for the neural network trained with regularized mean-square loss, and prove that the approximation error can be arbitrarily small when the width is sufficiently large for two-layer relu networks. we analyze the error bound for the classic ihvp method in the over-parameterized regime to understand when and why it fails or not. in detail, our theoretical analysis reveals that (1) the accuracy of ihvp depends on the regularization term, and is pretty low under weak regularization; (2) the accuracy of ihvp has a significant correlation with the probability density of corresponding training points. we further borrow the theory from ntk to understand the ifs better, including quantifying the complexity for influential samples and depicting the variation of ifs during the training dynamics. numerical experiments on real-world data confirm our theoretical results and demonstrate our findings.",
        "doi": "10.1609/aaai.v36i8.20893",
        "created": "2021-12-15",
        "url": "https://arxiv.org/abs/2112.08297",
        "authors": [
            "rui zhang",
            "shihua zhang"
        ]
    },
    {
        "id": "2202.11954",
        "title": "xautoml: a visual analytics tool for understanding and validating   automated machine learning",
        "abstract": "in the last ten years, various automated machine learning (autom ) systems have been proposed to build end-to-end machine learning (ml) pipelines with minimal human interaction. even though such automatically synthesized ml pipelines are able to achieve a competitive performance, recent studies have shown that users do not trust models constructed by automl due to missing transparency of automl systems and missing explanations for the constructed ml pipelines. in a requirements analysis study with 36 domain experts, data scientists, and automl researchers from different professions with vastly different expertise in ml, we collect detailed informational needs for automl. we propose xautoml, an interactive visual analytics tool for explaining arbitrary automl optimization procedures and ml pipelines constructed by automl. xautoml combines interactive visualizations with established techniques from explainable artificial intelligence (xai) to make the complete automl procedure transparent and explainable. by integrating xautoml with jupyterlab, experienced users can extend the visual analytics with ad-hoc visualizations based on information extracted from xautoml. we validate our approach in a user study with the same diverse user group from the requirements analysis. all participants were able to extract useful information from xautoml, leading to a significantly increased understanding of ml pipelines produced by automl and the automl optimization itself.",
        "doi": "10.1145/3625240",
        "created": "2022-02-24",
        "url": "https://arxiv.org/abs/2202.11954",
        "authors": [
            "marc-andr\u00e9 z\u00f6ller",
            "waldemar titov",
            "thomas schlegel",
            "marco f. huber"
        ]
    },
    {
        "id": "2203.02205",
        "title": "evaluating object (mis)detection from a safety and reliability   perspective: discussion and measures",
        "abstract": "we argue that object detectors in the safety critical domain should prioritize detection of objects that are most likely to interfere with the actions of the autonomous actor. especially, this applies to objects that can impact the actor's safety and reliability. to quantify the impact of object (mis)detection on safety and reliability in the context of autonomous driving, we propose new object detection measures that reward the correct identification of objects that are most dangerous and most likely to affect driving decisions. to achieve this, we build an object criticality model to reward the detection of the objects based on proximity, orientation, and relative velocity with respect to the subject vehicle. then, we apply our model on the recent autonomous driving dataset nuscenes, and we compare nine object detectors. results show that, in several settings, object detectors that perform best according to the nuscenes ranking are not the preferable ones when the focus is shifted on safety and reliability.",
        "doi": "10.1109/access.2023.3272979",
        "created": "2022-03-04",
        "url": "https://arxiv.org/abs/2203.02205",
        "authors": [
            "andrea ceccarelli",
            "leonardo montecchi"
        ]
    },
    {
        "id": "2204.10581",
        "title": "fused audio instance and representation for respiratory disease   detection",
        "abstract": "audio-based classification techniques on body sounds have long been studied to aid in the diagnosis of respiratory diseases. while most research is centered on the use of cough as the main biomarker, other body sounds also have the potential to detect respiratory diseases. recent studies on covid-19 have shown that breath and speech sounds, in addition to cough, correlate with the disease. our study proposes fused audio instance and representation (fair) as a method for respiratory disease detection. fair relies on constructing a joint feature vector from various body sounds represented in waveform and spectrogram form. we conducted experiments on the use case of covid-19 detection by combining waveform and spectrogram representation of body sounds. our findings show that the use of self-attention to combine extracted features from cough, breath, and speech sounds leads to the best performance with an area under the receiver operating characteristic curve (auc) score of 0.8658, a sensitivity of 0.8057, and a specificity of 0.7958. compared to models trained solely on spectrograms or waveforms, the use of both representations results in an improved auc score, demonstrating that combining spectrogram and waveform representation helps to enrich the extracted features and outperforms the models that use only one representation.",
        "doi": "",
        "created": "2022-04-22",
        "url": "https://arxiv.org/abs/2204.10581",
        "authors": [
            "tuan truong",
            "matthias lenga",
            "antoine serrurier",
            "sadegh mohammadi"
        ]
    },
    {
        "id": "2205.11952",
        "title": "3d helical ct reconstruction with a memory efficient learned primal-dual   architecture",
        "abstract": "deep learning based computed tomography (ct) reconstruction has demonstrated outstanding performance on simulated 2d low-dose ct data. this applies in particular to domain adapted neural networks, which incorporate a handcrafted physics model for ct imaging. empirical evidence shows that employing such architectures reduces the demand for training data and improves upon generalisation. however, their training requires large computational resources that quickly become prohibitive in 3d helical ct, which is the most common acquisition geometry used for medical imaging. furthermore, clinical data also comes with other challenges not accounted for in simulations, like errors in flux measurement, resolution mismatch and, most importantly, the absence of the real ground truth. the necessity to have a computationally feasible training combined with the need to address these issues has made it difficult to evaluate deep learning based reconstruction on clinical 3d helical ct. this paper modifies a domain adapted neural network architecture, the learned primal-dual (lpd), so that it can be trained and applied to reconstruction in this setting. we achieve this by splitting the helical trajectory into sections and applying the unrolled lpd iterations to those sections sequentially. to the best of our knowledge, this work is the first to apply an unrolled deep learning architecture for reconstruction on full-sized clinical data, like those in the low dose ct image and projection data set (ldct). moreover, training and testing is done on a single gpu card with 24gb of memory.",
        "doi": "",
        "created": "2022-05-24",
        "url": "https://arxiv.org/abs/2205.11952",
        "authors": [
            "jevgenija rudzusika",
            "buda baji\u0107",
            "thomas koehler",
            "ozan \u00f6ktem"
        ]
    },
    {
        "id": "2211.00539",
        "title": "dungeons and data: a large-scale nethack dataset",
        "abstract": "recent breakthroughs in the development of agents to solve challenging sequential decision making problems such as go, starcraft, or dota, have relied on both simulated environments and large-scale datasets. however, progress on this research has been hindered by the scarcity of open-sourced datasets and the prohibitive computational cost to work with them. here we present the nethack learning dataset (nld), a large and highly-scalable dataset of trajectories from the popular game of nethack, which is both extremely challenging for current methods and very fast to run. nld consists of three parts: 10 billion state transitions from 1.5 million human trajectories collected on the nao public nethack server from 2009 to 2020; 3 billion state-action-score transitions from 100,000 trajectories collected from the symbolic bot winner of the nethack challenge 2021; and, accompanying code for users to record, load and stream any collection of such trajectories in a highly compressed form. we evaluate a wide range of existing algorithms including online and offline rl, as well as learning from demonstrations, showing that significant research advances are needed to fully leverage large-scale datasets for challenging sequential decision making tasks.",
        "doi": "",
        "created": "2022-11-01",
        "url": "https://arxiv.org/abs/2211.00539",
        "authors": [
            "eric hambro",
            "roberta raileanu",
            "danielle rothermel",
            "vegard mella",
            "tim rockt\u00e4schel",
            "heinrich k\u00fcttler",
            "naila murray"
        ]
    },
    {
        "id": "2211.03157",
        "title": "examining the differential risk from high-level artificial intelligence   and the question of control",
        "abstract": "artificial intelligence (ai) is one of the most transformative technologies of the 21st century. the extent and scope of future ai capabilities remain a key uncertainty, with widespread disagreement on timelines and potential impacts. as nations and technology companies race toward greater complexity and autonomy in ai systems, there are concerns over the extent of integration and oversight of opaque ai decision processes. this is especially true in the subfield of machine learning (ml), where systems learn to optimize objectives without human assistance. objectives can be imperfectly specified or executed in an unexpected or potentially harmful way. this becomes more concerning as systems increase in power and autonomy, where an abrupt capability jump could result in unexpected shifts in power dynamics or even catastrophic failures. this study presents a hierarchical complex systems framework to model ai risk and provide a template for alternative futures analysis. survey data were collected from domain experts in the public and private sectors to classify ai impact and likelihood. the results show increased uncertainty over the powerful ai agent scenario, confidence in multiagent environments, and increased concern over ai alignment failures and influence-seeking behavior.",
        "doi": "10.1016/j.futures.2023.103182",
        "created": "2022-11-06",
        "url": "https://arxiv.org/abs/2211.03157",
        "authors": [
            "kyle a. kilian",
            "christopher j. ventura",
            "mark m. bailey"
        ]
    },
    {
        "id": "2211.11744",
        "title": "visual dexterity: in-hand reorientation of novel and complex object   shapes",
        "abstract": "in-hand object reorientation is necessary for performing many dexterous manipulation tasks, such as tool use in less structured environments that remain beyond the reach of current robots. prior works built reorientation systems assuming one or many of the following: reorienting only specific objects with simple shapes, limited range of reorientation, slow or quasistatic manipulation, simulation-only results, the need for specialized and costly sensor suites, and other constraints which make the system infeasible for real-world deployment. we present a general object reorientation controller that does not make these assumptions. it uses readings from a single commodity depth camera to dynamically reorient complex and new object shapes by any rotation in real-time, with the median reorientation time being close to seven seconds. the controller is trained using reinforcement learning in simulation and evaluated in the real world on new object shapes not used for training, including the most challenging scenario of reorienting objects held in the air by a downward-facing hand that must counteract gravity during reorientation. our hardware platform only uses open-source components that cost less than five thousand dollars. although we demonstrate the ability to overcome assumptions in prior work, there is ample scope for improving absolute performance. for instance, the challenging duck-shaped object not used for training was dropped in 56 percent of the trials. when it was not dropped, our controller reoriented the object within 0.4 radians (23 degrees) 75 percent of the time. videos are available at: https://taochenshh.github.io/projects/visual-dexterity.",
        "doi": "10.1126/scirobotics.adc9244",
        "created": "2022-11-21",
        "url": "https://arxiv.org/abs/2211.11744",
        "authors": [
            "tao chen",
            "megha tippur",
            "siyang wu",
            "vikash kumar",
            "edward adelson",
            "pulkit agrawal"
        ]
    },
    {
        "id": "2302.10106",
        "title": "towards understanding the survival of patients with high-grade   gastroenteropancreatic neuroendocrine neoplasms: an investigation of ensemble   feature selection in the prediction of overall survival",
        "abstract": "determining the most informative features for predicting the overall survival of patients diagnosed with high-grade gastroenteropancreatic neuroendocrine neoplasms is crucial to improve individual treatment plans for patients, as well as the biological understanding of the disease. recently developed ensemble feature selectors like the repeated elastic net technique for feature selection (rent) and the user-guided bayesian framework for feature selection (ubayfs) allow the user to identify such features in datasets with low sample sizes. while rent is purely data-driven, ubayfs is capable of integrating expert knowledge a priori in the feature selection process. in this work we compare both feature selectors on a dataset comprising of 63 patients and 134 features from multiple sources, including basic patient characteristics, baseline blood values, tumor histology, imaging, and treatment information. our experiments involve data-driven and expert-driven setups, as well as combinations of both. we use findings from clinical literature as a source of expert knowledge. our results demonstrate that both feature selectors allow accurate predictions, and that expert knowledge has a stabilizing effect on the feature set, while the impact on predictive performance is limited. the features who performance status, albumin, platelets, ki-67, tumor morphology, total mtv, total tlg, and suvmax are the most stable and predictive features in our study.",
        "doi": "10.1016/j.cmpb.2023.107934",
        "created": "2023-02-20",
        "url": "https://arxiv.org/abs/2302.10106",
        "authors": [
            "anna jenul",
            "henning langen stokmo",
            "stefan schrunner",
            "mona-elisabeth revheim",
            "geir olav hjortland",
            "oliver tomic"
        ]
    },
    {
        "id": "2303.11809",
        "title": "addressing class variable imbalance in federated semi-supervised   learning",
        "abstract": "federated semi-supervised learning (fssl) combines techniques from both fields of federated and semi-supervised learning to improve the accuracy and performance of models in a distributed environment by using a small fraction of labeled data and a large amount of unlabeled data. without the need to centralize all data in one place for training, it collect updates of model training after devices train models at local, and thus can protect the privacy of user data. however, during the federal training process, some of the devices fail to collect enough data for local training, while new devices will be included to the group training. this leads to an unbalanced global data distribution and thus affect the performance of the global model training. most of the current research is focusing on class imbalance with a fixed number of classes, while little attention is paid to data imbalance with a variable number of classes. therefore, in this paper, we propose federated semi-supervised learning for class variable imbalance (fcvi) to solve class variable imbalance. the class-variable learning algorithm is used to mitigate the data imbalance due to changes of the number of classes. our scheme is proved to be significantly better than baseline methods, while maintaining client privacy.",
        "doi": "10.5121/csit.2023.130522",
        "created": "2023-03-21",
        "url": "https://arxiv.org/abs/2303.11809",
        "authors": [
            "zehui dong",
            "wenjing liu",
            "siyuan liu",
            "xingzhi chen"
        ]
    },
    {
        "id": "2303.18223",
        "title": "a survey of large language models",
        "abstract": "language is essentially a complex, intricate system of human expressions governed by grammatical rules. it poses a significant challenge to develop capable ai algorithms for comprehending and grasping a language. as a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. recently, pre-trained language models (plms) have been proposed by pre-training transformer models over large-scale corpora, showing strong capabilities in solving various nlp tasks. since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. to discriminate the difference in parameter scale, the research community has coined the term large language models (llm) for the plms of significant size. recently, the research on llms has been largely advanced by both academia and industry, and a remarkable progress is the launch of chatgpt, which has attracted widespread attention from society. the technical evolution of llms has been making an important impact on the entire ai community, which would revolutionize the way how we develop and use ai algorithms. in this survey, we review the recent advances of llms by introducing the background, key findings, and mainstream techniques. in particular, we focus on four major aspects of llms, namely pre-training, adaptation tuning, utilization, and capacity evaluation. besides, we also summarize the available resources for developing llms and discuss the remaining issues for future directions.",
        "doi": "",
        "created": "2023-03-31",
        "url": "https://arxiv.org/abs/2303.18223",
        "authors": [
            "wayne xin zhao",
            "kun zhou",
            "junyi li",
            "tianyi tang",
            "xiaolei wang",
            "yupeng hou",
            "yingqian min",
            "beichen zhang",
            "junjie zhang",
            "zican dong",
            "yifan du",
            "chen yang",
            "yushuo chen",
            "zhipeng chen",
            "jinhao jiang",
            "ruiyang ren",
            "yifan li",
            "xinyu tang",
            "zikang liu",
            "peiyu liu",
            "jian-yun nie",
            "ji-rong wen"
        ]
    },
    {
        "id": "2304.05390",
        "title": "hrs-bench: holistic, reliable and scalable benchmark for text-to-image   models",
        "abstract": "in recent years, text-to-image (t2i) models have been extensively studied, especially with the emergence of diffusion models that achieve state-of-the-art results on t2i synthesis tasks. however, existing benchmarks heavily rely on subjective human evaluation, limiting their ability to holistically assess the model's capabilities. furthermore, there is a significant gap between efforts in developing new t2i architectures and those in evaluation. to address this, we introduce hrs-bench, a concrete evaluation benchmark for t2i models that is holistic, reliable, and scalable. unlike existing bench-marks that focus on limited aspects, hrs-bench measures 13 skills that can be categorized into five major categories: accuracy, robustness, generalization, fairness, and bias. in addition, hrs-bench covers 50 scenarios, including fashion, animals, transportation, food, and clothes. we evaluate nine recent large-scale t2i models using metrics that cover a wide range of skills. a human evaluation aligned with 95% of our evaluations on average was conducted to probe the effectiveness of hrs-bench. our experiments demonstrate that existing models often struggle to generate images with the desired count of objects, visual text, or grounded emotions. we hope that our benchmark help ease future text-to-image generation research. the code and data are available at https://eslambakr.github.io/hrsbench.github.io",
        "doi": "",
        "created": "2023-04-11",
        "url": "https://arxiv.org/abs/2304.05390",
        "authors": [
            "eslam mohamed bakr",
            "pengzhan sun",
            "xiaoqian shen",
            "faizan farooq khan",
            "li erran li",
            "mohamed elhoseiny"
        ]
    },
    {
        "id": "2304.07061",
        "title": "autodroid-0shot: a simple baseline for gpt-powered ui-grounded   smartphone task automation in android",
        "abstract": "this paper introduces autodroid-0shot, a tool that utilizes gpt-like large language models (llms) to automate the interactions with android mobile applications. given a natural language description of a desired task, autodroid-0shot can automatically generate and execute actions that navigate the app to complete the task. it works by translating the app gui state information and the available actions on the smartphone screen to natural language prompts and asking the llm to make a choice of actions. since the llm is typically trained on a large amount of data including the how-to manuals of diverse software applications, it has the ability to make reasonable choices of actions based on the provided information. we evaluate autodroid-0shot with a self-created dataset that contains 33 tasks collected from 17 android applications spanning 10 categories. it can successfully complete 39.39% of the tasks, and the average partial completion progress is about 66.76%. given the fact that our method is fully unsupervised (no modification required from both the app and the llm), we believe there is great potential to enhance automation performance with better app development paradigms and/or custom model training.",
        "doi": "",
        "created": "2023-04-14",
        "url": "https://arxiv.org/abs/2304.07061",
        "authors": [
            "hao wen",
            "hongming wang",
            "jiaxuan liu",
            "yuanchun li"
        ]
    },
    {
        "id": "2304.10410",
        "title": "radar-camera fusion for object detection and semantic segmentation in   autonomous driving: a comprehensive review",
        "abstract": "driven by deep learning techniques, perception technology in autonomous driving has developed rapidly in recent years, enabling vehicles to accurately detect and interpret surrounding environment for safe and efficient navigation. to achieve accurate and robust perception capabilities, autonomous vehicles are often equipped with multiple sensors, making sensor fusion a crucial part of the perception system. among these fused sensors, radars and cameras enable a complementary and cost-effective perception of the surrounding environment regardless of lighting and weather conditions. this review aims to provide a comprehensive guideline for radar-camera fusion, particularly concentrating on perception tasks related to object detection and semantic segmentation.based on the principles of the radar and camera sensors, we delve into the data processing process and representations, followed by an in-depth analysis and summary of radar-camera fusion datasets. in the review of methodologies in radar-camera fusion, we address interrogative questions, including \"why to fuse\", \"what to fuse\", \"where to fuse\", \"when to fuse\", and \"how to fuse\", subsequently discussing various challenges and potential research directions within this domain. to ease the retrieval and comparison of datasets and fusion methods, we also provide an interactive website: https://radar-camera-fusion.github.io.",
        "doi": "10.1109/tiv.2023.3307157",
        "created": "2023-04-20",
        "url": "https://arxiv.org/abs/2304.10410",
        "authors": [
            "shanliang yao",
            "runwei guan",
            "xiaoyu huang",
            "zhuoxiao li",
            "xiangyu sha",
            "yong yue",
            "eng gee lim",
            "hyungjoon seo",
            "ka lok man",
            "xiaohui zhu",
            "yutao yue"
        ]
    },
    {
        "id": "2305.01569",
        "title": "pick-a-pic: an open dataset of user preferences for text-to-image   generation",
        "abstract": "the ability to collect a large dataset of human preferences from text-to-image users is usually limited to companies, making such datasets inaccessible to the public. to address this issue, we create a web app that enables text-to-image users to generate images and specify their preferences. using this web app we build pick-a-pic, a large, open dataset of text-to-image prompts and real users' preferences over generated images. we leverage this dataset to train a clip-based scoring function, pickscore, which exhibits superhuman performance on the task of predicting human preferences. then, we test pickscore's ability to perform model evaluation and observe that it correlates better with human rankings than other automatic evaluation metrics. therefore, we recommend using pickscore for evaluating future text-to-image generation models, and using pick-a-pic prompts as a more relevant dataset than ms-coco. finally, we demonstrate how pickscore can enhance existing text-to-image models via ranking.",
        "doi": "",
        "created": "2023-05-02",
        "url": "https://arxiv.org/abs/2305.01569",
        "authors": [
            "yuval kirstain",
            "adam polyak",
            "uriel singer",
            "shahbuland matiana",
            "joe penna",
            "omer levy"
        ]
    },
    {
        "id": "2305.15771",
        "title": "on the planning abilities of large language models : a critical   investigation",
        "abstract": "intrigued by the claims of emergent reasoning capabilities in llms trained on general web corpora, in this paper, we set out to investigate their planning capabilities. we aim to evaluate (1) the effectiveness of llms in generating plans autonomously in commonsense planning tasks and (2) the potential of llms in llm-modulo settings where they act as a source of heuristic guidance for external planners and verifiers. we conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the international planning competition and evaluate llms in two distinct modes: autonomous and heuristic. our findings reveal that llms' ability to generate executable plans autonomously is rather limited, with the best model (gpt-4) having an average success rate of ~12% across the domains. however, the results in the llm-modulo setting show more promise. in the llm-modulo setting, we demonstrate that llm-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the llm for better plan generation.",
        "doi": "",
        "created": "2023-05-25",
        "url": "https://arxiv.org/abs/2305.15771",
        "authors": [
            "karthik valmeekam",
            "matthew marquez",
            "sarath sreedharan",
            "subbarao kambhampati"
        ]
    },
    {
        "id": "2305.19190",
        "title": "inverse approximation theory for nonlinear recurrent neural networks",
        "abstract": "we prove an inverse approximation theorem for the approximation of nonlinear sequence-to-sequence relationships using recurrent neural networks (rnns). this is a so-called bernstein-type result in approximation theory, which deduces properties of a target function under the assumption that it can be effectively approximated by a hypothesis space. in particular, we show that nonlinear sequence relationships that can be stably approximated by nonlinear rnns must have an exponential decaying memory structure - a notion that can be made precise. this extends the previously identified curse of memory in linear rnns into the general nonlinear setting, and quantifies the essential limitations of the rnn architecture for learning sequential relationships with long-term memory. based on the analysis, we propose a principled reparameterization method to overcome the limitations. our theoretical results are confirmed by numerical experiments. the code has been released in https://github.com/radarfudan/curse-of-memory",
        "doi": "",
        "created": "2023-05-30",
        "url": "https://arxiv.org/abs/2305.19190",
        "authors": [
            "shida wang",
            "zhong li",
            "qianxiao li"
        ]
    },
    {
        "id": "2306.10548",
        "title": "marble: music audio representation benchmark for universal evaluation",
        "abstract": "in the era of extensive intersection between art and artificial intelligence (ai), such as image generation and fiction co-creation, ai for music remains relatively nascent, particularly in music understanding. this is evident in the limited work on deep music representations, the scarcity of large-scale datasets, and the absence of a universal and community-driven benchmark. to address this issue, we introduce the music audio representation benchmark for universal evaluation, termed marble. it aims to provide a benchmark for various music information retrieval (mir) tasks by defining a comprehensive taxonomy with four hierarchy levels, including acoustic, performance, score, and high-level description. we then establish a unified protocol based on 14 tasks on 8 public-available datasets, providing a fair and standard assessment of representations of all open-sourced pre-trained models developed on music recordings as baselines. besides, marble offers an easy-to-use, extendable, and reproducible suite for the community, with a clear statement on copyright issues on datasets. results suggest recently proposed large-scale pre-trained musical language models perform the best in most tasks, with room for further improvement. the leaderboard and toolkit repository are published at https://marble-bm.shef.ac.uk to promote future music ai research.",
        "doi": "",
        "created": "2023-06-18",
        "url": "https://arxiv.org/abs/2306.10548",
        "authors": [
            "ruibin yuan",
            "yinghao ma",
            "yizhi li",
            "ge zhang",
            "xingran chen",
            "hanzhi yin",
            "le zhuo",
            "yiqi liu",
            "jiawen huang",
            "zeyue tian",
            "binyue deng",
            "ningzhi wang",
            "chenghua lin",
            "emmanouil benetos",
            "anton ragni",
            "norbert gyenge",
            "roger dannenberg",
            "wenhu chen",
            "gus xia",
            "wei xue",
            "si liu",
            "shi wang",
            "ruibo liu",
            "yike guo",
            "jie fu"
        ]
    },
    {
        "id": "2306.10841",
        "title": "blockchain-enabled federated learning: a reference architecture design,   implementation, and verification",
        "abstract": "this paper presents a novel reference architecture for blockchain-enabled federated learning (bcfl), a state-of-the-art approach that amalgamates the strengths of federated learning and blockchain technology.we define smart contract functions, stakeholders and their roles, and the use of interplanetary file system (ipfs) as key components of bcfl and conduct a comprehensive analysis. in traditional centralized federated learning, the selection of local nodes and the collection of learning results for each round are merged under the control of a central server. in contrast, in bcfl, all these processes are monitored and managed via smart contracts. additionally, we propose an extension architecture to support both crossdevice and cross-silo federated learning scenarios. furthermore, we implement and verify the architecture in a practical real-world ethereum development environment. our bcfl reference architecture provides significant flexibility and extensibility, accommodating the integration of various additional elements, as per specific requirements and use cases, thereby rendering it an adaptable solution for a wide range of bcfl applications. as a prominent example of extensibility, decentralized identifiers (dids) have been employed as an authentication method to introduce practical utilization within bcfl. this study not only bridges a crucial gap between research and practical deployment but also lays a solid foundation for future explorations in the realm of bcfl. the pivotal contribution of this study is the successful implementation and verification of a realistic bcfl reference architecture. we intend to make the source code publicly accessible shortly, fostering further advancements and adaptations within the community.",
        "doi": "",
        "created": "2023-06-19",
        "url": "https://arxiv.org/abs/2306.10841",
        "authors": [
            "eunsu goh",
            "dae-yeol kim",
            "kwangkee lee",
            "suyeong oh",
            "jong-eui chae",
            "do-yup kim"
        ]
    },
    {
        "id": "2306.13686",
        "title": "broadening the perspective for sustainable ai: comprehensive   sustainability criteria and indicators for ai systems",
        "abstract": "the increased use of ai systems is associated with multi-faceted societal, environmental, and economic consequences. these include non-transparent decision-making processes, discrimination, increasing inequalities, rising energy consumption and greenhouse gas emissions in ai model development and application, and an increasing concentration of economic power. by considering the multi-dimensionality of sustainability, this paper takes steps towards substantiating the call for an overarching perspective on \"sustainable ai\". it presents the scais framework (sustainability criteria and indicators for artificial intelligence systems) which contains a set 19 sustainability criteria for sustainable ai and 67 indicators that is based on the results of a critical review and expert workshops. this interdisciplinary approach contributes a unique holistic perspective to facilitate and structure the discourse on sustainable ai. further, it provides a concrete framework that lays the foundation for developing standards and tools to support the conscious development and application of ai systems.",
        "doi": "",
        "created": "2023-06-22",
        "url": "https://arxiv.org/abs/2306.13686",
        "authors": [
            "friederike rohde",
            "josephin wagner",
            "andreas meyer",
            "philipp reinhard",
            "marcus voss",
            "ulrich petschow",
            "anne mollen"
        ]
    },
    {
        "id": "2307.07871",
        "title": "the socialai school: insights from developmental psychology towards   artificial socio-cultural agents",
        "abstract": "developmental psychologists have long-established the importance of socio-cognitive abilities in human intelligence. these abilities enable us to enter, participate and benefit from human culture. ai research on social interactive agents mostly concerns the emergence of culture in a multi-agent setting (often without a strong grounding in developmental psychology). we argue that ai research should be informed by psychology and study socio-cognitive abilities enabling to enter a culture too. we discuss the theories of michael tomasello and jerome bruner to introduce some of their concepts to ai and outline key concepts and socio-cognitive abilities. we present the socialai school - a tool including a customizable parameterized uite of procedurally generated environments, which simplifies conducting experiments regarding those concepts. we show examples of such experiments with rl agents and large language models. the main motivation of this work is to engage the ai community around the problem of social intelligence informed by developmental psychology, and to provide a tool to simplify first steps in this direction. refer to the project website for code and additional information: https://sites.google.com/view/socialai-school.",
        "doi": "",
        "created": "2023-07-15",
        "url": "https://arxiv.org/abs/2307.07871",
        "authors": [
            "grgur kova\u010d",
            "r\u00e9my portelas",
            "peter ford dominey",
            "pierre-yves oudeyer"
        ]
    },
    {
        "id": "2307.08187",
        "title": "an empirical investigation of pre-trained model selection for   out-of-distribution generalization and calibration",
        "abstract": "in the realm of out-of-distribution (ood) generalization tasks, fine-tuning pre-trained models has become a prevalent strategy. different from most prior work that has focused on advancing learning algorithms, we systematically examined how pre-trained model size, pre-training data scale, and training strategies impact downstream generalization and uncertainty calibration. we evaluated 97 models across diverse pre-trained model sizes, five pre-training datasets, and five data augmentations through extensive experiments on four distribution shift datasets totaling over 100,000 gpu hours. our results demonstrate the significant impact of pre-trained model selection, with optimal choices substantially improving ood accuracy over algorithm improvement alone. we find larger models and bigger pre-training data improve ood performance and calibration, in contrast to some prior studies that found modern deep networks to calibrate worse than classical shallow models. our work underscores the overlooked importance of pre-trained model selection for out-of-distribution generalization and calibration.",
        "doi": "",
        "created": "2023-07-16",
        "url": "https://arxiv.org/abs/2307.08187",
        "authors": [
            "hiroki naganuma",
            "ryuichiro hataya",
            "ioannis mitliagkas"
        ]
    },
    {
        "id": "2307.12777",
        "title": "proceeding of the 1st workshop on social robots personalisation at the   crossroads between engineering and humanities (concatenate)",
        "abstract": "nowadays, robots are expected to interact more physically, cognitively, and socially with people. they should adapt to unpredictable contexts alongside individuals with various behaviours. for this reason, personalisation is a valuable attribute for social robots as it allows them to act according to a specific user's needs and preferences and achieve natural and transparent robot behaviours for humans. if correctly implemented, personalisation could also be the key to the large-scale adoption of social robotics. however, achieving personalisation is arduous as it requires us to expand the boundaries of robotics by taking advantage of the expertise of various domains. indeed, personalised robots need to analyse and model user interactions while considering their involvement in the adaptative process. it also requires us to address ethical and socio-cultural aspects of personalised hri to achieve inclusive and diverse interaction and avoid deception and misplaced trust when interacting with the users. at the same time, policymakers need to ensure regulations in view of possible short-term and long-term adaptive hri. this workshop aims to raise an interdisciplinary discussion on personalisation in robotics. it aims at bringing researchers from different fields together to propose guidelines for personalisation while addressing the following questions: how to define it - how to achieve it - and how it should be guided to fit legal and ethical requirements.",
        "doi": "",
        "created": "2023-07-10",
        "url": "https://arxiv.org/abs/2307.12777",
        "authors": [
            "imene tarakli",
            "georgios angelopoulos",
            "mehdi hellou",
            "camille vindolet",
            "boris abramovic",
            "rocco limongelli",
            "dimitri lacroix",
            "andrea bertolini",
            "silvia rossi",
            "alessandro di nuovo",
            "angelo cangelosi",
            "gordon cheng"
        ]
    },
    {
        "id": "2308.09687",
        "title": "graph of thoughts: solving elaborate problems with large language models",
        "abstract": "we introduce graph of thoughts (got): a framework that advances prompting capabilities in large language models (llms) beyond those offered by paradigms such as chain-of-thought or tree of thoughts (tot). the key idea and primary advantage of got is the ability to model the information generated by an llm as an arbitrary graph, where units of information (\"llm thoughts\") are vertices, and edges correspond to dependencies between these vertices. this approach enables combining arbitrary llm thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. we illustrate that got offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over tot, while simultaneously reducing costs by >31%. we ensure that got is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. this work brings the llm reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.",
        "doi": "",
        "created": "2023-08-18",
        "url": "https://arxiv.org/abs/2308.09687",
        "authors": [
            "maciej besta",
            "nils blach",
            "ales kubicek",
            "robert gerstenberger",
            "lukas gianinazzi",
            "joanna gajda",
            "tomasz lehmann",
            "michal podstawski",
            "hubert niewiadomski",
            "piotr nyczyk",
            "torsten hoefler"
        ]
    },
    {
        "id": "2308.12319",
        "title": "removalnet: dnn fingerprint removal attacks",
        "abstract": "with the performance of deep neural networks (dnns) remarkably improving, dnns have been widely used in many areas. consequently, the dnn model has become a valuable asset, and its intellectual property is safeguarded by ownership verification techniques (e.g., dnn fingerprinting). however, the feasibility of the dnn fingerprint removal attack and its potential influence remains an open problem. in this paper, we perform the first comprehensive investigation of dnn fingerprint removal attacks. generally, the knowledge contained in a dnn model can be categorized into general semantic and fingerprint-specific knowledge. to this end, we propose a min-max bilevel optimization-based dnn fingerprint removal attack named removalnet, to evade model ownership verification. the lower-level optimization is designed to remove fingerprint-specific knowledge. while in the upper-level optimization, we distill the victim model's general semantic knowledge to maintain the surrogate model's performance. we conduct extensive experiments to evaluate the fidelity, effectiveness, and efficiency of the removalnet against four advanced defense methods on six metrics. the empirical results demonstrate that (1) the removalnet is effective. after our dnn fingerprint removal attack, the model distance between the target and surrogate models is x100 times higher than that of the baseline attacks, (2) the removalnet is efficient. it uses only 0.2% (400 samples) of the substitute dataset and 1,000 iterations to conduct our attack. besides, compared with advanced model stealing attacks, the removalnet saves nearly 85% of computational resources at most, (3) the removalnet achieves high fidelity that the created surrogate model maintains high accuracy after the dnn fingerprint removal process. our code is available at: https://github.com/grasses/removalnet.",
        "doi": "",
        "created": "2023-08-23",
        "url": "https://arxiv.org/abs/2308.12319",
        "authors": [
            "hongwei yao",
            "zheng li",
            "kunzhe huang",
            "jian lou",
            "zhan qin",
            "kui ren"
        ]
    },
    {
        "id": "2308.14936",
        "title": "auto-prompting sam for mobile friendly 3d medical image segmentation",
        "abstract": "segment anything model (sam) has rapidly been adopted for segmenting a wide range of natural images. however, recent studies have indicated that sam exhibits subpar performance on 3d medical image segmentation tasks. in addition to the domain gaps between natural and medical images, disparities in the spatial arrangement between 2d and 3d images, the substantial computational burden imposed by powerful gpu servers, and the time-consuming manual prompt generation impede the extension of sam to a broader spectrum of medical image segmentation applications. to mitigate these challenges, we introduce a novel method, autosam adapter, designed specifically for 3d multi-organ ct-based segmentation. this approach utilizes parameter-efficient adaptation techniques and an automatic prompt learning paradigm, transforming sam's capabilities for 3d medical image segmentation. it eliminates the need for manual prompts and achieves sota performance in ct-based multi-organ segmentation tasks. furthermore, we successfully transfer the acquired knowledge of the autosam adapter to other lightweight models tailored for 3d medical image analysis with enhanced performance. through extensive experiments, the autosam adapter has been demonstrated as an effective method to adapt the foundational sam-based 2d natural image segmentation model for 3d medical image segmentation tasks.",
        "doi": "",
        "created": "2023-08-28",
        "url": "https://arxiv.org/abs/2308.14936",
        "authors": [
            "chengyin li",
            "prashant khanduri",
            "yao qiang",
            "rafi ibn sultan",
            "indrin chetty",
            "dongxiao zhu"
        ]
    },
    {
        "id": "2309.02600",
        "title": "comparative evaluation of metaheuristic algorithms for hyperparameter   selection in short-term weather forecasting",
        "abstract": "weather forecasting plays a vital role in numerous sectors, but accurately capturing the complex dynamics of weather systems remains a challenge for traditional statistical models. apart from auto regressive time forecasting models like arima, deep learning techniques (vanilla anns, lstm and gru networks), have shown promise in improving forecasting accuracy by capturing temporal dependencies. this paper explores the application of metaheuristic algorithms, namely genetic algorithm (ga), differential evolution (de), and particle swarm optimization (pso), to automate the search for optimal hyperparameters in these model architectures. metaheuristic algorithms excel in global optimization, offering robustness, versatility, and scalability in handling non-linear problems. we present a comparative analysis of different model architectures integrated with metaheuristic optimization, evaluating their performance in weather forecasting based on metrics such as mean squared error (mse) and mean absolute percentage error (mape). the results demonstrate the potential of metaheuristic algorithms in enhancing weather forecasting accuracy \\& helps in determining the optimal set of hyper-parameters for each model. the paper underscores the importance of harnessing advanced optimization techniques to select the most suitable metaheuristic algorithm for the given weather forecasting task.",
        "doi": "10.5220/0012187300003595",
        "created": "2023-09-05",
        "url": "https://arxiv.org/abs/2309.02600",
        "authors": [
            "anuvab sen",
            "arul rhik mazumder",
            "dibyarup dutta",
            "udayon sen",
            "pathikrit syam",
            "sandipan dhar"
        ]
    },
    {
        "id": "2309.05238",
        "title": "generating natural language queries for more effective systematic review   screening prioritisation",
        "abstract": "screening prioritisation in medical systematic reviews aims to rank the set of documents retrieved by complex boolean queries. prioritising the most important documents ensures that subsequent review steps can be carried out more efficiently and effectively. the current state of the art uses the final title of the review as a query to rank the documents using bert-based neural rankers. however, the final title is only formulated at the end of the review process, which makes this approach impractical as it relies on ex post facto information. at the time of screening, only a rough working title is available, with which the bert-based ranker performs significantly worse than with the final title. in this paper, we explore alternative sources of queries for prioritising screening, such as the boolean query used to retrieve the documents to be screened and queries generated by instruction-based generative large-scale language models such as chatgpt and alpaca. our best approach is not only viable based on the information available at the time of screening, but also has similar effectiveness to the final title.",
        "doi": "",
        "created": "2023-09-11",
        "url": "https://arxiv.org/abs/2309.05238",
        "authors": [
            "shuai wang",
            "harrisen scells",
            "martin potthast",
            "bevan koopman",
            "guido zuccon"
        ]
    },
    {
        "id": "2309.14216",
        "title": "memda: forecasting urban time series with memory-based drift adaptation",
        "abstract": "urban time series data forecasting featuring significant contributions to sustainable development is widely studied as an essential task of the smart city. however, with the dramatic and rapid changes in the world environment, the assumption that data obey independent identically distribution is undermined by the subsequent changes in data distribution, known as concept drift, leading to weak replicability and transferability of the model over unseen data. to address the issue, previous approaches typically retrain the model, forcing it to fit the most recent observed data. however, retraining is problematic in that it leads to model lag, consumption of resources, and model re-invalidation, causing the drift problem to be not well solved in realistic scenarios. in this study, we propose a new urban time series prediction model for the concept drift problem, which encodes the drift by considering the periodicity in the data and makes on-the-fly adjustments to the model based on the drift using a meta-dynamic network. experiments on real-world datasets show that our design significantly outperforms state-of-the-art methods and can be well generalized to existing prediction backbones by reducing their sensitivity to distribution changes.",
        "doi": "10.1145/3583780.3614962",
        "created": "2023-09-25",
        "url": "https://arxiv.org/abs/2309.14216",
        "authors": [
            "zekun cai",
            "renhe jiang",
            "xinyu yang",
            "zhaonan wang",
            "diansheng guo",
            "hiroki kobayashi",
            "xuan song",
            "ryosuke shibasaki"
        ]
    },
    {
        "id": "2309.17169",
        "title": "an evaluation of gpt models for phenotype concept recognition",
        "abstract": "objective: clinical deep phenotyping and phenotype annotation play a critical role in both the diagnosis of patients with rare disorders as well as in building computationally-tractable knowledge in the rare disorders field. these processes rely on using ontology concepts, often from the human phenotype ontology, in conjunction with a phenotype concept recognition task (supported usually by machine learning methods) to curate patient profiles or existing scientific literature. with the significant shift in the use of large language models (llms) for most nlp tasks, we examine the performance of the latest generative pre-trained transformer (gpt) models underpinning chatgpt as a foundation for the tasks of clinical phenotyping and phenotype annotation. materials and methods: the experimental setup of the study included seven prompts of various levels of specificity, two gpt models (gpt-3.5-turbo and gpt-4.0) and two established gold standard corpora for phenotype recognition, one consisting of publication abstracts and the other clinical observations. results: our results show that, with an appropriate setup, these models can achieve state of the art performance. the best run, using few-shot learning, achieved 0.58 macro f1 score on publication abstracts and 0.75 macro f1 score on clinical observations, the former being comparable with the state of the art, while the latter surpassing the current best in class tool. conclusion: while the results are promising, the non-deterministic nature of the outcomes, the high cost and the lack of concordance between different runs using the same prompt and input make the use of these llms challenging for this particular task.",
        "doi": "",
        "created": "2023-09-29",
        "url": "https://arxiv.org/abs/2309.17169",
        "authors": [
            "tudor groza",
            "harry caufield",
            "dylan gration",
            "gareth baynam",
            "melissa a haendel",
            "peter n robinson",
            "christopher j mungall",
            "justin t reese"
        ]
    },
    {
        "id": "2310.00116",
        "title": "certified robustness via dynamic margin maximization and improved   lipschitz regularization",
        "abstract": "to improve the robustness of deep classifiers against adversarial perturbations, many approaches have been proposed, such as designing new architectures with better robustness properties (e.g., lipschitz-capped networks), or modifying the training process itself (e.g., min-max optimization, constrained learning, or regularization). these approaches, however, might not be effective at increasing the margin in the input (feature) space. as a result, there has been an increasing interest in developing training procedures that can directly manipulate the decision boundary in the input space. in this paper, we build upon recent developments in this category by developing a robust training algorithm whose objective is to increase the margin in the output (logit) space while regularizing the lipschitz constant of the model along vulnerable directions. we show that these two objectives can directly promote larger margins in the input space. to this end, we develop a scalable method for calculating guaranteed differentiable upper bounds on the lipschitz constant of neural networks accurately and efficiently. the relative accuracy of the bounds prevents excessive regularization and allows for more direct manipulation of the decision boundary. furthermore, our lipschitz bounding algorithm exploits the monotonicity and lipschitz continuity of the activation layers, and the resulting bounds can be used to design new layers with controllable bounds on their lipschitz constant. experiments on the mnist, cifar-10, and tiny-imagenet data sets verify that our proposed algorithm obtains competitively improved results compared to the state-of-the-art.",
        "doi": "",
        "created": "2023-09-29",
        "url": "https://arxiv.org/abs/2310.00116",
        "authors": [
            "mahyar fazlyab",
            "taha entesari",
            "aniket roy",
            "rama chellappa"
        ]
    },
    {
        "id": "2310.00583",
        "title": "city foundation models for learning general purpose representations from   openstreetmap",
        "abstract": "pre-trained foundation models (pfms) have ushered in a paradigm-shift in artificial intelligence, due to their ability to learn general-purpose representations that can be readily employed in a wide range of downstream tasks. while pfms have been successfully adopted in various fields such as natural language processing and computer vision, their capacity in handling geospatial data and answering urban questions remains limited. this can be attributed to the intrinsic heterogeneity of geospatial data, which encompasses different data types, including points, segments and regions, as well as multiple information modalities, such as a spatial position, visual characteristics and textual annotations. the proliferation of volunteered geographic information initiatives, and the ever-increasing availability of open geospatial data sources, like openstreetmap, which is freely accessible globally, unveil a promising opportunity to bridge this gap. in this paper, we present cityfm, a self-supervised framework to train a foundation model within a selected geographical area of interest, such as a city. cityfm relies solely on open data from osm, and produces multimodal representations of entities of different types, incorporating spatial, visual, and textual information. we analyse the entity representations generated using our foundation models from a qualitative perspective, and conduct quantitative experiments on road, building, and region-level downstream tasks. we compare its results to algorithms tailored specifically for the respective applications. in all the experiments, cityfm achieves performance superior to, or on par with, the baselines.",
        "doi": "",
        "created": "2023-10-01",
        "url": "https://arxiv.org/abs/2310.00583",
        "authors": [
            "pasquale balsebre",
            "weiming huang",
            "gao cong",
            "yi li"
        ]
    },
    {
        "id": "2310.04483",
        "title": "reward dropout improves control: bi-objective perspective on reinforced   lm",
        "abstract": "we study the theoretical aspects of reinforced language models (rlms) from a bi-objective optimization perspective. specifically, we consider the rlms as a pareto optimization problem that maximizes the two conflicting objectives, i.e., reward objective and likelihood objectives, simultaneously. our main contribution consists of three parts. first, we establish the theoretical foundations of rlm as a pareto optimization problem by presenting reward upper bound (rubo) and pareto optimality. our theoretical outcomes are supported by not only deductive proofs but also empirical results. second, we propose reward dropout, a simple yet powerful method that guarantees to improve a bi-objective optimization of rlm. lastly, we demonstrate that the reward dropout is consistently effective across five benchmark datasets and four benchmark llms, meaning that the reward dropout significantly improves the optimization performance of rlms.",
        "doi": "",
        "created": "2023-10-06",
        "url": "https://arxiv.org/abs/2310.04483",
        "authors": [
            "changhun lee",
            "chiehyeon lim"
        ]
    },
    {
        "id": "2310.05028",
        "title": "revisiting large language models as zero-shot relation extractors",
        "abstract": "relation extraction (re) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. recent studies have shown that large language models (llms) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. this work focuses on the study of exploring llms, such as chatgpt, as zero-shot relation extractors. on the one hand, we analyze the drawbacks of existing re prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (cot) to improve zero-shot re. we propose the summarize-and-ask (\\textsc{sumask}) prompting, a simple prompt recursively using llms to transform re inputs to the effective question answering (qa) format. on the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of llms on zero-shot re. specifically, we have the following findings: (i) \\textsc{sumask} consistently and significantly improves llms performance on different model sizes, benchmarks and settings; (ii) zero-shot prompting with chatgpt achieves competitive or superior results compared with zero-shot and fully supervised methods; (iii) llms deliver promising performance in extracting overlapping relations; (iv) the performance varies greatly regarding different relations. different from small language models, llms are effective in handling challenge none-of-the-above (nota) relation.",
        "doi": "",
        "created": "2023-10-08",
        "url": "https://arxiv.org/abs/2310.05028",
        "authors": [
            "guozheng li",
            "peng wang",
            "wenjun ke"
        ]
    },
    {
        "id": "2310.05052",
        "title": "accurate battery lifetime prediction across diverse aging conditions   with deep learning",
        "abstract": "accurately predicting the lifetime of battery cells in early cycles holds tremendous value for battery research and development as well as numerous downstream applications. this task is rather challenging because diverse conditions, such as electrode materials, operating conditions, and working environments, collectively determine complex capacity-degradation behaviors. however, current prediction methods are developed and validated under limited aging conditions, resulting in questionable adaptability to varied aging conditions and an inability to fully benefit from historical data collected under different conditions. here we introduce a universal deep learning approach that is capable of accommodating various aging conditions and facilitating effective learning under low-resource conditions by leveraging data from rich conditions. our key finding is that incorporating inter-cell feature differences, rather than solely considering single-cell characteristics, significantly increases the accuracy of battery lifetime prediction and its cross-condition robustness. accordingly, we develop a holistic learning framework accommodating both single-cell and inter-cell modeling. a comprehensive benchmark is built for evaluation, encompassing 401 battery cells utilizing 5 prevalent electrode materials across 168 cycling conditions. we demonstrate remarkable capabilities in learning across diverse aging conditions, exclusively achieving 10% prediction error using the first 100 cycles, and in facilitating low-resource learning, almost halving the error of single-cell modeling in many cases. more broadly, by breaking the learning boundaries among different aging conditions, our approach could significantly accelerate the development and optimization of lithium-ion batteries.",
        "doi": "",
        "created": "2023-10-08",
        "url": "https://arxiv.org/abs/2310.05052",
        "authors": [
            "han zhang",
            "yuqi li",
            "shun zheng",
            "ziheng lu",
            "xiaofan gui",
            "wei xu",
            "jiang bian"
        ]
    },
    {
        "id": "2310.05898",
        "title": "lion secretly solves constrained optimization: as lyapunov predicts",
        "abstract": "lion (evolved sign momentum), a new optimizer discovered through program search, has shown promising results in training large ai models. it performs comparably or favorably to adamw but with greater memory efficiency. as we can expect from the results of a random search program, lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, polak, and nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. thus, even though lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. this lack of theoretical clarity limits opportunities to further enhance and expand lion's efficacy.   this work aims to demystify lion. based on both continuous-time and discrete-time analysis, we demonstrate that lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint $\\|x\\|_\\infty \\leq 1/\\lambda$. lion achieves this through the incorporation of decoupled weight decay, where $\\lambda$ represents the weight decay coefficient. our analysis is made possible by the development of a new lyapunov function for the lion updates. it applies to a broader family of lion-$\\kappa$ algorithms, where the $\\text{sign}(\\cdot)$ operator in lion is replaced by the subgradient of a convex function $\\kappa$, leading to the solution of a general composite optimization problem of $\\min_x f(x) + \\kappa^*(x)$. our findings provide valuable insights into the dynamics of lion and pave the way for further improvements and extensions of lion-related algorithms.",
        "doi": "",
        "created": "2023-10-09",
        "url": "https://arxiv.org/abs/2310.05898",
        "authors": [
            "lizhang chen",
            "bo liu",
            "kaizhao liang",
            "qiang liu"
        ]
    },
    {
        "id": "2310.13007",
        "title": "a critical survey on fairness benefits of xai",
        "abstract": "in this critical survey, we analyze typical claims on the relationship between explainable ai (xai) and fairness to disentangle the multidimensional relationship between these two concepts. based on a systematic literature review and a subsequent qualitative content analysis, we identify seven archetypal claims from 175 papers on the alleged fairness benefits of xai. we present crucial caveats with respect to these claims and provide an entry point for future discussions around the potentials and limitations of xai for specific fairness desiderata. while the literature often suggests xai to be an enabler for several fairness desiderata, we notice a divide between these desiderata and the capabilities of xai. we encourage to conceive xai as one of many tools to approach the multidimensional, sociotechnical challenge of algorithmic fairness and to be more specific about how exactly what kind of xai method enables whom to address which fairness desideratum.",
        "doi": "",
        "created": "2023-10-15",
        "url": "https://arxiv.org/abs/2310.13007",
        "authors": [
            "luca deck",
            "jakob schoeffer",
            "maria de-arteaga",
            "niklas k\u00fchl"
        ]
    },
    {
        "id": "2310.13102",
        "title": "particle guidance: non-i.i.d. diverse sampling with diffusion models",
        "abstract": "in light of the widespread success of generative models, a significant amount of research has gone into speeding up their sampling time. however, generative models are often sampled multiple times to obtain a diverse set incurring a cost that is orthogonal to sampling time. we tackle the question of how to improve diversity and sample efficiency by moving beyond the common assumption of independent samples. we propose particle guidance, an extension of diffusion-based generative sampling where a joint-particle time-evolving potential enforces diversity. we analyze theoretically the joint distribution that particle guidance generates, how to learn a potential that achieves optimal diversity, and the connections with methods in other disciplines. empirically, we test the framework both in the setting of conditional image generation, where we are able to increase diversity without affecting quality, and molecular conformer generation, where we reduce the state-of-the-art median error by 13% on average.",
        "doi": "",
        "created": "2023-10-19",
        "url": "https://arxiv.org/abs/2310.13102",
        "authors": [
            "gabriele corso",
            "yilun xu",
            "valentin de bortoli",
            "regina barzilay",
            "tommi jaakkola"
        ]
    },
    {
        "id": "2310.14790",
        "title": "weighted joint maximum mean discrepancy enabled   multi-source-multi-target unsupervised domain adaptation fault diagnosis",
        "abstract": "despite the remarkable results that can be achieved by data-driven intelligent fault diagnosis techniques, they presuppose the same distribution of training and test data as well as sufficient labeled data. various operating states often exist in practical scenarios, leading to the problem of domain shift that hinders the effectiveness of fault diagnosis. while recent unsupervised domain adaptation methods enable cross-domain fault diagnosis, they struggle to effectively utilize information from multiple source domains and achieve effective diagnosis faults in multiple target domains simultaneously. in this paper, we innovatively proposed a weighted joint maximum mean discrepancy enabled multi-source-multi-target unsupervised domain adaptation (wjmmd-mda), which realizes domain adaptation under multi-source-multi-target scenarios in the field of fault diagnosis for the first time. the proposed method extracts sufficient information from multiple labeled source domains and achieves domain alignment between source and target domains through an improved weighted distance loss. as a result, domain-invariant and discriminative features between multiple source and target domains are learned with cross-domain fault diagnosis realized. the performance of the proposed method is evaluated in comprehensive comparative experiments on three datasets, and the experimental results demonstrate the superiority of this method.",
        "doi": "",
        "created": "2023-10-20",
        "url": "https://arxiv.org/abs/2310.14790",
        "authors": [
            "zixuan wang",
            "haoran tang",
            "haibo wang",
            "bo qin",
            "mark d. butala",
            "weiming shen",
            "hongwei wang"
        ]
    },
    {
        "id": "2310.15386",
        "title": "course correcting koopman representations",
        "abstract": "koopman representations aim to learn features of nonlinear dynamical systems (nlds) which lead to linear dynamics in the latent space. theoretically, such features can be used to simplify many problems in modeling and control of nlds. in this work we study autoencoder formulations of this problem, and different ways they can be used to model dynamics, specifically for future state prediction over long horizons. we discover several limitations of predicting future states in the latent space and propose an inference-time mechanism, which we refer to as periodic reencoding, for faithfully capturing long term dynamics. we justify this method both analytically and empirically via experiments in low and high dimensional nlds.",
        "doi": "",
        "created": "2023-10-23",
        "url": "https://arxiv.org/abs/2310.15386",
        "authors": [
            "mahan fathi",
            "clement gehring",
            "jonathan pilault",
            "david kanaa",
            "pierre-luc bacon",
            "ross goroshin"
        ]
    },
    {
        "id": "2310.18075",
        "title": "duma: a dual-mind conversational agent with fast and slow thinking",
        "abstract": "inspired by the dual-process theory of human cognition, we introduce duma, a novel conversational agent framework that embodies a dual-mind mechanism through the utilization of two generative large language models (llms) dedicated to fast and slow thinking respectively. the fast thinking model serves as the primary interface for external interactions and initial response generation, evaluating the necessity for engaging the slow thinking model based on the complexity of the complete response. when invoked, the slow thinking model takes over the conversation, engaging in meticulous planning, reasoning, and tool utilization to provide a well-analyzed response. this dual-mind configuration allows for a seamless transition between intuitive responses and deliberate problem-solving processes based on the situation. we have constructed a conversational agent to handle online inquiries in the real estate industry. the experiment proves that our method balances effectiveness and efficiency, and has a significant improvement compared to the baseline.",
        "doi": "",
        "created": "2023-10-27",
        "url": "https://arxiv.org/abs/2310.18075",
        "authors": [
            "xiaoyu tian",
            "liangyu chen",
            "na liu",
            "yaxuan liu",
            "wei zou",
            "kaijiang chen",
            "ming cui"
        ]
    },
    {
        "id": "2310.18301",
        "title": "interactive joint planning for autonomous vehicles",
        "abstract": "in highly interactive driving scenarios, the actions of one agent greatly influences those of its neighbors. planning safe motions for autonomous vehicles in such interactive environments, therefore, requires reasoning about the impact of the ego's intended motion plan on nearby agents' behavior. deep-learning-based models have recently achieved great success in trajectory prediction and many models in the literature allow for ego-conditioned prediction. however, leveraging ego-conditioned prediction remains challenging in downstream planning due to the complex nature of neural networks, limiting the planner structure to simple ones, e.g., sampling-based planner. despite their ability to generate fine-grained high-quality motion plans, it is difficult for gradient-based planning algorithms, such as model predictive control (mpc), to leverage ego-conditioned prediction due to their iterative nature and need for gradient. we present interactive joint planning (ijp) that bridges mpc with learned prediction models in a computationally scalable manner to provide us the best of both the worlds. in particular, ijp jointly optimizes over the behavior of the ego and the surrounding agents and leverages deep-learned prediction models as prediction priors that the join trajectory optimization tries to stay close to. furthermore, by leveraging homotopy classes, our joint optimizer searches over diverse motion plans to avoid getting stuck at local minima. closed-loop simulation result shows that ijp significantly outperforms the baselines that are either without joint optimization or running sampling-based planning.",
        "doi": "",
        "created": "2023-10-27",
        "url": "https://arxiv.org/abs/2310.18301",
        "authors": [
            "yuxiao chen",
            "sushant veer",
            "peter karkus",
            "marco pavone"
        ]
    },
    {
        "id": "2310.18333",
        "title": "she had cobalt blue eyes: prompt testing to create aligned and   sustainable language models",
        "abstract": "as the use of large language models (llms) increases within society, as does the risk of their misuse. appropriate safeguards must be in place to ensure llm outputs uphold the ethical standards of society, highlighting the positive role that artificial intelligence technologies can have. recent events indicate ethical concerns around conventionally trained llms, leading to overall unsafe user experiences. this motivates our research question: how do we ensure llm alignment? in this work, we introduce a test suite of unique prompts to foster the development of aligned llms that are fair, safe, and robust. we show that prompting llms at every step of the development pipeline, including data curation, pre-training, and fine-tuning, will result in an overall more responsible model. our test suite evaluates outputs from four state-of-the-art language models: gpt-3.5, gpt-4, opt, and llama-2. the assessment presented in this paper highlights a gap between societal alignment and the capabilities of current llms. additionally, implementing a test suite such as ours lowers the environmental overhead of making models safe and fair.",
        "doi": "",
        "created": "2023-10-20",
        "url": "https://arxiv.org/abs/2310.18333",
        "authors": [
            "veronica chatrath",
            "oluwanifemi bamgbose",
            "shaina raza"
        ]
    },
    {
        "id": "2310.19204",
        "title": "can chatgpt advance software testing intelligence? an experience report   on metamorphic testing",
        "abstract": "while chatgpt is a well-known artificial intelligence chatbot being used to answer human's questions, one may want to discover its potential in advancing software testing. we examine the capability of chatgpt in advancing the intelligence of software testing through a case study on metamorphic testing (mt), a state-of-the-art software testing technique. we ask chatgpt to generate candidates of metamorphic relations (mrs), which are basically necessary properties of the object program and which traditionally require human intelligence to identify. these mr candidates are then evaluated in terms of correctness by domain experts. we show that chatgpt can be used to generate new correct mrs to test several software systems. having said that, the majority of mr candidates are either defined vaguely or incorrect, especially for systems that have never been tested with mt. chatgpt can be used to advance software testing intelligence by proposing mr candidates that can be later adopted for implementing tests; but human intelligence should still inevitably be involved to justify and rectify their correctness.",
        "doi": "",
        "created": "2023-10-29",
        "url": "https://arxiv.org/abs/2310.19204",
        "authors": [
            "quang-hung luu",
            "huai liu",
            "tsong yueh chen"
        ]
    },
    {
        "id": "2311.00860",
        "title": "zero coordinate shift: whetted automatic differentiation for   physics-informed operator learning",
        "abstract": "automatic differentiation (ad) is a critical step in physics-informed machine learning, required for computing the high-order derivatives of network output w.r.t. coordinates of collocation points. in this paper, we present a novel and lightweight algorithm to conduct ad for physics-informed operator learning, which we call the trick of zero coordinate shift (zcs). instead of making all sampled coordinates as leaf variables, zcs introduces only one scalar-valued leaf variable for each spatial or temporal dimension, simplifying the wanted derivatives from \"many-roots-many-leaves\" to \"one-root-many-leaves\" whereby reverse-mode ad becomes directly utilisable. it has led to an outstanding performance leap by avoiding the duplication of the computational graph along the dimension of functions (physical parameters). zcs is easy to implement with current deep learning libraries; our own implementation is achieved by extending the deepxde package. we carry out a comprehensive benchmark analysis and several case studies, training physics-informed deeponets to solve partial differential equations (pdes) without data. the results show that zcs has persistently reduced gpu memory consumption and wall time for training by an order of magnitude, and such reduction factor scales with the number of functions. as a low-level optimisation technique, zcs imposes no restrictions on data, physics (pde) or network architecture and does not compromise training results from any aspect.",
        "doi": "",
        "created": "2023-11-01",
        "url": "https://arxiv.org/abs/2311.00860",
        "authors": [
            "kuangdai leng",
            "mallikarjun shankar",
            "jeyan thiyagalingam"
        ]
    },
    {
        "id": "2311.01017",
        "title": "learning unsupervised world models for autonomous driving via discrete   diffusion",
        "abstract": "learning world models can teach an agent how the world works in an unsupervised manner. even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with generative pre-trained transformers (gpt). we identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. consequently, we propose a novel world modeling approach that first tokenizes sensor observations with vqvae, then predicts the future via discrete diffusion. to efficiently decode and denoise tokens in parallel, we recast masked generative image transformer into the discrete diffusion framework with a few simple changes, resulting in notable improvement. when applied to learning world models on point cloud observations, our model reduces prior sota chamfer distance by more than 65% for 1s prediction, and more than 50% for 3s prediction, across nuscenes, kitti odometry, and argoverse2 datasets. our results demonstrate that discrete diffusion on tokenized agent experience can unlock the power of gpt-like unsupervised learning for robotic agents.",
        "doi": "",
        "created": "2023-11-02",
        "url": "https://arxiv.org/abs/2311.01017",
        "authors": [
            "lunjun zhang",
            "yuwen xiong",
            "ze yang",
            "sergio casas",
            "rui hu",
            "raquel urtasun"
        ]
    },
    {
        "id": "2311.02082",
        "title": "semantic modelling of organizational knowledge as a basis for enterprise   data governance 4.0 -- application to a unified clinical data model",
        "abstract": "individuals and organizations cope with an always-growing amount of data, which is heterogeneous in its contents and formats. an adequate data management process yielding data quality and control over its lifecycle is a prerequisite to getting value out of this data and minimizing inherent risks related to multiple usages. common data governance frameworks rely on people, policies, and processes that fall short of the overwhelming complexity of data. yet, harnessing this complexity is necessary to achieve high-quality standards. the latter will condition any downstream data usage outcome, including generative artificial intelligence trained on this data. in this paper, we report our concrete experience establishing a simple, cost-efficient framework that enables metadata-driven, agile and (semi-)automated data governance (i.e. data governance 4.0). we explain how we implement and use this framework to integrate 25 years of clinical study data at an enterprise scale in a fully productive environment. the framework encompasses both methodologies and technologies leveraging semantic web principles. we built a knowledge graph describing avatars of data assets in their business context, including governance principles. multiple ontologies articulated by an enterprise upper ontology enable key governance actions such as fairification, lifecycle management, definition of roles and responsibilities, lineage across transformations and provenance from source systems. this metadata model is the keystone to data governance 4.0: a semi-automatised data management process that considers the business context in an agile manner to adapt governance constraints to each use case and dynamically tune it based on business changes.",
        "doi": "",
        "created": "2023-10-20",
        "url": "https://arxiv.org/abs/2311.02082",
        "authors": [
            "miguel ap oliveira",
            "stephane manara",
            "bruno mol\u00e9",
            "thomas muller",
            "aur\u00e9lien guillouche",
            "lysann hesske",
            "bruce jordan",
            "gilles hubert",
            "chinmay kulkarni",
            "pralipta jagdev",
            "cedric r. berger"
        ]
    },
    {
        "id": "2311.02198",
        "title": "imitation bootstrapped reinforcement learning",
        "abstract": "despite the considerable potential of reinforcement learning (rl), robotics control tasks predominantly rely on imitation learning (il) owing to its better sample efficiency. however, given the high cost of collecting extensive demonstrations, rl is still appealing if it can utilize limited imitation data for efficient autonomous self-improvement. existing rl methods that utilize demonstrations either initialize the replay buffer with demonstrations and oversample them during rl training, which does not benefit from the generalization potential of modern il methods, or pretrain the rl policy with il on the demonstrations, which requires additional mechanisms to prevent catastrophic forgetting during rl fine-tuning. we propose imitation bootstrapped reinforcement learning (ibrl), a novel framework that first trains an il policy on a limited number of demonstrations and then uses it to propose alternative actions for both online exploration and target value bootstrapping. ibrl achieves sota performance and sample efficiency on 7 challenging sparse reward continuous control tasks in simulation while learning directly from pixels. as a highlight of our method, ibrl achieves $6.4\\times$ higher success rate than rlpd, a strong method that combines the idea of oversampling demonstrations with modern rl improvements, under the budget of 10 demos and 100k interactions in the challenging pickplacecan task in the robomimic benchmark.",
        "doi": "",
        "created": "2023-11-03",
        "url": "https://arxiv.org/abs/2311.02198",
        "authors": [
            "hengyuan hu",
            "suvir mirchandani",
            "dorsa sadigh"
        ]
    },
    {
        "id": "2311.02651",
        "title": "compute at scale: a broad investigation into the data center industry",
        "abstract": "this report characterizes the data center industry and its importance for ai development. data centers are industrial facilities that efficiently provide compute at scale and thus constitute the engine rooms of today's digital economy. as large-scale ai training and inference become increasingly computationally expensive, they are dominantly executed from this designated infrastructure. key features of data centers include large-scale compute clusters that require extensive cooling and consume large amounts of power, the need for fast connectivity both within the data center and to the internet, and an emphasis on security and reliability. the global industry is valued at approximately $250b and is expected to double over the next seven years. there are likely about 500 large (above 10 mw) data centers globally, with the us, europe, and china constituting the most important markets. the report further covers important actors, business models, main inputs, and typical locations of data centers.",
        "doi": "",
        "created": "2023-11-05",
        "url": "https://arxiv.org/abs/2311.02651",
        "authors": [
            "konstantin pilz",
            "lennart heim"
        ]
    },
    {
        "id": "2311.03348",
        "title": "scalable and transferable black-box jailbreaks for language models via   persona modulation",
        "abstract": "despite efforts to align large language models to produce harmless responses, they are still vulnerable to jailbreak prompts that elicit unrestricted behaviour. in this work, we investigate persona modulation as a black-box jailbreaking method to steer a target model to take on personalities that are willing to comply with harmful instructions. rather than manually crafting prompts for each persona, we automate the generation of jailbreaks using a language model assistant. we demonstrate a range of harmful completions made possible by persona modulation, including detailed instructions for synthesising methamphetamine, building a bomb, and laundering money. these automated attacks achieve a harmful completion rate of 42.5% in gpt-4, which is 185 times larger than before modulation (0.23%). these prompts also transfer to claude 2 and vicuna with harmful completion rates of 61.0% and 35.9%, respectively. our work reveals yet another vulnerability in commercial large language models and highlights the need for more comprehensive safeguards.",
        "doi": "",
        "created": "2023-11-06",
        "url": "https://arxiv.org/abs/2311.03348",
        "authors": [
            "rusheb shah",
            "quentin feuillade--montixi",
            "soroush pour",
            "arush tagade",
            "stephen casper",
            "javier rando"
        ]
    },
    {
        "id": "2311.03739",
        "title": "leveraging large language models for automated proof synthesis in rust",
        "abstract": "formal verification can provably guarantee the correctness of critical system software, but the high proof burden has long hindered its wide adoption. recently, large language models (llms) have shown success in code analysis and synthesis. in this paper, we present a combination of llms and static analysis to synthesize invariants, assertions, and other proof structures for a rust-based formal verification framework called verus. in a few-shot setting, llms demonstrate impressive logical ability in generating postconditions and loop invariants, especially when analyzing short code snippets. however, llms lack the ability to retain and propagate context information, a strength of traditional static analysis. based on these observations, we developed a prototype based on openai's gpt-4 model. our prototype decomposes the verification task into multiple smaller ones, iteratively queries gpt-4, and combines its output with lightweight static analysis. we evaluated the prototype with a developer in the automation loop on 20 vector-manipulating programs. the results demonstrate that it significantly reduces human effort in writing entry-level proof code.",
        "doi": "",
        "created": "2023-11-07",
        "url": "https://arxiv.org/abs/2311.03739",
        "authors": [
            "jianan yao",
            "ziqiao zhou",
            "weiteng chen",
            "weidong cui"
        ]
    },
    {
        "id": "2311.05784",
        "title": "are \"hierarchical\" visual representations hierarchical?",
        "abstract": "learned visual representations often capture large amounts of semantic information for accurate downstream applications. human understanding of the world is fundamentally grounded in hierarchy. to mimic this and further improve representation capabilities, the community has explored \"hierarchical\" visual representations that aim at modeling the underlying hierarchy of the visual world. in this work, we set out to investigate if hierarchical visual representations truly capture the human perceived hierarchy better than standard learned representations. to this end, we create hiernet, a suite of 12 datasets spanning 3 kinds of hierarchy from the breeds subset of imagenet. after extensive evaluation of hyperbolic and matryoshka representations across training setups, we conclude that they do not capture hierarchy any better than the standard representations but can assist in other aspects like search efficiency and interpretability. our benchmark and the datasets are open-sourced at https://github.com/ethanlshen/hiernet.",
        "doi": "",
        "created": "2023-11-09",
        "url": "https://arxiv.org/abs/2311.05784",
        "authors": [
            "ethan shen",
            "ali farhadi",
            "aditya kusupati"
        ]
    },
    {
        "id": "2311.06258",
        "title": "post-covid highlights: challenges and solutions of ai techniques for   swift identification of covid-19",
        "abstract": "since the onset of the covid-19 pandemic in 2019, there has been a concerted effort to develop cost-effective, non-invasive, and rapid ai-based tools. these tools were intended to alleviate the burden on healthcare systems, control the rapid spread of the virus, and enhance intervention outcomes, all in response to this unprecedented global crisis. as we transition into a post-covid era, we retrospectively evaluate these proposed studies and offer a review of the techniques employed in ai diagnostic models, with a focus on the solutions proposed for different challenges. this review endeavors to provide insights into the diverse solutions designed to address the multifaceted challenges that arose during the pandemic. by doing so, we aim to prepare the ai community for the development of ai tools tailored to address public health emergencies effectively.",
        "doi": "",
        "created": "2023-09-24",
        "url": "https://arxiv.org/abs/2311.06258",
        "authors": [
            "yingying fang",
            "xiaodan xing",
            "shiyi wang",
            "simon walsh",
            "guang yang"
        ]
    },
    {
        "id": "2311.06330",
        "title": "smart agent-based modeling: on the use of large language models in   computer simulations",
        "abstract": "computer simulations offer a robust toolset for exploring complex systems across various disciplines. a particularly impactful approach within this realm is agent-based modeling (abm), which harnesses the interactions of individual agents to emulate intricate system dynamics. abm's strength lies in its bottom-up methodology, illuminating emergent phenomena by modeling the behaviors of individual components of a system. yet, abm has its own set of challenges, notably its struggle with modeling natural language instructions and common sense in mathematical equations or rules. this paper seeks to transcend these boundaries by integrating large language models (llms) like gpt into abm. this amalgamation gives birth to a novel framework, smart agent-based modeling (sabm). building upon the concept of smart agents -- entities characterized by their intelligence, adaptability, and computation ability -- we explore in the direction of utilizing llm-powered agents to simulate real-world scenarios with increased nuance and realism. in this comprehensive exploration, we elucidate the state of the art of abm, introduce sabm's potential and methodology, and present three case studies (source codes available at https://github.com/roihn/sabm), demonstrating the sabm methodology and validating its effectiveness in modeling real-world systems. furthermore, we cast a vision towards several aspects of the future of sabm, anticipating a broader horizon for its applications. through this endeavor, we aspire to redefine the boundaries of computer simulations, enabling a more profound understanding of complex systems.",
        "doi": "",
        "created": "2023-11-10",
        "url": "https://arxiv.org/abs/2311.06330",
        "authors": [
            "zengqing wu",
            "run peng",
            "xu han",
            "shuyuan zheng",
            "yixin zhang",
            "chuan xiao"
        ]
    },
    {
        "id": "2311.06607",
        "title": "monkey: image resolution and text label are important things for large   multi-modal models",
        "abstract": "large multimodal models (lmms) have shown promise in vision-language tasks but struggle with high-resolution input and detailed scene understanding. addressing these challenges, we introduce monkey to enhance lmm capabilities. firstly, monkey processes input images by dividing them into uniform patches, each matching the size (e.g., 448x448) used in the original training of the well-trained vision encoder. equipped with individual adapter for each patch, monkey can handle higher resolutions up to 1344x896 pixels, enabling the detailed capture of complex visual information. secondly, it employs a multi-level description generation method, enriching the context for scene-object associations. this two-part strategy ensures more effective learning from generated data: the higher resolution allows for a more detailed capture of visuals, which in turn enhances the effectiveness of comprehensive descriptions. extensive ablative results validate the effectiveness of our designs. additionally, experiments on 18 datasets further demonstrate that monkey surpasses existing lmms in many tasks like image captioning and various visual question answering formats. specially, in qualitative tests focused on dense text question answering, monkey has exhibited encouraging results compared with gpt4v. code is available at https://github.com/yuliang-liu/monkey.",
        "doi": "",
        "created": "2023-11-11",
        "url": "https://arxiv.org/abs/2311.06607",
        "authors": [
            "zhang li",
            "biao yang",
            "qiang liu",
            "zhiyin ma",
            "shuo zhang",
            "jingxu yang",
            "yabo sun",
            "yuliang liu",
            "xiang bai"
        ]
    },
    {
        "id": "2311.06622",
        "title": "traineragent: customizable and efficient model training through   llm-powered multi-agent system",
        "abstract": "training ai models has always been challenging, especially when there is a need for custom models to provide personalized services. algorithm engineers often face a lengthy process to iteratively develop models tailored to specific business requirements, making it even more difficult for non-experts. the quest for high-quality and efficient model development, along with the emergence of large language model (llm) agents, has become a key focus in the industry. leveraging the powerful analytical, planning, and decision-making capabilities of llm, we propose a traineragent system comprising a multi-agent framework including task, data, model and server agents. these agents analyze user-defined tasks, input data, and requirements (e.g., accuracy, speed), optimizing them comprehensively from both data and model perspectives to obtain satisfactory models, and finally deploy these models as online service. experimental evaluations on classical discriminative and generative tasks in computer vision and natural language processing domains demonstrate that our system consistently produces models that meet the desired criteria. furthermore, the system exhibits the ability to critically identify and reject unattainable tasks, such as fantastical scenarios or unethical requests, ensuring robustness and safety. this research presents a significant advancement in achieving desired models with increased efficiency and quality as compared to traditional model development, facilitated by the integration of llm-powered analysis, decision-making, and execution capabilities, as well as the collaboration among four agents. we anticipate that our work will contribute to the advancement of research on traineragent in both academic and industry communities, potentially establishing it as a new paradigm for model development in the field of ai.",
        "doi": "",
        "created": "2023-11-11",
        "url": "https://arxiv.org/abs/2311.06622",
        "authors": [
            "haoyuan li",
            "hao jiang",
            "tianke zhang",
            "zhelun yu",
            "aoxiong yin",
            "hao cheng",
            "siming fu",
            "yuhao zhang",
            "wanggui he"
        ]
    },
    {
        "id": "2311.06996",
        "title": "agramplifier: defending federated learning against poisoning attacks   through local update amplification",
        "abstract": "the collaborative nature of federated learning (fl) poses a major threat in the form of manipulation of local training data and local updates, known as the byzantine poisoning attack. to address this issue, many byzantine-robust aggregation rules (agrs) have been proposed to filter out or moderate suspicious local updates uploaded by byzantine participants.   this paper introduces a novel approach called agramplifier, aiming to simultaneously improve the robustness, fidelity, and efficiency of the existing agrs. the core idea of agramplifier is to amplify the \"morality\" of local updates by identifying the most repressive features of each gradient update, which provides a clearer distinction between malicious and benign updates, consequently improving the detection effect. to achieve this objective, two approaches, namely agrmp and agrxai, are proposed. agrmp organizes local updates into patches and extracts the largest value from each patch, while agrxai leverages explainable ai methods to extract the gradient of the most activated features. by equipping agramplifier with the existing byzantine-robust mechanisms, we successfully enhance the model's robustness, maintaining its fidelity and improving overall efficiency.   agramplifier is universally compatible with the existing byzantine-robust mechanisms. the paper demonstrates its effectiveness by integrating it with all mainstream agr mechanisms. extensive evaluations conducted on seven datasets from diverse domains against seven representative poisoning attacks consistently show enhancements in robustness, fidelity, and efficiency, with average gains of 40.08%, 39.18%, and 10.68%, respectively.",
        "doi": "10.1109/tifs.2023.3333555",
        "created": "2023-11-12",
        "url": "https://arxiv.org/abs/2311.06996",
        "authors": [
            "zirui gong",
            "liyue shen",
            "yanjun zhang",
            "leo yu zhang",
            "jingwei wang",
            "guangdong bai",
            "yong xiang"
        ]
    },
    {
        "id": "2311.07553",
        "title": "an extensive study on adversarial attack against pre-trained models of   code",
        "abstract": "transformer-based pre-trained models of code (ptmc) have been widely utilized and have achieved state-of-the-art performance in many mission-critical applications. however, they can be vulnerable to adversarial attacks through identifier substitution or coding style transformation, which can significantly degrade accuracy and may further incur security concerns. although several approaches have been proposed to generate adversarial examples for ptmc, the effectiveness and efficiency of such approaches, especially on different code intelligence tasks, has not been well understood. to bridge this gap, this study systematically analyzes five state-of-the-art adversarial attack approaches from three perspectives: effectiveness, efficiency, and the quality of generated examples. the results show that none of the five approaches balances all these perspectives. particularly, approaches with a high attack success rate tend to be time-consuming; the adversarial code they generate often lack naturalness, and vice versa. to address this limitation, we explore the impact of perturbing identifiers under different contexts and find that identifier substitution within for and if statements is the most effective. based on these findings, we propose a new approach that prioritizes different types of statements for various tasks and further utilizes beam search to generate adversarial examples. evaluation results show that it outperforms the state-of-the-art alert in terms of both effectiveness and efficiency while preserving the naturalness of the generated adversarial examples.",
        "doi": "",
        "created": "2023-11-13",
        "url": "https://arxiv.org/abs/2311.07553",
        "authors": [
            "xiaohu du",
            "ming wen",
            "zichao wei",
            "shangwen wang",
            "hai jin"
        ]
    },
    {
        "id": "2311.07585",
        "title": "input reconstruction attack against vertical federated large language   models",
        "abstract": "recently, large language models (llms) have drawn extensive attention from academia and the public, due to the advent of the chatgpt. while llms show their astonishing ability in text generation for various tasks, privacy concerns limit their usage in real-life businesses. more specifically, either the user's inputs (the user sends the query to the model-hosting server) or the model (the user downloads the complete model) itself will be revealed during the usage. vertical federated learning (vfl) is a promising solution to this kind of problem. it protects both the user's input and the knowledge of the model by splitting the model into a bottom part and a top part, which is maintained by the user and the model provider, respectively. however, in this paper, we demonstrate that in llms, vfl fails to protect the user input since it is simple and cheap to reconstruct the input from the intermediate embeddings. experiments show that even with a commercial gpu, the input sentence can be reconstructed in only one second. we also discuss several possible solutions to enhance the privacy of vertical federated llms.",
        "doi": "",
        "created": "2023-11-07",
        "url": "https://arxiv.org/abs/2311.07585",
        "authors": [
            "fei zheng"
        ]
    },
    {
        "id": "2311.09433",
        "title": "backdoor activation attack: attack large language models using   activation steering for safety-alignment",
        "abstract": "to ensure ai safety, instruction-tuned large language models (llms) are specifically trained to ensure alignment, which refers to making models behave in accordance with human intentions. while these models have demonstrated commendable results on various safety benchmarks, the vulnerability of their safety alignment has not been extensively studied. this is particularly troubling given the potential harm that llms can inflict. existing attack methods on llms often rely on poisoned training data or the injection of malicious prompts. these approaches compromise the stealthiness and generalizability of the attacks, making them susceptible to detection. additionally, these models often demand substantial computational resources for implementation, making them less practical for real-world applications. inspired by recent success in modifying model behavior through steering vectors without the need for optimization, and drawing on its effectiveness in red-teaming llms, we conducted experiments employing activation steering to target four key aspects of llms: truthfulness, toxicity, bias, and harmfulness - across a varied set of attack settings. to establish a universal attack strategy applicable to diverse target alignments without depending on manual analysis, we automatically select the intervention layer based on contrastive layer search. our experiment results show that activation attacks are highly effective and add little or no overhead to attack efficiency. additionally, we discuss potential countermeasures against such activation attacks. our code and data are available at https://github.com/wang2226/backdoor-activation-attack warning: this paper contains content that can be offensive or upsetting.",
        "doi": "",
        "created": "2023-11-15",
        "url": "https://arxiv.org/abs/2311.09433",
        "authors": [
            "haoran wang",
            "kai shu"
        ]
    },
    {
        "id": "2311.10049",
        "title": "inherently interpretable time series classification via multiple   instance learning",
        "abstract": "conventional time series classification (tsc) methods are often black boxes that obscure inherent interpretation of their decision-making processes. in this work, we leverage multiple instance learning (mil) to overcome this issue, and propose a new framework called millet: multiple instance learning for locally explainable time series classification. we apply millet to existing deep learning tsc models and show how they become inherently interpretable without compromising (and in some cases, even improving) predictive performance. we evaluate millet on 85 ucr tsc datasets and also present a novel synthetic dataset that is specially designed to facilitate interpretability evaluation. on these datasets, we show millet produces sparse explanations quickly that are of higher quality than other well-known interpretability methods. to the best of our knowledge, our work with millet, which is available on github (https://github.com/jaearly/miltimeseriesclassification), is the first to develop general mil methods for tsc and apply them to an extensive variety of domains",
        "doi": "",
        "created": "2023-11-16",
        "url": "https://arxiv.org/abs/2311.10049",
        "authors": [
            "joseph early",
            "gavin kc cheung",
            "kurt cutajar",
            "hanting xie",
            "jas kandola",
            "niall twomey"
        ]
    },
    {
        "id": "2311.10057",
        "title": "the song describer dataset: a corpus of audio captions for   music-and-language evaluation",
        "abstract": "we introduce the song describer dataset (sdd), a new crowdsourced corpus of high-quality audio-caption pairs, designed for the evaluation of music-and-language models. the dataset consists of 1.1k human-written natural language descriptions of 706 music recordings, all publicly accessible and released under creative common licenses. to showcase the use of our dataset, we benchmark popular models on three key music-and-language tasks (music captioning, text-to-music generation and music-language retrieval). our experiments highlight the importance of cross-dataset evaluation and offer insights into how researchers can use sdd to gain a broader understanding of model performance.",
        "doi": "",
        "created": "2023-11-16",
        "url": "https://arxiv.org/abs/2311.10057",
        "authors": [
            "ilaria manco",
            "benno weck",
            "seungheon doh",
            "minz won",
            "yixiao zhang",
            "dmitry bogdanov",
            "yusong wu",
            "ke chen",
            "philip tovstogan",
            "emmanouil benetos",
            "elio quinton",
            "gy\u00f6rgy fazekas",
            "juhan nam"
        ]
    },
    {
        "id": "2311.10538",
        "title": "testing language model agents safely in the wild",
        "abstract": "a prerequisite for safe autonomy-in-the-wild is safe testing-in-the-wild. yet real-world autonomous tests face several unique safety challenges, both due to the possibility of causing harm during a test, as well as the risk of encountering new unsafe agent behavior through interactions with real-world and potentially malicious actors. we propose a framework for conducting safe autonomous agent tests on the open internet: agent actions are audited by a context-sensitive monitor that enforces a stringent safety boundary to stop an unsafe test, with suspect behavior ranked and logged to be examined by humans. we a design a basic safety monitor (agentmonitor) that is flexible enough to monitor existing llm agents, and, using an adversarial simulated agent, we measure its ability to identify and stop unsafe situations. then we apply the agentmonitor on a battery of real-world tests of autogpt, and we identify several limitations and challenges that will face the creation of safe in-the-wild tests as autonomous agents grow more capable.",
        "doi": "",
        "created": "2023-11-17",
        "url": "https://arxiv.org/abs/2311.10538",
        "authors": [
            "silen naihin",
            "david atkinson",
            "marc green",
            "merwane hamadi",
            "craig swift",
            "douglas schonholtz",
            "adam tauman kalai",
            "david bau"
        ]
    },
    {
        "id": "2311.10751",
        "title": "proagent: from robotic process automation to agentic process automation",
        "abstract": "from ancient water wheels to robotic process automation (rpa), automation technology has evolved throughout history to liberate human beings from arduous tasks. yet, rpa struggles with tasks needing human-like intelligence, especially in elaborate design of workflow construction and dynamic decision-making in workflow execution. as large language models (llms) have emerged human-like intelligence, this paper introduces agentic process automation (apa), a groundbreaking automation paradigm using llm-based agents for advanced automation by offloading the human labor to agents associated with construction and execution. we then instantiate proagent, an llm-based agent designed to craft workflows from human instructions and make intricate decisions by coordinating specialized agents. empirical experiments are conducted to detail its construction and execution procedure of workflow, showcasing the feasibility of apa, unveiling the possibility of a new paradigm of automation driven by agents. our code is public at https://github.com/openbmb/proagent.",
        "doi": "",
        "created": "2023-11-02",
        "url": "https://arxiv.org/abs/2311.10751",
        "authors": [
            "yining ye",
            "xin cong",
            "shizuo tian",
            "jiannan cao",
            "hao wang",
            "yujia qin",
            "yaxi lu",
            "heyang yu",
            "huadong wang",
            "yankai lin",
            "zhiyuan liu",
            "maosong sun"
        ]
    },
    {
        "id": "2311.11215",
        "title": "splain: augmenting cybersecurity warnings with reasons and data",
        "abstract": "effective cyber threat recognition and prevention demand comprehensible forecasting systems, as prior approaches commonly offer limited and, ultimately, unconvincing information. we introduce simplified plaintext language (splain), a natural language generator that converts warning data into user-friendly cyber threat explanations. splain is designed to generate clear, actionable outputs, incorporating hierarchically organized explanatory details about input data and system functionality. given the inputs of individual sensor-induced forecasting signals and an overall warning from a fusion module, splain queries each signal for information on contributing sensors and data signals. this collected data is processed into a coherent english explanation, encompassing forecasting, sensing, and data elements for user review. splain's template-based approach ensures consistent warning structure and vocabulary. splain's hierarchical output structure allows each threat and its components to be expanded to reveal underlying explanations on demand. our conclusions emphasize the need for designers to specify the \"how\" and \"why\" behind cyber warnings, advocate for simple structured templates in generating consistent explanations, and recognize that direct causal links in machine learning approaches may not always be identifiable, requiring some explanations to focus on general methodologies, such as model and training data.",
        "doi": "",
        "created": "2023-11-18",
        "url": "https://arxiv.org/abs/2311.11215",
        "authors": [
            "vera a. kazakova",
            "jena d. hwang",
            "bonnie j. dorr",
            "yorick wilks",
            "j. blake gage",
            "alex memory",
            "mark a. clark"
        ]
    },
    {
        "id": "2311.11462",
        "title": "llm aided semi-supervision for extractive dialog summarization",
        "abstract": "generating high-quality summaries for chat dialogs often requires large labeled datasets. we propose a method to efficiently use unlabeled data for extractive summarization of customer-agent dialogs. in our method, we frame summarization as a question-answering problem and use state-of-the-art large language models (llms) to generate pseudo-labels for a dialog. we then use these pseudo-labels to fine-tune a chat summarization model, effectively transferring knowledge from the large llm into a smaller specialized model. we demonstrate our method on the \\tweetsumm dataset, and show that using 10% of the original labelled data set we can achieve 65.9/57.0/61.0 rouge-1/-2/-l, whereas the current state-of-the-art trained on the entire training data set obtains 65.16/55.81/64.37 rouge-1/-2/-l. in other words, in the worst case (i.e., rouge-l) we still effectively retain 94.7% of the performance while using only 10% of the data.",
        "doi": "",
        "created": "2023-11-19",
        "url": "https://arxiv.org/abs/2311.11462",
        "authors": [
            "nishant mishra",
            "gaurav sahu",
            "iacer calixto",
            "ameen abu-hanna",
            "issam h. laradji"
        ]
    },
    {
        "id": "2311.11995",
        "title": "brainwash: a poisoning attack to forget in continual learning",
        "abstract": "continual learning has gained substantial attention within the deep learning community, offering promising solutions to the challenging problem of sequential learning. yet, a largely unexplored facet of this paradigm is its susceptibility to adversarial attacks, especially with the aim of inducing forgetting. in this paper, we introduce \"brainwash,\" a novel data poisoning method tailored to impose forgetting on a continual learner. by adding the brainwash noise to a variety of baselines, we demonstrate how a trained continual learner can be induced to forget its previously learned tasks catastrophically, even when using these continual learning baselines. an important feature of our approach is that the attacker requires no access to previous tasks' data and is armed merely with the model's current parameters and the data belonging to the most recent task. our extensive experiments highlight the efficacy of brainwash, showcasing degradation in performance across various regularization-based continual learning methods.",
        "doi": "",
        "created": "2023-11-20",
        "url": "https://arxiv.org/abs/2311.11995",
        "authors": [
            "ali abbasi",
            "parsa nooralinejad",
            "hamed pirsiavash",
            "soheil kolouri"
        ]
    },
    {
        "id": "2311.12379",
        "title": "infinite forecast combinations based on dirichlet process",
        "abstract": "forecast combination integrates information from various sources by consolidating multiple forecast results from the target time series. instead of the need to select a single optimal forecasting model, this paper introduces a deep learning ensemble forecasting model based on the dirichlet process. initially, the learning rate is sampled with three basis distributions as hyperparameters to convert the infinite mixture into a finite one. all checkpoints are collected to establish a deep learning sub-model pool, and weight adjustment and diversity strategies are developed during the combination process. the main advantage of this method is its ability to generate the required base learners through a single training process, utilizing the decaying strategy to tackle the challenge posed by the stochastic nature of gradient descent in determining the optimal learning rate. to ensure the method's generalizability and competitiveness, this paper conducts an empirical analysis using the weekly dataset from the m4 competition and explores sensitivity to the number of models to be combined. the results demonstrate that the ensemble model proposed offers substantial improvements in prediction accuracy and stability compared to a single benchmark model.",
        "doi": "",
        "created": "2023-11-21",
        "url": "https://arxiv.org/abs/2311.12379",
        "authors": [
            "yinuo ren",
            "feng li",
            "yanfei kang",
            "jue wang"
        ]
    },
    {
        "id": "2311.12651",
        "title": "mobile-seed: joint semantic segmentation and boundary detection for   mobile robots",
        "abstract": "precise and rapid delineation of sharp boundaries and robust semantics is essential for numerous downstream robotic tasks, such as robot grasping and manipulation, real-time semantic mapping, and online sensor calibration performed on edge computing units. although boundary detection and semantic segmentation are complementary tasks, most studies focus on lightweight models for semantic segmentation but overlook the critical role of boundary detection. in this work, we introduce mobile-seed, a lightweight, dual-task framework tailored for simultaneous semantic segmentation and boundary detection. our framework features a two-stream encoder, an active fusion decoder (afd) and a dual-task regularization approach. the encoder is divided into two pathways: one captures category-aware semantic information, while the other discerns boundaries from multi-scale features. the afd module dynamically adapts the fusion of semantic and boundary information by learning channel-wise relationships, allowing for precise weight assignment of each channel. furthermore, we introduce a regularization loss to mitigate the conflicts in dual-task learning and deep diversity supervision. compared to existing methods, the proposed mobile-seed offers a lightweight framework to simultaneously improve semantic segmentation performance and accurately locate object boundaries. experiments on the cityscapes dataset have shown that mobile-seed achieves notable improvement over the state-of-the-art (sota) baseline by 2.2 percentage points (pp) in miou and 4.2 pp in mf-score, while maintaining an online inference speed of 23.9 frames-per-second (fps) with 1024x2048 resolution input on an rtx 2080 ti gpu. additional experiments on camvid and pascal context datasets confirm our method's generalizability. code and additional results are publicly available at https://whu-usi3dv.github.io/mobile-seed/.",
        "doi": "",
        "created": "2023-11-21",
        "url": "https://arxiv.org/abs/2311.12651",
        "authors": [
            "youqi liao",
            "shuhao kang",
            "jianping li",
            "yang liu",
            "yun liu",
            "zhen dong",
            "bisheng yang",
            "xieyuanli chen"
        ]
    },
    {
        "id": "2311.12716",
        "title": "minimax: efficient baselines for autocurricula in jax",
        "abstract": "unsupervised environment design (ued) is a form of automatic curriculum learning for training robust decision-making agents to zero-shot transfer into unseen environments. such autocurricula have received much interest from the rl community. however, ued experiments, based on cpu rollouts and gpu model updates, have often required several weeks of training. this compute requirement is a major obstacle to rapid innovation for the field. this work introduces the minimax library for ued training on accelerated hardware. using jax to implement fully-tensorized environments and autocurriculum algorithms, minimax allows the entire training loop to be compiled for hardware acceleration. to provide a petri dish for rapid experimentation, minimax includes a tensorized grid-world based on minigrid, in addition to reusable abstractions for conducting autocurricula in procedurally-generated environments. with these components, minimax provides strong ued baselines, including new parallelized variants, which achieve over 120$\\times$ speedups in wall time compared to previous implementations when training with equal batch sizes. the minimax library is available under the apache 2.0 license at https://github.com/facebookresearch/minimax.",
        "doi": "",
        "created": "2023-11-21",
        "url": "https://arxiv.org/abs/2311.12716",
        "authors": [
            "minqi jiang",
            "michael dennis",
            "edward grefenstette",
            "tim rockt\u00e4schel"
        ]
    },
    {
        "id": "2311.12856",
        "title": "density of states prediction of crystalline materials via prompt-guided   multi-modal transformer",
        "abstract": "the density of states (dos) is a spectral property of crystalline materials, which provides fundamental insights into various characteristics of the materials. while previous works mainly focus on obtaining high-quality representations of crystalline materials for dos prediction, we focus on predicting the dos from the obtained representations by reflecting the nature of dos: dos determines the general distribution of states as a function of energy. that is, dos is not solely determined by the crystalline material but also by the energy levels, which has been neglected in previous works. in this paper, we propose to integrate heterogeneous information obtained from the crystalline materials and the energies via a multi-modal transformer, thereby modeling the complex relationships between the atoms in the crystalline materials and various energy levels for dos prediction. moreover, we propose to utilize prompts to guide the model to learn the crystal structural system-specific interactions between crystalline materials and energies. extensive experiments on two types of dos, i.e., phonon dos and electron dos, with various real-world scenarios demonstrate the superiority of dostransformer. the source code for dostransformer is available at https://github.com/heewoongnoh/dostransformer.",
        "doi": "",
        "created": "2023-10-24",
        "url": "https://arxiv.org/abs/2311.12856",
        "authors": [
            "namkyeong lee",
            "heewoong noh",
            "sungwon kim",
            "dongmin hyun",
            "gyoung s. na",
            "chanyoung park"
        ]
    },
    {
        "id": "2311.13231",
        "title": "using human feedback to fine-tune diffusion models without any reward   model",
        "abstract": "using reinforcement learning with human feedback (rlhf) has shown significant promise in fine-tuning diffusion models. previous methods start by training a reward model that aligns with human preferences, then leverage rl techniques to fine-tune the underlying models. however, crafting an efficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter tuning, making the process both time and cost-intensive. the direct preference optimization (dpo) method, effective in fine-tuning large language models, eliminates the necessity for a reward model. however, the extensive gpu memory requirement of the diffusion model's denoising process hinders the direct application of the dpo method. to address this issue, we introduce the direct preference for denoising diffusion policy optimization (d3po) method to directly fine-tune diffusion models. the theoretical analysis demonstrates that although d3po omits training a reward model, it effectively functions as the optimal reward model trained using human feedback data to guide the learning process. this approach requires no training of a reward model, proving to be more direct, cost-effective, and minimizing computational overhead. in experiments, our method uses the relative scale of objectives as a proxy for human preference, delivering comparable results to methods using ground-truth rewards. moreover, d3po demonstrates the ability to reduce image distortion rates and generate safer images, overcoming challenges lacking robust reward models. our code is publicly available in https://github.com/yk7333/d3po/tree/main.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13231",
        "authors": [
            "kai yang",
            "jian tao",
            "jiafei lyu",
            "chunjiang ge",
            "jiaxin chen",
            "qimai li",
            "weihan shen",
            "xiaolong zhu",
            "xiu li"
        ]
    },
    {
        "id": "2311.13559",
        "title": "transfer learning-based real-time handgun detection",
        "abstract": "traditional surveillance systems rely on human attention, limiting their effectiveness. this study employs convolutional neural networks and transfer learning to develop a real-time computer vision system for automatic handgun detection. comprehensive analysis of online handgun detection methods is conducted, emphasizing reducing false positives and learning time. transfer learning is demonstrated as an effective approach. despite technical challenges, the proposed system achieves a precision rate of 84.74%, demonstrating promising performance comparable to related works, enabling faster learning and accurate automatic handgun detection for enhanced security. this research advances security measures by reducing human monitoring dependence, showcasing the potential of transfer learning-based approaches for efficient and reliable handgun detection.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13559",
        "authors": [
            "youssef elmir",
            "sid ahmed laouar",
            "larbi hamdaoui"
        ]
    },
    {
        "id": "2311.13614",
        "title": "hallucidoctor: mitigating hallucinatory toxicity in visual instruction   data",
        "abstract": "multi-modal large language models (mllms) tuned on machine-generated instruction-following data have demonstrated remarkable performance in various multi-modal understanding and generation tasks. however, the hallucinations inherent in machine-generated data, which could lead to hallucinatory outputs in mllms, remain under-explored. this work aims to investigate various hallucinations (i.e., object, relation, attribute hallucinations) and mitigate those hallucinatory toxicities in large-scale machine-generated visual instruction datasets. drawing on the human ability to identify factual errors, we present a novel hallucination detection and elimination framework, hallucidoctor, based on the cross-checking paradigm. we use our framework to identify and eliminate hallucinations in the training data automatically. interestingly, hallucidoctor also indicates that spurious correlations arising from long-tail object co-occurrences contribute to hallucinations. based on that, we execute counterfactual visual instruction expansion to balance data distribution, thereby enhancing mllms' resistance to hallucinations. comprehensive experiments on hallucination evaluation benchmarks show that our method successfully mitigates 44.6% hallucinations relatively and maintains competitive performance compared to llava.the source code will be released at \\url{https://github.com/yuqifan1117/hallucidoctor}.",
        "doi": "",
        "created": "2023-11-21",
        "url": "https://arxiv.org/abs/2311.13614",
        "authors": [
            "qifan yu",
            "juncheng li",
            "longhui wei",
            "liang pang",
            "wentao ye",
            "bosheng qin",
            "siliang tang",
            "qi tian",
            "yueting zhuang"
        ]
    },
    {
        "id": "2311.13626",
        "title": "physics-driven generative adversarial networks empower single-pixel   infrared hyperspectral imaging",
        "abstract": "a physics-driven generative adversarial network (gan) was established here for single-pixel hyperspectral imaging (hsi) in the infrared spectrum, to eliminate the extensive data training work required by traditional data-driven model. within the gan framework, the physical process of single-pixel imaging (spi) was integrated into the generator, and the actual and estimated one-dimensional (1d) bucket signals were employed as constraints in the objective function to update the network's parameters and optimize the generator with the assistance of the discriminator. in comparison to single-pixel infrared hsi methods based on compressed sensing and physics-driven convolution neural networks, our physics-driven gan-based single-pixel infrared hsi can achieve higher imaging performance but with fewer measurements. we believe that this physics-driven gan will promote practical applications of computational imaging, especially various spi-based techniques.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13626",
        "authors": [
            "dong-yin wang",
            "shu-hang bie",
            "xi-hao chen",
            "wen-kai yu"
        ]
    },
    {
        "id": "2311.13627",
        "title": "vamos: versatile action models for video understanding",
        "abstract": "what makes good video representations for video understanding, such as anticipating future activities, or answering video-conditioned questions? while earlier approaches focus on end-to-end learning directly from video pixels, we propose to revisit text-based representations, such as discrete action labels, or free-form video captions, which are interpretable and can be directly consumed by large language models (llms). intuitively, different video understanding tasks may require representations that are complementary and at different granularities. to this end, we propose versatile action models (vamos), a learning framework powered by a large language model as the \"reasoner\", and can flexibly leverage visual embeddings, action labels, and free-form descriptions extracted from videos as its input. we evaluate vamos on four complementary video understanding benchmarks, ego4d, next-qa, intentqa, and egoschema, on its capability to model temporal dynamics, encode visual history, and perform reasoning. surprisingly, we observe that text-based representations consistently achieve competitive performance on all benchmarks, and that visual embeddings provide marginal or no performance improvement, demonstrating the effectiveness of text-based video representation in the llm era. we perform extensive ablation study and qualitative analysis to support our observations, and achieve state-of-the-art performance on three benchmarks.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13627",
        "authors": [
            "shijie wang",
            "qi zhao",
            "minh quan do",
            "nakul agarwal",
            "kwonjoon lee",
            "chen sun"
        ]
    },
    {
        "id": "2311.13628",
        "title": "prompt risk control: a rigorous framework for responsible deployment of   large language models",
        "abstract": "the recent explosion in the capabilities of large language models has led to a wave of interest in how best to prompt a model to perform a given task. while it may be tempting to simply choose a prompt based on average performance on a validation set, this can lead to a deployment where unexpectedly poor responses are generated, especially for the worst-off users. to mitigate this prospect, we propose prompt risk control, a lightweight framework for selecting a prompt based on rigorous upper bounds on families of informative risk measures. we offer methods for producing bounds on a diverse set of metrics, including quantities that measure worst-case responses and disparities in generation quality across the population of users. in addition, we extend the underlying statistical bounding techniques to accommodate the possibility of distribution shifts in deployment. experiments on applications such as open-ended chat, medical question summarization, and code generation highlight how such a framework can foster responsible deployment by reducing the risk of the worst outcomes.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13628",
        "authors": [
            "thomas p. zollo",
            "todd morrill",
            "zhun deng",
            "jake c. snell",
            "toniann pitassi",
            "richard zemel"
        ]
    },
    {
        "id": "2311.13664",
        "title": "sample as you infer: predictive coding with langevin dynamics",
        "abstract": "we present a novel algorithm for parameter learning in generic deep generative models that builds upon the predictive coding (pc) framework of computational neuroscience. our approach modifies the standard pc algorithm to bring performance on-par and exceeding that obtained from standard variational auto-encoder (vae) training. by injecting gaussian noise into the pc inference procedure we re-envision it as an overdamped langevin sampling, which facilitates optimisation with respect to a tight evidence lower bound (elbo). we improve the resultant encoder-free training method by incorporating an encoder network to provide an amortised warm-start to our langevin sampling and test three different objectives for doing so. finally, to increase robustness to the sampling step size and reduce sensitivity to curvature, we validate a lightweight and easily computable form of preconditioning, inspired by riemann manifold langevin and adaptive optimizers from the sgd literature. we compare against vaes by training like-for-like generative models using our technique against those trained with standard reparameterisation-trick-based elbos. we observe our method out-performs or matches performance across a number of metrics, including sample quality, while converging in a fraction of the number of sgd training iterations.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13664",
        "authors": [
            "umais zahid",
            "qinghai guo",
            "zafeirios fountas"
        ]
    },
    {
        "id": "2311.13668",
        "title": "maira-1: a specialised large multimodal model for radiology report   generation",
        "abstract": "we present a radiology-specific multimodal model for the task for generating radiological reports from chest x-rays (cxrs). our work builds on the idea that large language model(s) can be equipped with multimodal capabilities through alignment with pre-trained vision encoders. on natural images, this has been shown to allow multimodal models to gain image understanding and description capabilities. our proposed model (maira-1) leverages a cxr-specific image encoder in conjunction with a fine-tuned large language model based on vicuna-7b, and text-based data augmentation, to produce reports with state-of-the-art quality. in particular, maira-1 significantly improves on the radiologist-aligned radcliq metric and across all lexical metrics considered. manual review of model outputs demonstrates promising fluency and accuracy of generated reports while uncovering failure modes not captured by existing evaluation practices. more information and resources can be found on the project website: https://aka.ms/maira.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13668",
        "authors": [
            "stephanie l. hyland",
            "shruthi bannur",
            "kenza bouzid",
            "daniel c. castro",
            "mercy ranjit",
            "anton schwaighofer",
            "fernando p\u00e9rez-garc\u00eda",
            "valentina salvatelli",
            "shaury srivastav",
            "anja thieme",
            "noel codella",
            "matthew p. lungren",
            "maria teodora wetscherek",
            "ozan oktay",
            "javier alvarez-valle"
        ]
    },
    {
        "id": "2311.13691",
        "title": "next-generation earth system models: towards reliable hybrid models for   weather and climate applications",
        "abstract": "we review how machine learning has transformed our ability to model the earth system, and how we expect recent breakthroughs to benefit end-users in switzerland in the near future.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13691",
        "authors": [
            "tom beucler",
            "erwan koch",
            "sven kotlarski",
            "david leutwyler",
            "adrien michel",
            "jonathan koh"
        ]
    },
    {
        "id": "2311.13693",
        "title": "scalable cp decomposition for tensor learning using gpu tensor cores",
        "abstract": "cp decomposition is a powerful tool for data science, especially gene analysis, deep learning, and quantum computation. however, the application of tensor decomposition is largely hindered by the exponential increment of the computational complexity and storage consumption with the size of tensors. while the data in our real world is usually presented as trillion- or even exascale-scale tensors, existing work can only support billion-scale scale tensors. in our work, we propose the exascale-tensor to mitigate the significant gap. specifically, we propose a compression-based tensor decomposition framework, namely the exascale-tensor, to support exascale tensor decomposition. then, we carefully analyze the inherent parallelism and propose a bag of strategies to improve computational efficiency. last, we conduct experiments to decompose tensors ranging from million-scale to trillion-scale for evaluation. compared to the baselines, the exascale-tensor supports 8,000x larger tensors and a speedup up to 6.95x. we also apply our method to two real-world applications, including gene analysis and tensor layer neural networks, of which the numeric results demonstrate the scalability and effectiveness of our method.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13693",
        "authors": [
            "zeliang zhang",
            "zhuo liu",
            "susan liang",
            "zhiyuan wang",
            "yifan zhu",
            "chen ding",
            "chenliang xu"
        ]
    },
    {
        "id": "2311.13712",
        "title": "data acquisition: a new frontier in data-centric ai",
        "abstract": "as machine learning (ml) systems continue to grow, the demand for relevant and comprehensive datasets becomes imperative. there is limited study on the challenges of data acquisition due to ad-hoc processes and lack of consistent methodologies. we first present an investigation of current data marketplaces, revealing lack of platforms offering detailed information about datasets, transparent pricing, standardized data formats. with the objective of inciting participation from the data-centric ai community, we then introduce the dam challenge, a benchmark to model the interaction between the data providers and acquirers. the benchmark was released as a part of dataperf. our evaluation of the submitted strategies underlines the need for effective data acquisition strategies in ml.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13712",
        "authors": [
            "lingjiao chen",
            "bilge acun",
            "newsha ardalani",
            "yifan sun",
            "feiyang kang",
            "hanrui lyu",
            "yongchan kwon",
            "ruoxi jia",
            "carole-jean wu",
            "matei zaharia",
            "james zou"
        ]
    },
    {
        "id": "2311.13718",
        "title": "a unified approach to count-based weakly-supervised learning",
        "abstract": "high-quality labels are often very scarce, whereas unlabeled data with inferred weak labels occurs more naturally. in many cases, these weak labels dictate the frequency of each respective class over a set of instances. in this paper, we develop a unified approach to learning from such weakly-labeled data, which we call count-based weakly-supervised learning. at the heart of our approach is the ability to compute the probability of exactly k out of n outputs being set to true. this computation is differentiable, exact, and efficient. building upon the previous computation, we derive a count loss penalizing the model for deviations in its distribution from an arithmetic constraint defined over label counts. we evaluate our approach on three common weakly-supervised learning paradigms and observe that our proposed approach achieves state-of-the-art or highly competitive results across all three of the paradigms.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13718",
        "authors": [
            "vinay shukla",
            "zhe zeng",
            "kareem ahmed",
            "guy van den broeck"
        ]
    },
    {
        "id": "2311.13719",
        "title": "deep learning-based instance segmentation for the precise automated   quantification of digital breast cancer immunohistochemistry images",
        "abstract": "the quantification of biomarkers on immunohistochemistry breast cancer images is essential for defining appropriate therapy for breast cancer patients, as well as for extracting relevant information on disease prognosis. this is an arduous and time-consuming task that may introduce a bias in the results due to intra- and inter-observer variability which could be alleviated by making use of automatic quantification tools. however, this is not a simple processing task given the heterogeneity of breast tumors that results in non-uniformly distributed tumor cells exhibiting different staining colors and intensity, size, shape, and texture, of the nucleus, cytoplasm and membrane. in this research work, we demonstrate the feasibility of using a deep learning-based instance segmentation architecture for the automatic quantification of both nuclear and membrane biomarkers applied to ihc-stained slides. we have solved the cumbersome task of training set generation with the design and implementation of a web platform, which has served as a hub for communication and feedback between researchers and pathologists as well as a system for the validation of the automatic image processing models. through this tool, we have collected annotations over samples of he, er and ki-67 (nuclear biomarkers) and her2 (membrane biomarker) ihc-stained images. using the same deep learning network architecture, we have trained two models, so-called nuclei- and membrane-aware segmentation models, which, once successfully validated, have revealed to be a promising method to segment nuclei instances in ihc-stained images. the quantification method proposed in this work has been integrated into the developed web platform and is currently being used as a decision-support tool by pathologists.",
        "doi": "10.1016/j.eswa.2021.116471",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13719",
        "authors": [
            "blanca maria priego-torresa",
            "barbara lobato-delgado",
            "lidia atienza-cuevas",
            "daniel sanchez-morillo"
        ]
    },
    {
        "id": "2311.13720",
        "title": "towards more likely models for ai planning",
        "abstract": "this is the first work to look at the application of large language models (llms) for the purpose of model space edits in automated planning tasks. to set the stage for this sangam, we explore two different flavors of model space problems that have been studied in the ai planning literature and explore the effect of an llm on those tasks. we empirically demonstrate how the performance of an llm contrasts with combinatorial search (cs) - an approach that has been traditionally used to solve model space tasks in planning, both with the llm in the role of a standalone model space reasoner as well as in the role of a statistical signal in concert with the cs approach as part of a two-stage process. our experiments show promising results suggesting further forays of llms into the exciting world of model space reasoning for planning tasks in the future.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13720",
        "authors": [
            "turgay caglar",
            "sirine belhaj",
            "tathagata chakraborti",
            "michael katz",
            "sarath sreedharan"
        ]
    },
    {
        "id": "2311.13725",
        "title": "studying artist sentiments around ai-generated artwork",
        "abstract": "art created using generated artificial intelligence has taken the world by storm and generated excitement for many digital creators and technologists. however, the reception and reaction from artists have been mixed. concerns about plagiarizing their artworks and styles for datasets and uncertainty around the future of digital art sparked movements in artist communities shunning the use of ai for generating art and protecting artists' rights. collaborating with these tools for novel creative use cases also sparked hope from some creators. artists are an integral stakeholder in the rapidly evolving digital creativity industry and understanding their concerns and hopes inform responsible development and use of creativity support tools. in this work, we study artists' sentiments about ai-generated art. we interviewed 7 artists and analyzed public posts from artists on social media platforms reddit, twitter and artstation. we report artists' main concerns and hopes around ai-generated artwork, informing a way forward for inclusive development of these tools.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13725",
        "authors": [
            "safinah ali",
            "cynthia breazeal"
        ]
    },
    {
        "id": "2311.13731",
        "title": "a survey of blockchain, artificial intelligence, and edge computing for   web 3.0",
        "abstract": "web 3.0, as the third generation of the world wide web, aims to solve contemporary problems of trust, centralization, and data ownership. driven by the latest advances in cutting-edge technologies, web 3.0 is moving towards a more open, decentralized, intelligent, and interconnected network. however, increasingly widespread data breaches have raised awareness of online privacy and security of personal data. additionally, since web 3.0 is a sophisticated and complex convergence, the technical details behind it are not as clear as the characteristics it presents. in this survey, we conduct an in-depth exploration of web 3.0 from the perspectives of blockchain, artificial intelligence, and edge computing. specifically, we begin with summarizing the evolution of the internet and providing an overview of these three key technological factors. afterward, we provide a thorough analysis of each technology separately, including its relevance to web 3.0, key technology components, and practical applications. we also propose decentralized storage and computing solutions by exploring the integration of technologies. finally, we highlight the key challenges alongside potential research directions. through the combination and mutual complementation of multiple technologies, web 3.0 is expected to return more control and ownership of data and digital assets back to users.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13731",
        "authors": [
            "jianjun zhu",
            "fan li",
            "jinyuan chen"
        ]
    },
    {
        "id": "2311.13739",
        "title": "oasis: offsetting active reconstruction attacks in federated learning",
        "abstract": "federated learning (fl) has garnered significant attention for its potential to protect user privacy while enhancing model training efficiency. however, recent research has demonstrated that fl protocols can be easily compromised by active reconstruction attacks executed by dishonest servers. these attacks involve the malicious modification of global model parameters, allowing the server to obtain a verbatim copy of users' private data by inverting their gradient updates. tackling this class of attack remains a crucial challenge due to the strong threat model. in this paper, we propose oasis, a defense mechanism based on image augmentation that effectively counteracts active reconstruction attacks while preserving model performance. we first uncover the core principle of gradient inversion that enables these attacks and theoretically identify the main conditions by which the defense can be robust regardless of the attack strategies. we then construct oasis with image augmentation showing that it can undermine the attack principle. comprehensive evaluations demonstrate the efficacy of oasis highlighting its feasibility as a solution.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13739",
        "authors": [
            "tre' r. jeter",
            "truc nguyen",
            "raed alharbi",
            "my t. thai"
        ]
    },
    {
        "id": "2311.13743",
        "title": "finme: a performance-enhanced large language model trading agent with   layered memory and character design",
        "abstract": "recent advancements in large language models (llms) have exhibited notable efficacy in question-answering (qa) tasks across diverse domains. their prowess in integrating extensive web knowledge has fueled interest in developing llm autonomous agents. while llms are efficient in decoding human instructions and deriving solutions by holistically processing historical inputs, transitioning to purpose-driven agents requires a supplementary rational architecture to process multi-source information, establish reasoning chains, and prioritize critical tasks. addressing this, we introduce \\textsc{finme}, a novel llm-based agent framework devised for financial decision-making, encompassing three core modules: profiling, to outline the agent's characteristics; memory, with layered processing, to aid the agent in assimilating realistic hierarchical financial data; and decision-making, to convert insights gained from memories into investment decisions. notably, \\textsc{finme}'s memory module aligns closely with the cognitive structure of human traders, offering robust interpretability and real-time tuning. its adjustable cognitive span allows for the retention of critical information beyond human perceptual limits, thereby enhancing trading outcomes. this framework enables the agent to self-evolve its professional knowledge, react agilely to new investment cues, and continuously refine trading decisions in the volatile financial environment. we first compare \\textsc{finme} with various algorithmic agents on a scalable real-world financial dataset, underscoring its leading trading performance in stocks and funds. we then fine-tuned the agent's perceptual spans to achieve a significant trading performance. collectively, \\textsc{finme} presents a cutting-edge llm agent framework for automated trading, boosting cumulative investment returns.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13743",
        "authors": [
            "yangyang yu",
            "haohang li",
            "zhi chen",
            "yuechen jiang",
            "yang li",
            "denghui zhang",
            "rong liu",
            "jordan w. suchow",
            "khaldoun khashanah"
        ]
    },
    {
        "id": "2311.13744",
        "title": "security and privacy challenges in deep learning models",
        "abstract": "these days, deep learning models have achieved great success in multiple fields, from autonomous driving to medical diagnosis. these models have expanded the abilities of artificial intelligence by offering great solutions to complex problems that were very difficult to solve earlier. in spite of their unseen success in various, it has been identified, through research conducted, that deep learning models can be subjected to various attacks that compromise model security and data privacy of the deep neural network models. deep learning models can be subjected to various attacks at different stages of their lifecycle. during the testing phase, attackers can exploit vulnerabilities through different kinds of attacks such as model extraction attacks, model inversion attacks, and adversarial attacks. model extraction attacks are aimed at reverse-engineering a trained deep learning model, with the primary objective of revealing its architecture and parameters. model inversion attacks aim to compromise the privacy of the data used in the deep learning model. these attacks are done to compromise the confidentiality of the model by going through the sensitive training data from the model's predictions. by analyzing the model's responses, attackers aim to reconstruct sensitive information. in this way, the model's data privacy is compromised. adversarial attacks, mainly employed on computer vision models, are made to corrupt models into confidently making incorrect predictions through malicious testing data. these attacks subtly alter the input data, making it look normal but misleading deep learning models to make incorrect decisions. such attacks can happen during both the model's evaluation and training phases. data poisoning attacks add harmful data to the training set, disrupting the learning process and reducing the reliability of the deep learning mode.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13744",
        "authors": [
            "gopichandh golla"
        ]
    },
    {
        "id": "2311.13750",
        "title": "towards transferable multi-modal perception representation learning for   autonomy: nerf-supervised masked autoencoder",
        "abstract": "this work proposes a unified self-supervised pre-training framework for transferable multi-modal perception representation learning via masked multi-modal reconstruction in neural radiance field (nerf), namely nerf-supervised masked autoencoder (ns-mae). specifically, conditioned on certain view directions and locations, multi-modal embeddings extracted from corrupted multi-modal input signals, i.e., lidar point clouds and images, are rendered into projected multi-modal feature maps via neural rendering. then, original multi-modal signals serve as reconstruction targets for the rendered multi-modal feature maps to enable self-supervised representation learning. extensive experiments show that the representation learned via ns-mae shows promising transferability for diverse multi-modal and single-modal (camera-only and lidar-only) perception models on diverse 3d perception downstream tasks (3d object detection and bev map segmentation) with diverse amounts of fine-tuning labeled data. moreover, we empirically find that ns-mae enjoys the synergy of both the mechanism of masked autoencoder and neural radiance field. our code shall be released upon acceptance.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13750",
        "authors": [
            "xiaohao xu"
        ]
    },
    {
        "id": "2311.13752",
        "title": "3d-mir: a benchmark and empirical study on 3d medical image retrieval in   radiology",
        "abstract": "the increasing use of medical imaging in healthcare settings presents a significant challenge due to the increasing workload for radiologists, yet it also offers opportunity for enhancing healthcare outcomes if effectively leveraged. 3d image retrieval holds potential to reduce radiologist workloads by enabling clinicians to efficiently search through diagnostically similar or otherwise relevant cases, resulting in faster and more precise diagnoses. however, the field of 3d medical image retrieval is still emerging, lacking established evaluation benchmarks, comprehensive datasets, and thorough studies. this paper attempts to bridge this gap by introducing a novel benchmark for 3d medical image retrieval (3d-mir) that encompasses four different anatomies imaged with computed tomography. using this benchmark, we explore a diverse set of search strategies that use aggregated 2d slices, 3d volumes, and multi-modal embeddings from popular multi-modal foundation models as queries. quantitative and qualitative assessments of each approach are provided alongside an in-depth discussion that offers insight for future research. to promote the advancement of this field, our benchmark, dataset, and code are made publicly available.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13752",
        "authors": [
            "asma ben abacha",
            "alberto santamaria-pang",
            "ho hin lee",
            "jameson merkow",
            "qin cai",
            "surya teja devarakonda",
            "abdullah islam",
            "julia gong",
            "matthew p. lungren",
            "thomas lin",
            "noel c codella",
            "ivan tarapov"
        ]
    },
    {
        "id": "2311.13782",
        "title": "scalable ai generative content for vehicular network semantic   communication",
        "abstract": "perceiving vehicles in a driver's blind spot is vital for safe driving. the detection of potentially dangerous vehicles in these blind spots can benefit from vehicular network semantic communication technology. however, efficient semantic communication involves a trade-off between accuracy and delay, especially in bandwidth-limited situations. this paper unveils a scalable artificial intelligence generated content (aigc) system that leverages an encoder-decoder architecture. this system converts images into textual representations and reconstructs them into quality-acceptable images, optimizing transmission for vehicular network semantic communication. moreover, when bandwidth allows, auxiliary information is integrated. the encoder-decoder aims to maintain semantic equivalence with the original images across various tasks. then the proposed approach employs reinforcement learning to enhance the reliability of the generated contents. experimental results suggest that the proposed method surpasses the baseline in perceiving vehicles in blind spots and effectively compresses communication data. while this method is specifically designed for driving scenarios, this encoder-decoder architecture also holds potential for wide use across various semantic communication scenarios.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13782",
        "authors": [
            "hao feng",
            "yi yang",
            "zhu han"
        ]
    },
    {
        "id": "2311.13800",
        "title": "enhancing intrusion detection in internet of vehicles through federated   learning",
        "abstract": "federated learning is a technique of decentralized machine learning. that allows multiple parties to collaborate and learn a shared model without sharing their raw data. our paper proposes a federated learning framework for intrusion detection in internet of vehicles (iovs) using the cic-ids 2017 dataset. the proposed framework employs smote for handling class imbalance, outlier detection for identifying and removing abnormal observations, and hyperparameter tuning to optimize the model's performance. the authors evaluated the proposed framework using various performance metrics and demonstrated its effectiveness in detecting intrusions with other datasets (kdd-cup 99 and unsw- nb-15) and conventional classifiers. furthermore, the proposed framework can protect sensitive data while achieving high intrusion detection performance.",
        "doi": "",
        "created": "2023-11-22",
        "url": "https://arxiv.org/abs/2311.13800",
        "authors": [
            "abhishek sebastian",
            "pragna r",
            "sudhakaran g",
            "renjith p n",
            "leela karthikeyan h"
        ]
    },
    {
        "id": "2311.13810",
        "title": "bridging classical and quantum machine learning: knowledge transfer from   classical to quantum neural networks using knowledge distillation",
        "abstract": "very recently, studies have shown that quantum neural networks surpass classical neural networks in tasks like image classification when a similar number of learnable parameters are used. however, the development and optimization of quantum models are currently hindered by issues such as qubit instability and limited qubit availability, leading to error-prone systems with weak performance. in contrast, classical models can exhibit high-performance owing to substantial resource availability. as a result, more studies have been focusing on hybrid classical-quantum integration. a line of research particularly focuses on transfer learning through classical-quantum integration or quantum-quantum approaches. unlike previous studies, this paper introduces a new method to transfer knowledge from classical to quantum neural networks using knowledge distillation, effectively bridging the gap between classical machine learning and emergent quantum computing techniques. we adapt classical convolutional neural network (cnn) architectures like lenet and alexnet to serve as teacher networks, facilitating the training of student quantum models by sending supervisory signals during backpropagation through kl-divergence. the approach yields significant performance improvements for the quantum models by solely depending on classical cnns, with quantum models achieving an average accuracy improvement of 0.80% on the mnist dataset and 5.40% on the more complex fashion mnist dataset. applying this technique eliminates the cumbersome training of huge quantum models for transfer learning in resource-constrained settings and enables re-using existing pre-trained classical models to improve performance.thus, this study paves the way for future research in quantum machine learning (qml) by positioning knowledge distillation as a core technique for advancing qml applications.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13810",
        "authors": [
            "mohammad junayed hasan",
            "m. r. c. mahdy"
        ]
    },
    {
        "id": "2311.13812",
        "title": "mechanical characterization and inverse design of stochastic architected   metamaterials using neural operators",
        "abstract": "machine learning (ml) is emerging as a transformative tool for the design of architected materials, offering properties that far surpass those achievable through lab-based trial-and-error methods. however, a major challenge in current inverse design strategies is their reliance on extensive computational and/or experimental datasets, which becomes particularly problematic for designing micro-scale stochastic architected materials that exhibit nonlinear mechanical behaviors. here, we introduce a new end-to-end scientific ml framework, leveraging deep neural operators (deeponet), to directly learn the relationship between the complete microstructure and mechanical response of architected metamaterials from sparse but high-quality in situ experimental data. the approach facilitates the inverse design of structures tailored to specific nonlinear mechanical behaviors. results obtained from spinodal microstructures, printed using two-photon lithography, reveal that the prediction error for mechanical responses is within a range of 5 - 10%. our work underscores that by employing neural operators with advanced micro-mechanics experimental techniques, the design of complex micro-architected materials with desired properties becomes feasible, even in scenarios constrained by data scarcity. our work marks a significant advancement in the field of materials-by-design, potentially heralding a new era in the discovery and development of next-generation metamaterials with unparalleled mechanical characteristics derived directly from experimental insights.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13812",
        "authors": [
            "hanxun jin",
            "enrui zhang",
            "boyu zhang",
            "sridhar krishnaswamy",
            "george em karniadakis",
            "horacio d. espinosa"
        ]
    },
    {
        "id": "2311.13816",
        "title": "fairness-aware domain generalization under covariate and dependence   shifts",
        "abstract": "achieving the generalization of an invariant classifier from source domains to shifted target domains while simultaneously considering model fairness is a substantial and complex challenge in machine learning. existing domain generalization research typically attributes domain shifts to concept shift, which relates to alterations in class labels, and covariate shift, which pertains to variations in data styles. in this paper, by introducing another form of distribution shift, known as dependence shift, which involves variations in fair dependence patterns across domains, we propose a novel domain generalization approach that addresses domain shifts by considering both covariate and dependence shifts. we assert the existence of an underlying transformation model can transform data from one domain to another. by generating data in synthetic domains through the model, a fairness-aware invariant classifier is learned that enforces both model accuracy and fairness in unseen domains. extensive empirical studies on four benchmark datasets demonstrate that our approach surpasses state-of-the-art methods.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13816",
        "authors": [
            "chen zhao",
            "kai jiang",
            "xintao wu",
            "haoliang wang",
            "latifur khan",
            "christan grant",
            "feng chen"
        ]
    },
    {
        "id": "2311.13821",
        "title": "hypuc: hyperfine uncertainty calibration with gradient-boosted   corrections for reliable regression on imbalanced electrocardiograms",
        "abstract": "the automated analysis of medical time series, such as the electrocardiogram (ecg), electroencephalogram (eeg), pulse oximetry, etc, has the potential to serve as a valuable tool for diagnostic decisions, allowing for remote monitoring of patients and more efficient use of expensive and time-consuming medical procedures. deep neural networks (dnns) have been demonstrated to process such signals effectively. however, previous research has primarily focused on classifying medical time series rather than attempting to regress the continuous-valued physiological parameters central to diagnosis. one significant challenge in this regard is the imbalanced nature of the dataset, as a low prevalence of abnormal conditions can lead to heavily skewed data that results in inaccurate predictions and a lack of certainty in such predictions when deployed. to address these challenges, we propose hypuc, a framework for imbalanced probabilistic regression in medical time series, making several contributions. (i) we introduce a simple kernel density-based technique to tackle the imbalanced regression problem with medical time series. (ii) moreover, we employ a probabilistic regression framework that allows uncertainty estimation for the predicted continuous values. (iii) we also present a new approach to calibrate the predicted uncertainty further. (iv) finally, we demonstrate a technique to use calibrated uncertainty estimates to improve the predicted continuous value and show the efficacy of the calibrated uncertainty estimates to flag unreliable predictions. hypuc is evaluated on a large, diverse, real-world dataset of ecgs collected from millions of patients, outperforming several conventional baselines on various diagnostic tasks, suggesting a potential use-case for the reliable clinical deployment of deep learning models.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13821",
        "authors": [
            "uddeshya upadhyay",
            "sairam bade",
            "arjun puranik",
            "shahir asfahan",
            "melwin babu",
            "francisco lopez-jimenez",
            "samuel j. asirvatham",
            "ashim prasad",
            "ajit rajasekharan",
            "samir awasthi",
            "rakesh barve"
        ]
    },
    {
        "id": "2311.13843",
        "title": "exact combinatorial optimization with temporo-attentional graph neural   networks",
        "abstract": "combinatorial optimization finds an optimal solution within a discrete set of variables and constraints. the field has seen tremendous progress both in research and industry. with the success of deep learning in the past decade, a recent trend in combinatorial optimization has been to improve state-of-the-art combinatorial optimization solvers by replacing key heuristic components with machine learning (ml) models. in this paper, we investigate two essential aspects of machine learning algorithms for combinatorial optimization: temporal characteristics and attention. we argue that for the task of variable selection in the branch-and-bound (b&b) algorithm, incorporating the temporal information as well as the bipartite graph attention improves the solver's performance. we support our claims with intuitions and numerical results over several standard datasets used in the literature and competitions. code is available at: https://developer.huaweicloud.com/develop/aigallery/notebook/detail?id=047c6cf2-8463-40d7-b92f-7b2ca998e935",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13843",
        "authors": [
            "mehdi seyfi",
            "amin banitalebi-dehkordi",
            "zirui zhou",
            "yong zhang"
        ]
    },
    {
        "id": "2311.13845",
        "title": "touring sampling with pushforward maps",
        "abstract": "the number of sampling methods could be daunting for a practitioner looking to cast powerful machine learning methods to their specific problem. this paper takes a theoretical stance to review and organize many sampling approaches in the ``generative modeling'' setting, where one wants to generate new data that are similar to some training examples. by revealing links between existing methods, it might prove useful to overcome some of the current challenges in sampling with diffusion models, such as long inference time due to diffusion simulation, or the lack of diversity in generated samples.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13845",
        "authors": [
            "vivien cabannes",
            "charles arnal"
        ]
    },
    {
        "id": "2311.13852",
        "title": "a cross attention approach to diagnostic explainability using clinical   practice guidelines for depression",
        "abstract": "the lack of explainability using relevant clinical knowledge hinders the adoption of artificial intelligence-powered analysis of unstructured clinical dialogue. a wealth of relevant, untapped mental health (mh) data is available in online communities, providing the opportunity to address the explainability problem with substantial potential impact as a screening tool for both online and offline applications. we develop a method to enhance attention in popular transformer models and generate clinician-understandable explanations for classification by incorporating external clinical knowledge. inspired by how clinicians rely on their expertise when interacting with patients, we leverage relevant clinical knowledge to model patient inputs, providing meaningful explanations for classification. this will save manual review time and engender trust. we develop such a system in the context of mh using clinical practice guidelines (cpg) for diagnosing depression, a mental health disorder of global concern. we propose an application-specific language model called process knowledge-infused cross attention (psat), which incorporates cpgs when computing attention. through rigorous evaluation on three expert-curated datasets related to depression, we demonstrate application-relevant explainability of psat. psat also surpasses the performance of nine baseline models and can provide explanations where other baselines fall short. we transform a cpg resource focused on depression, such as the patient health questionnaire (e.g. phq-9) and related questions, into a machine-readable ontology using snomed-ct. with this resource, psat enhances the ability of models like gpt-3.5 to generate application-relevant explanations.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13852",
        "authors": [
            "sumit dalal",
            "deepa tilwani",
            "manas gaur",
            "sarika jain",
            "valerie shalin",
            "amit seth"
        ]
    },
    {
        "id": "2311.13857",
        "title": "challenges of large language models for mental health counseling",
        "abstract": "the global mental health crisis is looming with a rapid increase in mental disorders, limited resources, and the social stigma of seeking treatment. as the field of artificial intelligence (ai) has witnessed significant advancements in recent years, large language models (llms) capable of understanding and generating human-like text may be used in supporting or providing psychological counseling. however, the application of llms in the mental health domain raises concerns regarding the accuracy, effectiveness, and reliability of the information provided. this paper investigates the major challenges associated with the development of llms for psychological counseling, including model hallucination, interpretability, bias, privacy, and clinical effectiveness. we explore potential solutions to these challenges that are practical and applicable to the current paradigm of ai. from our experience in developing and deploying llms for mental health, ai holds a great promise for improving mental health care, if we can carefully navigate and overcome pitfalls of llms.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13857",
        "authors": [
            "neo christopher chung",
            "george dyer",
            "lennart brocki"
        ]
    },
    {
        "id": "2311.13871",
        "title": "legal requirements analysis",
        "abstract": "modern software has been an integral part of everyday activities in many disciplines and application contexts. introducing intelligent automation by leveraging artificial intelligence (ai) led to break-throughs in many fields. the effectiveness of ai can be attributed to several factors, among which is the increasing availability of data. regulations such as the general data protection regulation (gdpr) in the european union (eu) are introduced to ensure the protection of personal data. software systems that collect, process, or share personal data are subject to compliance with such regulations. developing compliant software depends heavily on addressing legal requirements stipulated in applicable regulations, a central activity in the requirements engineering (re) phase of the software development process. re is concerned with specifying and maintaining requirements of a system-to-be, including legal requirements. legal agreements which describe the policies organizations implement for processing personal data can provide an additional source to regulations for eliciting legal requirements. in this chapter, we explore a variety of methods for analyzing legal requirements and exemplify them on gdpr. specifically, we describe possible alternatives for creating machine-analyzable representations from regulations, survey the existing automated means for enabling compliance verification against regulations, and further reflect on the current challenges of legal requirements analysis.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13871",
        "authors": [
            "sallam abualhaija",
            "marcello ceci",
            "lionel briand"
        ]
    },
    {
        "id": "2311.13878",
        "title": "minimizing factual inconsistency and hallucination in large language   models",
        "abstract": "large language models (llms) are widely used in critical fields such as healthcare, education, and finance due to their remarkable proficiency in various language-related tasks. however, llms are prone to generating factually incorrect responses or \"hallucinations,\" which can lead to a loss of credibility and trust among users. to address this issue, we propose a multi-stage framework that generates the rationale first, verifies and refines incorrect ones, and uses them as supporting references to generate the answer. the generated rationale enhances the transparency of the answer and our framework provides insights into how the model arrived at this answer, by using this rationale and the references to the context. in this paper, we demonstrate its effectiveness in improving the quality of responses to drug-related inquiries in the life sciences industry. our framework improves traditional retrieval augmented generation (rag) by enabling openai gpt-3.5-turbo to be 14-25% more faithful and 16-22% more accurate on two datasets. furthermore, fine-tuning samples based on our framework improves the accuracy of smaller open-access llms by 33-42% and competes with rag on commercial models.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13878",
        "authors": [
            "muneeswaran i",
            "shreya saxena",
            "siva prasad",
            "m v sai prakash",
            "advaith shankar",
            "varun v",
            "vishal vaddina",
            "saisubramaniam gopalakrishnan"
        ]
    },
    {
        "id": "2311.13881",
        "title": "a multi-solution study on gdpr ai-enabled completeness checking of dpas",
        "abstract": "specifying legal requirements for software systems to ensure their compliance with the applicable regulations is a major concern to requirements engineering (re). personal data which is collected by an organization is often shared with other organizations to perform certain processing activities. in such cases, the general data protection regulation (gdpr) requires issuing a data processing agreement (dpa) which regulates the processing and further ensures that personal data remains protected. violating gdpr can lead to huge fines reaching to billions of euros. software systems involving personal data processing must adhere to the legal obligations stipulated in gdpr and outlined in dpas. requirements engineers can elicit from dpas legal requirements for regulating the data processing activities in software systems. checking the completeness of a dpa according to the gdpr provisions is therefore an essential prerequisite to ensure that the elicited requirements are complete. analyzing dpas entirely manually is time consuming and requires adequate legal expertise. in this paper, we propose an automation strategy to address the completeness checking of dpas against gdpr. specifically, we pursue ten alternative solutions which are enabled by different technologies, namely traditional machine learning, deep learning, language modeling, and few-shot learning. the goal of our work is to empirically examine how these different technologies fare in the legal domain. we computed f2 score on a set of 30 real dpas. our evaluation shows that best-performing solutions yield f2 score of 86.7% and 89.7% are based on pre-trained bert and roberta language models. our analysis further shows that other alternative solutions based on deep learning (e.g., bilstm) and few-shot learning (e.g., setfit) can achieve comparable accuracy, yet are more efficient to develop.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13881",
        "authors": [
            "muhammad ilyas azeem",
            "sallam abualhaija"
        ]
    },
    {
        "id": "2311.13884",
        "title": "controlling large language model-based agents for large-scale   decision-making: an actor-critic approach",
        "abstract": "the significant advancements in large language models (llms) have presented novel opportunities for tackling planning and decision-making within multi-agent systems. however, as the number of agents increases, the issues of hallucination in llms and coordination in multi-agent systems (mas) have become increasingly pronounced. additionally, the efficient utilization of tokens becomes a critical consideration when employing llms to facilitate the interactions of large numbers of agents. in this paper, we present a novel framework aimed at enhancing coordination and decision-making capabilities of llms within large-scale multi-agent environments. our approach draws inspiration from the actor-critic framework employed in multi-agent reinforcement learning, and we develop a modular and token-efficient solution that effectively addresses challenges presented by llms and mas. through evaluations conducted in experiments involving system resource allocation and robot grid transportation, we demonstrate the considerable advantages afforded by our proposed approach.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13884",
        "authors": [
            "bin zhang",
            "hangyu mao",
            "jingqing ruan",
            "ying wen",
            "yang li",
            "shao zhang",
            "zhiwei xu",
            "dapeng li",
            "ziyue li",
            "rui zhao",
            "lijuan li",
            "guoliang fan"
        ]
    },
    {
        "id": "2311.13885",
        "title": "can physics informed neural operators self improve?",
        "abstract": "self-training techniques have shown remarkable value across many deep learning models and tasks. however, such techniques remain largely unexplored when considered in the context of learning fast solvers for systems of partial differential equations (eg: neural operators). in this work, we explore the use of self-training for fourier neural operators (fno). neural operators emerged as a data driven technique, however, data from experiments or traditional solvers is not always readily available. physics informed neural operators (pino) overcome this constraint by utilizing a physics loss for the training, however the accuracy of pino trained without data does not match the performance obtained by training with data. in this work we show that self-training can be used to close this gap in performance. we examine canonical examples, namely the 1d-burgers and 2d-darcy pdes, to showcase the efficacy of self-training. specifically, fnos, when trained exclusively with physics loss through self-training, approach 1.07x for burgers and 1.02x for darcy, compared to fnos trained with both data and physics loss. furthermore, we discover that pseudo-labels can be used for self-training without necessarily training to convergence in each iteration. a consequence of this is that we are able to discover self-training schedules that improve upon the baseline performance of pino in terms of accuracy as well as time.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13885",
        "authors": [
            "ritam majumdar",
            "amey varhade",
            "shirish karande",
            "lovekesh vig"
        ]
    },
    {
        "id": "2311.13892",
        "title": "general phrase debiaser: debiasing masked language models at a   multi-token level",
        "abstract": "the social biases and unwelcome stereotypes revealed by pretrained language models are becoming obstacles to their application. compared to numerous debiasing methods targeting word level, there has been relatively less attention on biases present at phrase level, limiting the performance of debiasing in discipline domains. in this paper, we propose an automatic multi-token debiasing pipeline called \\textbf{general phrase debiaser}, which is capable of mitigating phrase-level biases in masked language models. specifically, our method consists of a \\textit{phrase filter stage} that generates stereotypical phrases from wikipedia pages as well as a \\textit{model debias stage} that can debias models at the multi-token level to tackle bias challenges on phrases. the latter searches for prompts that trigger model's bias, and then uses them for debiasing. state-of-the-art results on standard datasets and metrics show that our approach can significantly reduce gender biases on both career and multiple disciplines, across models with varying parameter sizes.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13892",
        "authors": [
            "bingkang shi",
            "xiaodan zhang",
            "dehan kong",
            "yulei wu",
            "zongzhen liu",
            "honglei lyu",
            "longtao huang"
        ]
    },
    {
        "id": "2311.13905",
        "title": "a drl solution to help reduce the cost in waiting time of securing a   traffic light for cyclists",
        "abstract": "cyclists prefer to use infrastructure that separates them from motorized traffic. using a traffic light to segregate car and bike flows, with the addition of bike-specific green phases, is a lightweight and cheap solution that can be deployed dynamically to assess the opportunity of a heavier infrastructure such as a separate bike lane. to compensate for the increased waiting time induced by these new phases, we introduce in this paper a deep reinforcement learning solution that adapts the green phase cycle of a traffic light to the traffic. vehicle counter data are used to compare the drl approach with the actuated traffic light control algorithm over whole days. results show that drl achieves better minimization of vehicle waiting time at almost all hours. our drl approach is also robust to moderate changes in bike traffic. the code of this paper is available at https://github.com/lucasmagnana/a-drl-solution-to-help-reduce-the-cost-in-waiting-time-of-securing-a-traffic-light-for-cyclists.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13905",
        "authors": [
            "lucas magnana",
            "herv\u00e9 rivano",
            "nicolas chiabaut"
        ]
    },
    {
        "id": "2311.13928",
        "title": "parameter exchange for robust dynamic domain generalization",
        "abstract": "agnostic domain shift is the main reason of model degradation on the unknown target domains, which brings an urgent need to develop domain generalization (dg). recent advances at dg use dynamic networks to achieve training-free adaptation on the unknown target domains, termed dynamic domain generalization (ddg), which compensates for the lack of self-adaptability in static models with fixed weights. the parameters of dynamic networks can be decoupled into a static and a dynamic component, which are designed to learn domain-invariant and domain-specific features, respectively. based on the existing arts, in this work, we try to push the limits of ddg by disentangling the static and dynamic components more thoroughly from an optimization perspective. our main consideration is that we can enable the static component to learn domain-invariant features more comprehensively by augmenting the domain-specific information. as a result, the more comprehensive domain-invariant features learned by the static component can then enforce the dynamic component to focus more on learning adaptive domain-specific features. to this end, we propose a simple yet effective parameter exchange (pe) method to perturb the combination between the static and dynamic components. we optimize the model using the gradients from both the perturbed and non-perturbed feed-forward jointly to implicitly achieve the aforementioned disentanglement. in this way, the two components can be optimized in a mutually-beneficial manner, which can resist the agnostic domain shifts and improve the self-adaptability on the unknown target domain. extensive experiments show that pe can be easily plugged into existing dynamic networks to improve their generalization ability without bells and whistles.",
        "doi": "10.1145/3581783.3612318",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13928",
        "authors": [
            "luojun lin",
            "zhifeng shen",
            "zhishu sun",
            "yuanlong yu",
            "lei zhang",
            "weijie chen"
        ]
    },
    {
        "id": "2311.13953",
        "title": "learning uniform clusters on hypersphere for deep graph-level clustering",
        "abstract": "graph clustering has been popularly studied in recent years. however, most existing graph clustering methods focus on node-level clustering, i.e., grouping nodes in a single graph into clusters. in contrast, graph-level clustering, i.e., grouping multiple graphs into clusters, remains largely unexplored. graph-level clustering is critical in a variety of real-world applications, such as, properties prediction of molecules and community analysis in social networks. however, graph-level clustering is challenging due to the insufficient discriminability of graph-level representations, and the insufficient discriminability makes deep clustering be more likely to obtain degenerate solutions (cluster collapse). to address the issue, we propose a novel deep graph-level clustering method called uniform deep graph clustering (udgc). udgc assigns instances evenly to different clusters and then scatters those clusters on unit hypersphere, leading to a more uniform cluster-level distribution and a slighter cluster collapse. specifically, we first propose augmentation-consensus optimal transport (acot) for generating uniformly distributed and reliable pseudo labels for partitioning clusters. then we adopt contrastive learning to scatter those clusters. besides, we propose center alignment optimal transport (caot) for guiding the model to learn better parameters, which further promotes the cluster performance. our empirical study on eight well-known datasets demonstrates that udgc significantly outperforms the state-of-the-art models.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13953",
        "authors": [
            "mengling hu",
            "chaochao chen",
            "weiming liu",
            "xinyi zhang",
            "xinting liao",
            "xiaolin zheng"
        ]
    },
    {
        "id": "2311.13960",
        "title": "human machine co-creation. a complementary cognitive approach to   creative character design process using gans",
        "abstract": "recent advances in generative adversarial networks gans applications continue to attract the attention of researchers in different fields. in such a framework, two neural networks compete adversely to generate new visual contents indistinguishable from the original dataset. the objective of this research is to create a complementary codesign process between humans and machines to augment character designers abilities in visualizing and creating new characters for multimedia projects such as games and animation. driven by design cognitive scaffolding, the proposed approach aims to inform the process of perceiving, knowing, and making. the machine generated concepts are used as a launching platform for character designers to conceptualize new characters. a labelled dataset of 22,000 characters was developed for this work and deployed using different gans to evaluate the most suited for the context, followed by mixed methods evaluation for the machine output and human derivations. the discussed results substantiate the value of the proposed cocreation framework and elucidate how the generated concepts are used as cognitive substances that interact with designers competencies in a versatile manner to influence the creative processes of conceptualizing novel characters.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13960",
        "authors": [
            "mohammad lataifeh",
            "xavier a carrascoa",
            "ashraf m elnagara",
            "naveed ahmeda",
            "imran junejo"
        ]
    },
    {
        "id": "2311.13964",
        "title": "deep interactive segmentation of medical images: a systematic review and   taxonomy",
        "abstract": "interactive segmentation is a crucial research area in medical image analysis aiming to boost the efficiency of costly annotations by incorporating human feedback. this feedback takes the form of clicks, scribbles, or masks and allows for iterative refinement of the model output so as to efficiently guide the system towards the desired behavior. in recent years, deep learning-based approaches have propelled results to a new level causing a rapid growth in the field with 121 methods proposed in the medical imaging domain alone. in this review, we provide a structured overview of this emerging field featuring a comprehensive taxonomy, a systematic review of existing methods, and an in-depth analysis of current practices. based on these contributions, we discuss the challenges and opportunities in the field. for instance, we find that there is a severe lack of comparison across methods which needs to be tackled by standardized baselines and benchmarks.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13964",
        "authors": [
            "zdravko marinov",
            "paul f. j\u00e4ger",
            "jan egger",
            "jens kleesiek",
            "rainer stiefelhagen"
        ]
    },
    {
        "id": "2311.13982",
        "title": "probabilistic tree-of-thought reasoning for answering   knowledge-intensive complex questions",
        "abstract": "large language models (llms) are capable of answering knowledge-intensive complex questions with chain-of-thought (cot) reasoning. however, they tend to generate factually incorrect reasoning steps when the required knowledge is not available or up-to-date in models' parameters. recent works turn to retrieving external knowledge to augment cot reasoning. despite being promising, these chain-based methods suffer from: 1) negative retrieval. unnecessary or incorrect retrieval may mislead the reasoning; 2) limited sight. lacking the ability to look backward or forward, a local error in one step will propagate along the chain.   in this paper, we propose a novel approach: probabilistic tree-of-thought reasoning (probtree). first, llms translate a complex question into a query tree, in which each non-root node denotes a sub-question of its parent node. then, probabilistic reasoning is conducted over the tree, by solving questions from leaf to root considering the confidence of both question decomposing and answering. during reasoning, for leaf nodes, llms choose a more confident answer from closed-book qa that employs parametric knowledge and open-book qa that employs retrieved external knowledge, thus eliminating the negative retrieval problem. for non-leaf nodes, with the hierarchical structure, llms have broader sights and are able to globally reason with the information from child nodes, thus recovering from local errors. the experiments on three complex qa datasets under the open-domain setting show that our approach outperforms sota methods significantly, demonstrating the effect of probabilistic tree-of-thought reasoning.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13982",
        "authors": [
            "shulin cao",
            "jiajie zhang",
            "jiaxin shi",
            "xin lv",
            "zijun yao",
            "qi tian",
            "juanzi li",
            "lei hou"
        ]
    },
    {
        "id": "2311.13983",
        "title": "learning dynamic selection and pricing of out-of-home deliveries",
        "abstract": "home delivery failures, traffic congestion, and relatively large handling times have a negative impact on the profitability of last-mile logistics. these external factors contribute to up to $28\\%$ of the overall costs and $25\\%$ of emissions for the home delivery supply chain. a potential solution, showing annual growth rates up to $36\\%$, is the delivery to parcel lockers or parcel shops, denoted by out-of-home (ooh) delivery. in the academic literature, models of customer behavior with respect to ooh delivery were so far limited to deterministic settings, contrasting with the stochastic nature of actual customer choices. we model the sequential decision-making problem of which ooh location to offer against what incentive for each incoming customer, taking into account future customer arrivals and choices. we propose dynamic selection and pricing of ooh (dspo), an algorithmic pipeline that uses a novel spatial-temporal state encoding as input to a convolutional neural network. we demonstrate the performance of our method by benchmarking it against three state-of-the-art approaches. our extensive numerical study, guided by real-world data, reveals that dspo can save $20.8\\%$ in costs compared to a situation without ooh locations, $8.1\\%$ compared to a static selection and pricing policy, and $4.6\\%$ compared to a state-of-the-art demand management benchmark. we provide comprehensive insights into the complex interplay between ooh delivery dynamics and customer behavior influenced by pricing strategies. the implications of our findings suggest practitioners to adopt dynamic selection and pricing policies as ooh delivery gains a larger market share.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.13983",
        "authors": [
            "fabian akkerman",
            "peter dieter",
            "martijn mes"
        ]
    },
    {
        "id": "2311.14003",
        "title": "direct preference-based evolutionary multi-objective optimization with   dueling bandit",
        "abstract": "optimization problems find widespread use in both single-objective and multi-objective scenarios. in practical applications, users aspire for solutions that converge to the region of interest (roi) along the pareto front (pf). while the conventional approach involves approximating a fitness function or an objective function to reflect user preferences, this paper explores an alternative avenue. specifically, we aim to discover a method that sidesteps the need for calculating the fitness function, relying solely on human feedback. our proposed approach entails conducting direct preference learning facilitated by an active dueling bandit algorithm. the experimental phase is structured into three sessions. firstly, we assess the performance of our active dueling bandit algorithm. secondly, we implement our proposed method within the context of multi-objective evolutionary algorithms (moeas). finally, we deploy our method in a practical problem, specifically in protein structure prediction (psp). this research presents a novel interactive preference-based moea framework that not only addresses the limitations of traditional techniques but also unveils new possibilities for optimization problems.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14003",
        "authors": [
            "tian huang",
            "ke li"
        ]
    },
    {
        "id": "2311.14005",
        "title": "when side-channel attacks break the black-box property of embedded   artificial intelligence",
        "abstract": "artificial intelligence, and specifically deep neural networks (dnns), has rapidly emerged in the past decade as the standard for several tasks from specific advertising to object detection. the performance offered has led dnn algorithms to become a part of critical embedded systems, requiring both efficiency and reliability. in particular, dnns are subject to malicious examples designed in a way to fool the network while being undetectable to the human observer: the adversarial examples. while previous studies propose frameworks to implement such attacks in black box settings, those often rely on the hypothesis that the attacker has access to the logits of the neural network, breaking the assumption of the traditional black box. in this paper, we investigate a real black box scenario where the attacker has no access to the logits. in particular, we propose an architecture-agnostic attack which solve this constraint by extracting the logits. our method combines hardware and software attacks, by performing a side-channel attack that exploits electromagnetic leakages to extract the logits for a given input, allowing an attacker to estimate the gradients and produce state-of-the-art adversarial examples to fool the targeted neural network. through this example of adversarial attack, we demonstrate the effectiveness of logits extraction using side-channel as a first step for more general attack frameworks requiring either the logits or the confidence scores.",
        "doi": "10.1145/3605764.3623903",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14005",
        "authors": [
            "benoit coqueret",
            "mathieu carbone",
            "olivier sentieys",
            "gabriel zaid"
        ]
    },
    {
        "id": "2311.14028",
        "title": "continual learning of diffusion models with generative distillation",
        "abstract": "diffusion models are powerful generative models that achieve state-of-the-art performance in tasks such as image synthesis. however, training them demands substantial amounts of data and computational resources. continual learning would allow for incrementally learning new tasks and accumulating knowledge, thus reusing already trained models would be possible. one potentially suitable approach is generative replay, where a copy of a generative model trained on previous tasks produces synthetic data that are interleaved with data from the current task. however, standard generative replay applied to diffusion models results in a catastrophic loss in denoising capabilities. in this paper, we propose generative distillation, an approach that distils the entire reverse process of a diffusion model. we demonstrate that our approach significantly improves the continual learning performance of generative replay with only a moderate increase in the computational costs.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14028",
        "authors": [
            "sergi masip",
            "pau rodriguez",
            "tinne tuytelaars",
            "gido m. van de ven"
        ]
    },
    {
        "id": "2311.14030",
        "title": "privatelora for efficient privacy preserving llm",
        "abstract": "end users face a choice between privacy and efficiency in current large language model (llm) service paradigms. in cloud-based paradigms, users are forced to compromise data locality for generation quality and processing speed. conversely, edge device paradigms maintain data locality but fail to deliver satisfactory performance. in this work, we propose a novel llm service paradigm that distributes privacy-sensitive computation on edge devices and shared computation in the cloud. only activations are transmitted between the central cloud and edge devices to ensure data locality. our core innovation, privatelora, addresses the challenging communication overhead by exploiting the low rank of residual activations, achieving over 95% communication reduction. consequently, privatelora effectively maintains data locality and is extremely resource efficient. under standard 5g networks, privatelora achieves throughput over 300% of device-only solutions for 7b models and over 80% of an a100 gpu for 33b models. privatelora also provides tuning performance comparable to lora for advanced personalization. our approach democratizes access to state-of-the-art generative ai for edge devices, paving the way for more tailored llm experiences for the general public. to our knowledge, our proposed framework is the first efficient and privacy-preserving llm solution in the literature.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14030",
        "authors": [
            "yiming wang",
            "yu lin",
            "xiaodong zeng",
            "guannan zhang"
        ]
    },
    {
        "id": "2311.14057",
        "title": "assessing the impact of noise on quantum neural networks: an   experimental analysis",
        "abstract": "in the race towards quantum computing, the potential benefits of quantum neural networks (qnns) have become increasingly apparent. however, noisy intermediate-scale quantum (nisq) processors are prone to errors, which poses a significant challenge for the execution of complex algorithms or quantum machine learning. to ensure the quality and security of qnns, it is crucial to explore the impact of noise on their performance. this paper provides a comprehensive analysis of the impact of noise on qnns, examining the mottonen state preparation algorithm under various noise models and studying the degradation of quantum states as they pass through multiple layers of qnns. additionally, the paper evaluates the effect of noise on the performance of pre-trained qnns and highlights the challenges posed by noise models in quantum computing. the findings of this study have significant implications for the development of quantum software, emphasizing the importance of prioritizing stability and noise-correction measures when developing qnns to ensure reliable and trustworthy results. this paper contributes to the growing body of literature on quantum computing and quantum machine learning, providing new insights into the impact of noise on qnns and paving the way towards the development of more robust and efficient quantum algorithms.",
        "doi": "10.1007/978-3-031-40725-3_27",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14057",
        "authors": [
            "erik b. terres escudero",
            "danel arias alamo",
            "oier mentxaka g\u00f3mez",
            "pablo garc\u00eda bringas"
        ]
    },
    {
        "id": "2311.14058",
        "title": "identification for tree-shaped structural causal models in polynomial   time",
        "abstract": "linear structural causal models (scms) are used to express and analyse the relationships between random variables. direct causal effects are represented as directed edges and confounding factors as bidirected edges. identifying the causal parameters from correlations between the nodes is an open problem in artificial intelligence. in this paper, we study scms whose directed component forms a tree. van der zander et al. (aistats'22, plmr 151, pp. 6770--6792, 2022) give a pspace-algorithm for the identification problem in this case, which is a significant improvement over the general gr\\\"obner basis approach, which has doubly-exponential time complexity in the number of structural parameters. in this work, we present a randomized polynomial-time algorithm, which solves the identification problem for tree-shaped scms. for every structural parameter, our algorithms decides whether it is generically identifiable, generically 2-identifiable, or generically unidentifiable. (no other cases can occur.) in the first two cases, it provides one or two fractional affine square root terms of polynomials (fastps) for the corresponding parameter, respectively.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14058",
        "authors": [
            "aaryan gupta",
            "markus bl\u00e4ser"
        ]
    },
    {
        "id": "2311.14061",
        "title": "towards explainable strategy templates using nlp transformers",
        "abstract": "this paper bridges the gap between mathematical heuristic strategies learned from deep reinforcement learning (drl) in automated agent negotiation, and comprehensible, natural language explanations. our aim is to make these strategies more accessible to non-experts. by leveraging traditional natural language processing (nlp) techniques and large language models (llms) equipped with transformers, we outline how parts of drl strategies composed of parts within strategy templates can be transformed into user-friendly, human-like english narratives. to achieve this, we present a top-level algorithm that involves parsing mathematical expressions of strategy templates, semantically interpreting variables and structures, generating rule-based primary explanations, and utilizing a generative pre-trained transformer (gpt) model to refine and contextualize these explanations. subsequent customization for varied audiences and meticulous validation processes in an example illustrate the applicability and potential of this approach.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14061",
        "authors": [
            "pallavi bagga",
            "kostas stathis"
        ]
    },
    {
        "id": "2311.14073",
        "title": "learning saliency from fixations",
        "abstract": "we present a novel approach for saliency prediction in images, leveraging parallel decoding in transformers to learn saliency solely from fixation maps. models typically rely on continuous saliency maps, to overcome the difficulty of optimizing for the discrete fixation map. we attempt to replicate the experimental setup that generates saliency datasets. our approach treats saliency prediction as a direct set prediction problem, via a global loss that enforces unique fixations prediction through bipartite matching and a transformer encoder-decoder architecture. by utilizing a fixed set of learned fixation queries, the cross-attention reasons over the image features to directly output the fixation points, distinguishing it from other modern saliency predictors. our approach, named saliency transformer (saltr), achieves metric scores on par with state-of-the-art approaches on the salicon and mit300 benchmarks.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14073",
        "authors": [
            "yasser abdelaziz dahou djilali",
            "kevin mcguiness",
            "noel o'connor"
        ]
    },
    {
        "id": "2311.14091",
        "title": "portfoliomentor: multimodal generative ai companion for learning and   crafting interactive digital art portfolios",
        "abstract": "digital art portfolios serve as impactful mediums for artists to convey their visions, weaving together visuals, audio, interactions, and narratives. however, without technical backgrounds, design students often find it challenging to translate creative ideas into tangible codes and designs, given the lack of tailored resources for the non-technical, academic support in art schools, and a comprehensive guiding tool throughout the mentally demanding process. recognizing the role of companionship in code learning and leveraging generative ai models' capabilities in supporting creative tasks, we present portfoliomentor, a coding companion chatbot for ides. this tool guides and collaborates with students through proactive suggestions and responsible q&as for learning, inspiration, and support. in detail, the system starts with the understanding of the task and artist's visions, follows the co-creation of visual illustrations, audio or music suggestions and files, click-scroll effects for interactions, and creative vision conceptualization, and finally synthesizes these facets into a polished interactive digital portfolio.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14091",
        "authors": [
            "tao long",
            "weirui peng"
        ]
    },
    {
        "id": "2311.14096",
        "title": "auditing and mitigating cultural bias in llms",
        "abstract": "culture fundamentally shapes people's reasoning, behavior, and communication. generative artificial intelligence (ai) technologies may cause a shift towards a dominant culture. as people increasingly use ai to expedite and even automate various professional and personal tasks, cultural values embedded in ai models may bias authentic expression. we audit large language models for cultural bias, comparing their responses to nationally representative survey data, and evaluate country-specific prompting as a mitigation strategy. we find that gpt-4, 3.5 and 3 exhibit cultural values resembling english-speaking and protestant european countries. our mitigation strategy reduces cultural bias in recent models but not for all countries/territories. to avoid cultural bias in generative ai, especially in high-stakes contexts, we suggest using culture matching and ongoing cultural audits.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14096",
        "authors": [
            "yan tao",
            "olga viberg",
            "ryan s. baker",
            "rene f. kizilcec"
        ]
    },
    {
        "id": "2311.14109",
        "title": "boosting the power of small multimodal reasoning models to match larger   models with self-consistency training",
        "abstract": "multimodal reasoning is a challenging task that requires models to reason across multiple modalities to answer questions. existing approaches have made progress by incorporating language and visual modalities into a two-stage reasoning framework, separating rationale generation from answer inference. however, these approaches often fall short due to the inadequate quality of the generated rationales. in this work, we delve into the importance of rationales in model reasoning. we observe that when rationales are completely accurate, the model's accuracy significantly improves, highlighting the need for high-quality rationale generation. motivated by this, we propose mc-cot, a self-consistency training strategy that generates multiple rationales and answers, subsequently selecting the most accurate through a voting process. this approach not only enhances the quality of generated rationales but also leads to more accurate and robust answers. through extensive experiments, we demonstrate that our approach significantly improves model performance across various benchmarks. remarkably, we show that even smaller base models, when equipped with our proposed approach, can achieve results comparable to those of larger models, illustrating the potential of our approach in harnessing the power of rationales for improved multimodal reasoning. the code is available at https://github.com/chengtan9907/mc-cot.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14109",
        "authors": [
            "cheng tan",
            "jingxuan wei",
            "zhangyang gao",
            "linzhuang sun",
            "siyuan li",
            "xihong yang",
            "stan z. li"
        ]
    },
    {
        "id": "2311.14110",
        "title": "when is off-policy evaluation useful? a data-centric perspective",
        "abstract": "evaluating the value of a hypothetical target policy with only a logged dataset is important but challenging. on the one hand, it brings opportunities for safe policy improvement under high-stakes scenarios like clinical guidelines. on the other hand, such opportunities raise a need for precise off-policy evaluation (ope). while previous work on ope focused on improving the algorithm in value estimation, in this work, we emphasize the importance of the offline dataset, hence putting forward a data-centric framework for evaluating ope problems. we propose datacope, a data-centric framework for evaluating ope, that answers the questions of whether and to what extent we can evaluate a target policy given a dataset. datacope (1) forecasts the overall performance of ope algorithms without access to the environment, which is especially useful before real-world deployment where evaluating ope is impossible; (2) identifies the sub-group in the dataset where ope can be inaccurate; (3) permits evaluations of datasets or data-collection strategies for ope problems. our empirical analysis of datacope in the logged contextual bandit settings using healthcare datasets confirms its ability to evaluate both machine-learning and human expert policies like clinical guidelines.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14110",
        "authors": [
            "hao sun",
            "alex j. chan",
            "nabeel seedat",
            "alihan h\u00fcy\u00fck",
            "mihaela van der schaar"
        ]
    },
    {
        "id": "2311.14115",
        "title": "a density estimation perspective on learning from pairwise human   preferences",
        "abstract": "learning from human feedback (lhf) -- and in particular learning from pairwise preferences -- has recently become a crucial ingredient in training large language models (llms), and has been the subject of much research. most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the llm is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. we propose an alternative interpretation which centers on the generative process for pairwise preferences and treats lhf as a density estimation problem. we provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. finally, we discuss and present findings on \"annotator misspecification\" -- failure cases where wrong modeling assumptions are made about annotator behavior, resulting in poorly-adapted models -- suggesting that approaches that learn from pairwise human preferences could have trouble learning from a population of annotators with diverse viewpoints.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14115",
        "authors": [
            "vincent dumoulin",
            "daniel d. johnson",
            "pablo samuel castro",
            "hugo larochelle",
            "yann dauphin"
        ]
    },
    {
        "id": "2311.14125",
        "title": "scalable ai safety via doubly-efficient debate",
        "abstract": "the emergence of pre-trained ai systems with powerful capabilities across a diverse and ever-increasing set of complex domains has raised a critical challenge for ai safety as tasks can become too complicated for humans to judge directly. irving et al. [2018] proposed a debate method in this direction with the goal of pitting the power of such ai models against each other until the problem of identifying (mis)-alignment is broken down into a manageable subtask. while the promise of this approach is clear, the original framework was based on the assumption that the honest strategy is able to simulate deterministic ai systems for an exponential number of steps, limiting its applicability. in this paper, we show how to address these challenges by designing a new set of debate protocols where the honest strategy can always succeed using a simulation of a polynomial number of steps, whilst being able to verify the alignment of stochastic ai systems, even when the dishonest strategy is allowed to use exponentially many simulation steps.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14125",
        "authors": [
            "jonah brown-cohen",
            "geoffrey irving",
            "georgios piliouras"
        ]
    },
    {
        "id": "2311.14126",
        "title": "towards auditing large language models: improving text-based stereotype   detection",
        "abstract": "large language models (llm) have made significant advances in the recent past becoming more mainstream in artificial intelligence (ai) enabled human-facing applications. however, llms often generate stereotypical output inherited from historical data, amplifying societal biases and raising ethical concerns. this work introduces i) the multi-grain stereotype dataset, which includes 52,751 instances of gender, race, profession and religion stereotypic text and ii) a novel stereotype classifier for english text. we design several experiments to rigorously test the proposed model trained on the novel dataset. our experiments show that training the model in a multi-class setting can outperform the one-vs-all binary counterpart. consistent feature importance signals from different explainable ai tools demonstrate that the new model exploits relevant text features. we utilise the newly created model to assess the stereotypic behaviour of the popular gpt family of models and observe the reduction of bias over time. in summary, our work establishes a robust and practical framework for auditing and evaluating the stereotypic bias in llm.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14126",
        "authors": [
            "wu zekun",
            "sahan bulathwela",
            "adriano soares koshiyama"
        ]
    },
    {
        "id": "2311.14127",
        "title": "byzantine robustness and partial participation can be achieved   simultaneously: just clip gradient differences",
        "abstract": "distributed learning has emerged as a leading paradigm for training large machine learning models. however, in real-world scenarios, participants may be unreliable or malicious, posing a significant challenge to the integrity and accuracy of the trained models. byzantine fault tolerance mechanisms have been proposed to address these issues, but they often assume full participation from all clients, which is not always practical due to the unavailability of some clients or communication constraints. in our work, we propose the first distributed method with client sampling and provable tolerance to byzantine workers. the key idea behind the developed method is the use of gradient clipping to control stochastic gradient differences in recursive variance reduction. this allows us to bound the potential harm caused by byzantine workers, even during iterations when all sampled clients are byzantine. furthermore, we incorporate communication compression into the method to enhance communication efficiency. under quite general assumptions, we prove convergence rates for the proposed method that match the existing state-of-the-art (sota) theoretical results.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14127",
        "authors": [
            "grigory malinovsky",
            "peter richt\u00e1rik",
            "samuel horv\u00e1th",
            "eduard gorbunov"
        ]
    },
    {
        "id": "2311.14139",
        "title": "machine learning for an explainable cost prediction of medical insurance",
        "abstract": "predictive modeling in healthcare continues to be an active actuarial research topic as more insurance companies aim to maximize the potential of machine learning approaches to increase their productivity and efficiency. in this paper, the authors deployed three regression-based ensemble ml models that combine variations of decision trees through extreme gradient boosting, gradient-boosting machine, and random forest) methods in predicting medical insurance costs. explainable artificial intelligence methods shapley additive explanations and individual conditional expectation plots were deployed to discover and explain the key determinant factors that influence medical insurance premium prices in the dataset. the dataset used comprised 986 records and is publicly available in the kaggle repository. the models were evaluated using four performance evaluation metrics, including r-squared, mean absolute error, root mean squared error, and mean absolute percentage error. the results show that all models produced impressive outcomes; however, the xgboost model achieved a better overall performance although it also expanded more computational resources, while the rf model recorded a lesser prediction error and consumed far fewer computing resources than the xgboost model. furthermore, we compared the outcome of both xai methods in identifying the key determinant features that influenced the premiumprices for each model and whereas both xai methods produced similar outcomes, we found that the ice plots showed in more detail the interactions between each variable than the shap analysis which seemed to be more high-level. it is the aim of the authors that the contributions of this study will help policymakers, insurers, and potential medical insurance buyers in their decision-making process for selecting the right policies that meet their specific needs.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14139",
        "authors": [
            "ugochukwu orji",
            "elochukwu ukwandu"
        ]
    },
    {
        "id": "2311.14153",
        "title": "tube-nerf: efficient imitation learning of visuomotor policies from mpc   using tube-guided data augmentation and nerfs",
        "abstract": "imitation learning (il) can train computationally-efficient sensorimotor policies from a resource-intensive model predictive controller (mpc), but it often requires many samples, leading to long training times or limited robustness. to address these issues, we combine il with a variant of robust mpc that accounts for process and sensing uncertainties, and we design a data augmentation (da) strategy that enables efficient learning of vision-based policies. the proposed da method, named tube-nerf, leverages neural radiance fields (nerfs) to generate novel synthetic images, and uses properties of the robust mpc (the tube) to select relevant views and to efficiently compute the corresponding actions. we tailor our approach to the task of localization and trajectory tracking on a multirotor, by learning a visuomotor policy that generates control actions using images from the onboard camera as only source of horizontal position. our evaluations numerically demonstrate learning of a robust visuomotor policy with an 80-fold increase in demonstration efficiency and a 50% reduction in training time over current il methods. additionally, our policies successfully transfer to a real multirotor, achieving accurate localization and low tracking errors despite large disturbances, with an onboard inference time of only 1.5 ms.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14153",
        "authors": [
            "andrea tagliabue",
            "jonathan p. how"
        ]
    },
    {
        "id": "2311.14156",
        "title": "variational annealing on graphs for combinatorial optimization",
        "abstract": "several recent unsupervised learning methods use probabilistic approaches to solve combinatorial optimization (co) problems based on the assumption of statistically independent solution variables. we demonstrate that this assumption imposes performance limitations in particular on difficult problem instances. our results corroborate that an autoregressive approach which captures statistical dependencies among solution variables yields superior performance on many popular co problems. we introduce subgraph tokenization in which the configuration of a set of solution variables is represented by a single token. this tokenization technique alleviates the drawback of the long sequential sampling procedure which is inherent to autoregressive methods without sacrificing expressivity. importantly, we theoretically motivate an annealed entropy regularization and show empirically that it is essential for efficient and stable learning.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14156",
        "authors": [
            "sebastian sanokowski",
            "wilhelm berghammer",
            "sepp hochreiter",
            "sebastian lehner"
        ]
    },
    {
        "id": "2311.14169",
        "title": "evaluating gpt-4's vision capabilities on brazilian university admission   exams",
        "abstract": "recent advancements in language models have showcased human-comparable performance in academic entrance exams. however, existing studies often overlook questions that require the integration of visual comprehension, thus compromising the full spectrum and complexity inherent in real-world scenarios. to address this gap, we present a comprehensive framework to evaluate language models on entrance exams, which incorporates both textual and visual elements. we evaluate the two most recent editions of exame nacional do ensino m\\'edio (enem), the main standardized entrance examination adopted by brazilian universities. our study not only reaffirms the capabilities of gpt-4 as the state of the art for handling complex multidisciplinary questions, but also pioneers in offering a realistic assessment of multimodal language models on portuguese examinations. one of the highlights is that text captions transcribing visual content outperform the direct use of images, suggesting that the vision model has room for improvement. yet, despite improvements afforded by images or captions, mathematical questions remain a challenge for these state-of-the-art models. the code and data used on experiments are available at https://github.com/piresramon/gpt-4-enem.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14169",
        "authors": [
            "ramon pires",
            "thales sales almeida",
            "hugo abonizio",
            "rodrigo nogueira"
        ]
    },
    {
        "id": "2311.14175",
        "title": "appearance-based gaze estimation enhanced with synthetic images using   deep neural networks",
        "abstract": "human eye gaze estimation is an important cognitive ingredient for successful human-robot interaction, enabling the robot to read and predict human behavior. we approach this problem using artificial neural networks and build a modular system estimating gaze from separately cropped eyes, taking advantage of existing well-functioning components for face detection (retinaface) and head pose estimation (6drepnet). our proposed method does not require any special hardware or infrared filters but uses a standard notebook-builtin rgb camera, as often approached with appearance-based methods. using the metahuman tool, we also generated a large synthetic dataset of more than 57,000 human faces and made it publicly available. the inclusion of this dataset (with eye gaze and head pose information) on top of the standard columbia gaze dataset into training the model led to better accuracy with a mean average error below two degrees in eye pitch and yaw directions, which compares favourably to related methods. we also verified the feasibility of our model by its preliminary testing in real-world setting using the builtin 4k camera in nico semi-humanoid robot's eye.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14175",
        "authors": [
            "dmytro herashchenko",
            "igor farka\u0161"
        ]
    },
    {
        "id": "2311.14203",
        "title": "data-driven risk modeling for infrastructure projects using artificial   intelligence techniques",
        "abstract": "managing project risk is a key part of the successful implementation of any large project and is widely recognized as a best practice for public agencies to deliver infrastructures. the conventional method of identifying and evaluating project risks involves getting input from subject matter experts at risk workshops in the early phases of a project. as a project moves through its life cycle, these identified risks and their assessments evolve. some risks are realized to become issues, some are mitigated, and some are retired as no longer important. despite the value provided by conventional expert-based approaches, several challenges remain due to the time-consuming and expensive processes involved. moreover, limited is known about how risks evolve from ex-ante to ex-post over time. how well does the project team identify and evaluate risks in the initial phase compared to what happens during project execution? using historical data and artificial intelligence techniques, this study addressed these limitations by introducing a data-driven framework to identify risks automatically and to examine the quality of early risk registers and risk assessments. risk registers from more than 70 u.s. major transportation projects form the input dataset.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14203",
        "authors": [
            "abdolmajid erfani"
        ]
    },
    {
        "id": "2311.14270",
        "title": "efficient open-world reinforcement learning via knowledge distillation   and autonomous rule discovery",
        "abstract": "deep reinforcement learning suffers from catastrophic forgetting and sample inefficiency making it less applicable to the ever-changing real world. however, the ability to use previously learned knowledge is essential for ai agents to quickly adapt to novelties. often, certain spatial information observed by the agent in the previous interactions can be leveraged to infer task-specific rules. inferred rules can then help the agent to avoid potentially dangerous situations in the previously unseen states and guide the learning process increasing agent's novelty adaptation speed. in this work, we propose a general framework that is applicable to deep reinforcement learning agents. our framework provides the agent with an autonomous way to discover the task-specific rules in the novel environments and self-supervise it's learning. we provide a rule-driven deep q-learning agent (rdq) as one possible implementation of that framework. we show that rdq successfully extracts task-specific rules as it interacts with the world and uses them to drastically increase its learning efficiency. in our experiments, we show that the rdq agent is significantly more resilient to the novelties than the baseline agents, and is able to detect and adapt to novel situations faster.",
        "doi": "",
        "created": "2023-11-23",
        "url": "https://arxiv.org/abs/2311.14270",
        "authors": [
            "ekaterina nikonova",
            "cheng xue",
            "jochen renz"
        ]
    },
    {
        "id": "2311.14305",
        "title": "new epochs in ai supervision: design and implementation of an autonomous   radiology ai monitoring system",
        "abstract": "with the increasingly widespread adoption of ai in healthcare, maintaining the accuracy and reliability of ai models in clinical practice has become crucial. in this context, we introduce novel methods for monitoring the performance of radiology ai classification models in practice, addressing the challenges of obtaining real-time ground truth for performance monitoring. we propose two metrics - predictive divergence and temporal stability - to be used for preemptive alerts of ai performance changes. predictive divergence, measured using kullback-leibler and jensen-shannon divergences, evaluates model accuracy by comparing predictions with those of two supplementary models. temporal stability is assessed through a comparison of current predictions against historical moving averages, identifying potential model decay or data drift. this approach was retrospectively validated using chest x-ray data from a single-center imaging clinic, demonstrating its effectiveness in maintaining ai model reliability. by providing continuous, real-time insights into model performance, our system ensures the safe and effective use of ai in clinical decision-making, paving the way for more robust ai integration in healthcare",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14305",
        "authors": [
            "vasantha kumar venugopal",
            "abhishek gupta",
            "rohit takhar",
            "vidur mahajan"
        ]
    },
    {
        "id": "2311.14315",
        "title": "robust domain misinformation detection via multi-modal feature alignment",
        "abstract": "social media misinformation harms individuals and societies and is potentialized by fast-growing multi-modal content (i.e., texts and images), which accounts for higher \"credibility\" than text-only news pieces. although existing supervised misinformation detection methods have obtained acceptable performances in key setups, they may require large amounts of labeled data from various events, which can be time-consuming and tedious. in turn, directly training a model by leveraging a publicly available dataset may fail to generalize due to domain shifts between the training data (a.k.a. source domains) and the data from target domains. most prior work on domain shift focuses on a single modality (e.g., text modality) and ignores the scenario where sufficient unlabeled target domain data may not be readily available in an early stage. the lack of data often happens due to the dynamic propagation trend (i.e., the number of posts related to fake news increases slowly before catching the public attention). we propose a novel robust domain and cross-modal approach (\\textbf{rdcm}) for multi-modal misinformation detection. it reduces the domain shift by aligning the joint distribution of textual and visual modalities through an inter-domain alignment module and bridges the semantic gap between both modalities through a cross-modality alignment module. we also propose a framework that simultaneously considers application scenarios of domain generalization (in which the target domain data is unavailable) and domain adaptation (in which unlabeled target domain data is available). evaluation results on two public multi-modal misinformation detection datasets (pheme and twitter datasets) evince the superiority of the proposed model. the formal implementation of this paper can be found in this link: https://github.com/less-and-less-bugs/rdcm",
        "doi": "10.1109/tifs.2023.3326368",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14315",
        "authors": [
            "hui liu",
            "wenya wang",
            "hao sun",
            "anderson rocha",
            "haoliang li"
        ]
    },
    {
        "id": "2311.14316",
        "title": "windformer:bi-directional long-distance spatio-temporal network for wind   speed prediction",
        "abstract": "wind speed prediction is critical to the management of wind power generation. due to the large range of wind speed fluctuations and wake effect, there may also be strong correlations between long-distance wind turbines. this difficult-to-extract feature has become a bottleneck for improving accuracy. history and future time information includes the trend of airflow changes, whether this dynamic information can be utilized will also affect the prediction effect. in response to the above problems, this paper proposes windformer. first, windformer divides the wind turbine cluster into multiple non-overlapping windows and calculates correlations inside the windows, then shifts the windows partially to provide connectivity between windows, and finally fuses multi-channel features based on detailed and global information. to dynamically model the change process of wind speed, this paper extracts time series in both history and future directions simultaneously. compared with other current-advanced methods, the mean square error (mse) of windformer is reduced by 0.5\\% to 15\\% on two datasets from nerl.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14316",
        "authors": [
            "xuewei li",
            "zewen shang",
            "zhiqiang liu",
            "jian yu",
            "wei xiong",
            "mei yu"
        ]
    },
    {
        "id": "2311.14324",
        "title": "large language models as topological structure enhancers for   text-attributed graphs",
        "abstract": "the latest advancements in large language models (llms) have revolutionized the field of natural language processing (nlp). inspired by the success of llms in nlp tasks, some recent work has begun investigating the potential of applying llms in graph learning tasks. however, most of the existing work focuses on utilizing llms as powerful node feature augmenters, leaving employing llms to enhance graph topological structures an understudied problem. in this work, we explore how to leverage the information retrieval and text generation capabilities of llms to refine/enhance the topological structure of text-attributed graphs (tags) under the node classification setting. first, we propose using llms to help remove unreliable edges and add reliable ones in the tag. specifically, we first let the llm output the semantic similarity between node attributes through delicate prompt designs, and then perform edge deletion and edge addition based on the similarity. second, we propose using pseudo-labels generated by the llm to improve graph topology, that is, we introduce the pseudo-label propagation as a regularization to guide the graph neural network (gnn) in learning proper edge weights. finally, we incorporate the two aforementioned llm-based methods for graph topological refinement into the process of gnn training, and perform extensive experiments on four real-world datasets. the experimental results demonstrate the effectiveness of llm-based graph topology refinement (achieving a 0.15%--2.47% performance gain on public benchmarks).",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14324",
        "authors": [
            "shengyin sun",
            "yuxiang ren",
            "chen ma",
            "xuecang zhang"
        ]
    },
    {
        "id": "2311.14335",
        "title": "comparative analysis of transformers for modeling tabular data: a   casestudy using industry scale dataset",
        "abstract": "we perform a comparative analysis of transformer-based models designed for modeling tabular data, specifically on an industry-scale dataset. while earlier studies demonstrated promising outcomes on smaller public or synthetic datasets, the effectiveness did not extend to larger industry-scale datasets. the challenges identified include handling high-dimensional data, the necessity for efficient pre-processing of categorical and numerical features, and addressing substantial computational requirements.   to overcome the identified challenges, the study conducts an extensive examination of various transformer-based models using both synthetic datasets and the default prediction kaggle dataset (2022) from american express. the paper presents crucial insights into optimal data pre-processing, compares pre-training and direct supervised learning methods, discusses strategies for managing categorical and numerical features, and highlights trade-offs between computational resources and performance. focusing on temporal financial data modeling, the research aims to facilitate the systematic development and deployment of transformer-based models in real-world scenarios, emphasizing scalability.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14335",
        "authors": [
            "usneek singh",
            "piyush arora",
            "shamika ganesan",
            "mohit kumar",
            "siddhant kulkarni",
            "salil r. joshi"
        ]
    },
    {
        "id": "2311.14371",
        "title": "federated transformed learning for a circular, secure, and tiny ai",
        "abstract": "deep learning (dl) is penetrating into a diverse range of mass mobility, smart living, and industrial applications, rapidly transforming the way we live and work. dl is at the heart of many ai implementations. a key set of challenges is to produce ai modules that are: (1) \"circular\" - can solve new tasks without forgetting how to solve previous ones, (2) \"secure\" - have immunity to adversarial data attacks, and (3) \"tiny\" - implementable in low power low cost embedded hardware. clearly it is difficult to achieve all three aspects on a single horizontal layer of platforms, as the techniques require transformed deep representations that incur different computation and communication requirements. here we set out the vision to achieve transformed dl representations across a 5g and beyond networked architecture. we first detail the cross-sectoral motivations for each challenge area, before demonstrating recent advances in dl research that can achieve circular, secure, and tiny ai (cst-ai). recognising the conflicting demand of each transformed deep representation, we federate their deep learning transformations and functionalities across the network to achieve connected run-time capabilities.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14371",
        "authors": [
            "weisi guo",
            "schyler sun",
            "bin li",
            "sam blakeman"
        ]
    },
    {
        "id": "2311.14378",
        "title": "ethical implications of chatgpt in higher education: a scoping review",
        "abstract": "this scoping review explores the ethical challenges of using chatgpt in education, focusing particularly on issues related to higher education. by reviewing recent academic articles written in english, chinese, and japanese, we aimed to provide a comprehensive overview of relevant research while identifying gaps for future considerations. drawing on arksey and o'malley's (2005) five-stage scoping review framework, we identified research questions, search terms, and conducted article search from four databases in the target three languages. each article was reviewed by at least two researchers identifying the main ethical issues of utilizing ai in education, particularly higher education. our analysis of ethical issues followed the framework developed by deepmind (weiginger et al., 2021) to identify six main areas of ethical concern in language models. the majority of papers were concerned with misinformation harms (n=25) and/or human-computer interaction related harms (n=24). given the rapid deployment of generative artificial intelligence (gai), it is imperative for educators to conduct more empirical studies to develop sound ethical policies for the use of gai.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14378",
        "authors": [
            "ming li",
            "ariunaa enkhtur",
            "fei cheng",
            "beverley anne yamamoto"
        ]
    },
    {
        "id": "2311.14381",
        "title": "potential societal biases of chatgpt in higher education: a scoping   review",
        "abstract": "chatgpt and other generative artificial intelligence (gai) models tend to inherit and even amplify prevailing societal biases as they are trained on large amounts of existing data. given the increasing usage of chatgpt and other gai by students, faculty members, and staff in higher education institutions (heis), there is an urgent need to examine the ethical issues involved such as its potential biases. in this scoping review, we clarify the ways in which biases related to gai in higher education settings have been discussed in recent academic publications and identify what type of potential biases are commonly reported in this body of literature. we searched for academic articles written in english, chinese, and japanese across four main databases concerned with gai usage in higher education and bias. our findings show that while there is an awareness of potential biases around large language models (llms) and gai, the majority of articles touch on ``bias'' at a relatively superficial level. few identify what types of bias may occur under what circumstances. neither do they discuss the possible implications for the higher education, staff, faculty members, or students. there is a notable lack of empirical work at this point, and we call for higher education researchers and ai experts to conduct more research in this area.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14381",
        "authors": [
            "ming li",
            "ariunaa enkhtur",
            "beverley anne yamamoto",
            "fei cheng"
        ]
    },
    {
        "id": "2311.14390",
        "title": "directly attention loss adjusted prioritized experience replay",
        "abstract": "prioritized experience replay (per) enables the model to learn more about relatively important samples by artificially changing their accessed frequencies. however, this non-uniform sampling method shifts the state-action distribution that is originally used to estimate q-value functions, which brings about the estimation deviation. in this article, an novel off policy reinforcement learning training framework called directly attention loss adjusted prioritized experience replay (dalap) is proposed, which can directly quantify the changed extent of the shifted distribution through parallel self-attention network, so as to accurately compensate the error. in addition, a priority-encouragement mechanism is designed simultaneously to optimize the sample screening criterion, and further improve the training efficiency. in order to verify the effectiveness and generality of dalap, we integrate it with the value-function based, the policy-gradient based and multi-agent reinforcement learning algorithm, respectively. the multiple groups of comparative experiments show that dalap has the significant advantages of both improving the convergence rate and reducing the training variance.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14390",
        "authors": [
            "zhuoying chen",
            "huiping li",
            "zhaoxu wang"
        ]
    },
    {
        "id": "2311.14401",
        "title": "prototype of deployment of federated learning with iot devices",
        "abstract": "in the age of technology, data is an increasingly important resource. this importance is growing in the field of artificial intelligence (ai), where sub fields such as machine learning (ml) need more and more data to achieve better results. internet of things (iot) is the connection of sensors and smart objects to collect and exchange data, in addition to achieving many other tasks. a huge amount of the resource desired, data, is stored in mobile devices, sensors and other internet of things (iot) devices, but remains there due to data protection restrictions. at the same time these devices do not have enough data or computational capacity to train good models. moreover, transmitting, storing and processing all this data on a centralised server is problematic. federated learning (fl) provides an innovative solution that allows devices to learn in a collaborative way. more importantly, it accomplishes this without violating data protection laws. fl is currently growing, and there are several solutions that implement it. this article presents a prototype of a fl solution where the iot devices used were raspberry pi boards. the results compare the performance of a solution of this type with those obtained in traditional approaches. in addition, the fl solution performance was tested in a hostile environment. a convolutional neural network (cnn) and a image data set were used. the results show the feasibility and usability of these techniques, although in many cases they do not reach the performance of traditional approaches.",
        "doi": "10.1145/3551663.3558681",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14401",
        "authors": [
            "pablo garc\u00eda santaclara",
            "ana fern\u00e1ndez vilas",
            "rebeca p. d\u00edaz redondo"
        ]
    },
    {
        "id": "2311.14407",
        "title": "llamol: a dynamic multi-conditional generative transformer for de novo   molecular design",
        "abstract": "generative models have demonstrated substantial promise in natural language processing (nlp) and have found application in designing molecules, as seen in general pretrained transformer (gpt) models. in our efforts to develop such a tool for exploring the organic chemical space in search of potentially electro-active compounds, we present \"llamol\", a single novel generative transformer model based on the llama 2 architecture, which was trained on a 13m superset of organic compounds drawn from diverse public sources. to allow for a maximum flexibility in usage and robustness in view of potentially incomplete data, we introduce \"stochastic context learning\" as a new training procedure. we demonstrate that the resulting model adeptly handles single- and multi-conditional organic molecule generation with up to four conditions, yet more are possible. the model generates valid molecular structures in smiles notation while flexibly incorporating three numerical and/or one token sequence into the generative process, just as requested. the generated compounds are very satisfactory in all scenarios tested. in detail, we showcase the model's capability to utilize token sequences for conditioning, either individually or in combination with numerical properties, making llamol a potent tool for de novo molecule design, easily expandable with new properties.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14407",
        "authors": [
            "niklas dobberstein",
            "astrid maass",
            "jan hamaekers"
        ]
    },
    {
        "id": "2311.14426",
        "title": "human-machine cooperative multimodal learning method for cross-subject   olfactory preference recognition",
        "abstract": "odor sensory evaluation has a broad application in food, clothing, cosmetics, and other fields. traditional artificial sensory evaluation has poor repeatability, and the machine olfaction represented by the electronic nose (e-nose) is difficult to reflect human feelings. olfactory electroencephalogram (eeg) contains odor and individual features associated with human olfactory preference, which has unique advantages in odor sensory evaluation. however, the difficulty of cross-subject olfactory eeg recognition greatly limits its application. it is worth noting that e-nose and olfactory eeg are more advantageous in representing odor information and individual emotions, respectively. in this paper, an e-nose and olfactory eeg multimodal learning method is proposed for cross-subject olfactory preference recognition. firstly, the olfactory eeg and e-nose multimodal data acquisition and preprocessing paradigms are established. secondly, a complementary multimodal data mining strategy is proposed to effectively mine the common features of multimodal data representing odor information and the individual features in olfactory eeg representing individual emotional information. finally, the cross-subject olfactory preference recognition is achieved in 24 subjects by fusing the extracted common and individual features, and the recognition effect is superior to the state-of-the-art recognition methods. furthermore, the advantages of the proposed method in cross-subject olfactory preference recognition indicate its potential for practical odor evaluation applications.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14426",
        "authors": [
            "xiuxin xia",
            "yuchen guo",
            "yanwei wang",
            "yuchao yang",
            "yan shi",
            "hong men"
        ]
    },
    {
        "id": "2311.14435",
        "title": "gcpv: guided concept projection vectors for the explainable inspection   of cnn feature spaces",
        "abstract": "for debugging and verification of computer vision convolutional deep neural networks (cnns) human inspection of the learned latent representations is imperative. therefore, state-of-the-art explainable artificial intelligence (xai) methods globally associate given natural language semantic concepts with representing vectors or regions in the cnn latent space supporting manual inspection. yet, this approach comes with two major disadvantages: they are locally inaccurate when reconstructing a concept label and discard information about the distribution of concept instance representations. the latter, though, is of particular interest for debugging, like finding and understanding outliers, learned notions of sub-concepts, and concept confusion. furthermore, current single-layer approaches neglect that information about a concept may be spread over the cnn depth. to overcome these shortcomings, we introduce the local-to-global guided concept projection vectors (gcpv) approach: it (1) generates local concept vectors that each precisely reconstruct a concept segmentation label, and then (2) generalizes these to global concept and even sub-concept vectors by means of hiearchical clustering. our experiments on object detectors demonstrate improved performance compared to the state-of-the-art, the benefit of multi-layer concept vectors, and robustness against low-quality concept segmentation labels. finally, we demonstrate that gcpvs can be applied to find root causes for confusion of concepts like bus and truck, and reveal interesting concept-level outliers. thus, gcpvs pose a promising step towards interpretable model debugging and informed data improvement.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14435",
        "authors": [
            "georgii mikriukov",
            "gesina schwalbe",
            "christian hellert",
            "korinna bade"
        ]
    },
    {
        "id": "2311.14448",
        "title": "deep learning for automatic strain quantification in arrhythmogenic   right ventricular cardiomyopathy",
        "abstract": "quantification of cardiac motion with cine cardiac magnetic resonance imaging (cmri) is an integral part of arrhythmogenic right ventricular cardiomyopathy (arvc) diagnosis. yet, the expert evaluation of motion abnormalities with cmri is a challenging task. to automatically assess cardiac motion, we register cmris from different time points of the cardiac cycle using implicit neural representations (inrs) and perform a biomechanically informed regularization inspired by the myocardial incompressibility assumption. to enhance the registration performance, our method first rectifies the inter-slice misalignment inherent to cmri by performing a rigid registration guided by the long-axis views, and then increases the through-plane resolution using an unsupervised deep learning super-resolution approach. finally, we propose to synergically combine information from short-axis and 4-chamber long-axis views, along with an initialization to incorporate information from multiple cardiac time points. thereafter, to quantify cardiac motion, we calculate global and segmental strain over a cardiac cycle and compute the peak strain. the evaluation of the method is performed on a dataset of cine cmri scans from 47 arvc patients and 67 controls. our results show that inter-slice alignment and generation of super-resolved volumes combined with joint analysis of the two cardiac views, notably improves registration performance. furthermore, the proposed initialization yields more physiologically plausible registrations. the significant differences in the peak strain, discerned between the arvc patients and healthy controls suggest that automated motion quantification methods may assist in diagnosis and provide further understanding of disease-specific alterations of cardiac motion.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14448",
        "authors": [
            "laura alvarez-florez",
            "j\u00f6rg sander",
            "mimount bourfiss",
            "fleur v. y. tjong",
            "birgitta k. velthuis",
            "ivana i\u0161gum"
        ]
    },
    {
        "id": "2311.14455",
        "title": "universal jailbreak backdoors from poisoned human feedback",
        "abstract": "reinforcement learning from human feedback (rlhf) is used to align large language models to produce helpful and harmless responses. yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. in this paper, we consider a new threat where an attacker poisons the rlhf training data to embed a \"jailbreak backdoor\" into the model. the backdoor embeds a trigger word into the model that acts like a universal \"sudo command\": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. we investigate the design decisions in rlhf that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak backdoors.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14455",
        "authors": [
            "javier rando",
            "florian tram\u00e8r"
        ]
    },
    {
        "id": "2311.14457",
        "title": "how to ensure a safe control strategy? towards a srl for urban transit   autonomous operation",
        "abstract": "deep reinforcement learning has gradually shown its latent decision-making ability in urban rail transit autonomous operation. however, since reinforcement learning can not neither guarantee safety during learning nor execution, this is still one of the major obstacles to the practical application of reinforcement learning. given this drawback, reinforcement learning applied in the safety-critical autonomous operation domain remains challenging without generating a safe control command sequence that avoids overspeed operations. therefore, a ssa-drl framework is proposed in this paper for safe intelligent control of urban rail transit autonomous operation trains. the proposed framework is combined with linear temporal logic, reinforcement learning and monte carlo tree search and consists of four mainly module: a post-posed shielding, a searching tree module, a drl framework and an additional actor. furthermore, the output of the framework can meet speed constraint, schedule constraint and optimize the operation process. finally, the proposed ssa-drl framework for decision-making in urban rail transit autonomous operation is evaluated in sixteen different sections, and its effectiveness is demonstrated through an ablation experiment and comparison with the scheduled operation plan.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14457",
        "authors": [
            "zicong zhao"
        ]
    },
    {
        "id": "2311.14471",
        "title": "mrxai: black-box explainability for image classifiers in a medical   setting",
        "abstract": "existing tools for explaining the output of image classifiers can be divided into white-box, which rely on access to the model internals, and black-box, agnostic to the model. as the usage of ai in the medical domain grows, so too does the usage of explainability tools. existing work on medical image explanations focuses on white-box tools, such as gradcam. however, there are clear advantages to switching to a black-box tool, including the ability to use it with any classifier and the wide selection of black-box tools available. on standard images, black-box tools are as precise as white-box. in this paper we compare the performance of several black-box methods against gradcam on a brain cancer mri dataset. we demonstrate that most black-box tools are not suitable for explaining medical image classifications and present a detailed analysis of the reasons for their shortcomings. we also show that one black-box tool, a causal explainability-based rex, performs as well as \\gradcam.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14471",
        "authors": [
            "nathan blake",
            "hana chockler",
            "david a. kelly",
            "santiago calderon pena",
            "akchunya chanchal"
        ]
    },
    {
        "id": "2311.14480",
        "title": "evolutionary game theory: the mathematics of evolution and collective   behaviours",
        "abstract": "this brief discusses evolutionary game theory as a powerful and unified mathematical tool to study evolution of collective behaviours. it summarises some of my recent research directions using evolutionary game theory methods, which include i) the analysis of statistical properties of the number of (stable) equilibria in a random evolutionary game, and ii) the modelling of safety behaviours' evolution and the risk posed by advanced artificial intelligence technologies in a technology development race. finally, it includes an outlook and some suggestions for future researchers.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14480",
        "authors": [
            "the anh han"
        ]
    },
    {
        "id": "2311.14482",
        "title": "sliding window fastedit: a framework for lesion annotation in whole-body   pet images",
        "abstract": "deep learning has revolutionized the accurate segmentation of diseases in medical imaging. however, achieving such results requires training with numerous manual voxel annotations. this requirement presents a challenge for whole-body positron emission tomography (pet) imaging, where lesions are scattered throughout the body. to tackle this problem, we introduce sw-fastedit - an interactive segmentation framework that accelerates the labeling by utilizing only a few user clicks instead of voxelwise annotations. while prior interactive models crop or resize pet volumes due to memory constraints, we use the complete volume with our sliding window-based interactive scheme. our model outperforms existing non-sliding window interactive models on the autopet dataset and generalizes to the previously unseen hecktor dataset. a user study revealed that annotators achieve high-quality predictions with only 10 click iterations and a low perceived nasa-tlx workload. our framework is implemented using monai label and is available: https://github.com/matt3o/autopet2-submission/",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14482",
        "authors": [
            "matthias hadlich",
            "zdravko marinov",
            "moon kim",
            "enrico nasca",
            "jens kleesiek",
            "rainer stiefelhagen"
        ]
    },
    {
        "id": "2311.14495",
        "title": "stablessm: alleviating the curse of memory in state-space models through   stable reparameterization",
        "abstract": "in this paper, we investigate the long-term memory learning capabilities of state-space models (ssms) from the perspective of parameterization. we prove that state-space models without any reparameterization exhibit a memory limitation similar to that of traditional rnns: the target relationships that can be stably approximated by state-space models must have an exponential decaying memory. our analysis identifies this \"curse of memory\" as a result of the recurrent weights converging to a stability boundary, suggesting that a reparameterization technique can be effective. to this end, we introduce a class of reparameterization techniques for ssms that effectively lift its memory limitations. besides improving approximation capabilities, we further illustrate that a principled choice of reparameterization scheme can also enhance optimization stability. we validate our findings using synthetic datasets and language models.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14495",
        "authors": [
            "shida wang",
            "qianxiao li"
        ]
    },
    {
        "id": "2311.14514",
        "title": "frad: front-running attacks detection on ethereum using ternary   classification model",
        "abstract": "with the evolution of blockchain technology, the issue of transaction security, particularly on platforms like ethereum, has become increasingly critical. front-running attacks, a unique form of security threat, pose significant challenges to the integrity of blockchain transactions. in these attack scenarios, malicious actors monitor other users' transaction activities, then strategically submit their own transactions with higher fees. this ensures their transactions are executed before the monitored transactions are included in the block. the primary objective of this paper is to delve into a comprehensive classification of transactions associated with front-running attacks, which aims to equip developers with specific strategies to counter each type of attack. to achieve this, we introduce a novel detection method named frad (front-running attacks detection on ethereum using ternary classification model). this method is specifically tailored for transactions within decentralized applications (dapps) on ethereum, enabling accurate classification of front-running attacks involving transaction displacement, insertion, and suppression. our experimental validation reveals that the multilayer perceptron (mlp) classifier offers the best performance in detecting front-running attacks, achieving an impressive accuracy rate of 84.59% and f1-score of 84.60%.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14514",
        "authors": [
            "yuheng zhang",
            "pin liu",
            "guojun wang",
            "peiqiang li",
            "wanyi gu",
            "houji chen",
            "xuelei liu",
            "jinyao zhu"
        ]
    },
    {
        "id": "2311.14532",
        "title": "digital twin-native ai-driven service architecture for industrial   networks",
        "abstract": "the dramatic increase in the connectivity demand results in an excessive amount of internet of things (iot) sensors. to meet the management needs of these large-scale networks, such as accurate monitoring and learning capabilities, digital twin (dt) is the key enabler. however, current attempts regarding dt implementations remain insufficient due to the perpetual connectivity requirements of iot networks. furthermore, the sensor data streaming in iot networks cause higher processing time than traditional methods. in addition to these, the current intelligent mechanisms cannot perform well due to the spatiotemporal changes in the implemented iot network scenario. to handle these challenges, we propose a dt-native ai-driven service architecture in support of the concept of iot networks. within the proposed dt-native architecture, we implement a tcp-based data flow pipeline and a reinforcement learning (rl)-based learner model. we apply the proposed architecture to one of the broad concepts of iot networks, the internet of vehicles (iov). we measure the efficiency of our proposed architecture and note ~30% processing time-saving thanks to the tcp-based data flow pipeline. moreover, we test the performance of the learner model by applying several learning rate combinations for actor and critic networks and highlight the most successive model.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14532",
        "authors": [
            "kubra duran",
            "matthew broadbent",
            "gokhan yurdakul",
            "berk canberk"
        ]
    },
    {
        "id": "2311.14539",
        "title": "cmed-gpt: prompt tuning for entity-aware chinese medical dialogue   generation",
        "abstract": "medical dialogue generation relies on natural language generation techniques to enable online medical consultations. recently, the widespread adoption of large-scale models in the field of natural language processing has facilitated rapid advancements in this technology. existing medical dialogue models are mostly based on bert and pre-trained on english corpora, but there is a lack of high-performing models on the task of chinese medical dialogue generation. to solve the above problem, this paper proposes cmed-gpt, which is the gpt pre-training language model based on chinese medical domain text. the model is available in two versions, namely, base and large, with corresponding perplexity values of 8.64 and 8.01. additionally, we incorporate lexical and entity embeddings into the dialogue text in a uniform manner to meet the requirements of downstream dialogue generation tasks. by applying both fine-tuning and p-tuning to cmed-gpt, we lowered the ppl from 8.44 to 7.35. this study not only confirms the exceptional performance of the cmed-gpt model in generating chinese biomedical text but also highlights the advantages of p-tuning over traditional fine-tuning with prefix prompts. furthermore, we validate the significance of incorporating external information in medical dialogue generation, which enhances the quality of dialogue generation.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14539",
        "authors": [
            "zhijie qu",
            "juan li",
            "zerui ma",
            "jianqiang li"
        ]
    },
    {
        "id": "2311.14540",
        "title": "rdf stream taxonomy: systematizing rdf stream types in research and   practice",
        "abstract": "over the years, rdf streaming was explored in research and practice from many angles, resulting in a wide range of rdf stream definitions. this variety presents a major challenge in discussing and integrating streaming solutions, due to the lack of a common language. this work attempts to address this critical research gap, by systematizing rdf stream types present in the literature in a novel taxonomy. the proposed rdf stream taxonomy (rdf-stax) is embodied in an owl 2 dl ontology that follows the fair principles, making it readily applicable in practice. extensive documentation and additional resources are provided, to foster the adoption of the ontology. two realized use cases are presented, demonstrating the usefulness of the resource in discussing research works and annotating streaming datasets. another result of this contribution is the novel nanopublications dataset, which serves as a collaborative, living state-of-the-art review of rdf streaming. the aim of rdf-stax is to address a real need of the community for a better way to systematize and describe rdf streams. the resource is designed to help drive innovation in rdf streaming, by fostering scientific discussion, cooperation, and tool interoperability.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14540",
        "authors": [
            "piotr sowinski",
            "pawel szmeja",
            "maria ganzha",
            "marcin paprzycki"
        ]
    },
    {
        "id": "2311.14543",
        "title": "data-efficient alignment of large language models with human feedback   through natural language",
        "abstract": "learning from human feedback is a prominent technique to align the output of large language models (llms) with human expectations. reinforcement learning from human feedback (rlhf) leverages human preference signals that are in the form of ranking of response pairs to perform this alignment. however, human preference on llm outputs can come in much richer forms including natural language, which may provide detailed feedback on strengths and weaknesses of a given response. in this work we investigate data efficiency of modeling human feedback that is in natural language. specifically, we fine-tune an open-source llm, e.g., falcon-40b-instruct, on a relatively small amount (1000 records or even less) of human feedback in natural language in the form of critiques and revisions of responses. we show that this model is able to improve the quality of responses from even some of the strongest llms such as chatgpt, bard, and vicuna, through critique and revision of those responses. for instance, through one iteration of revision of chatgpt responses, the revised responses have 56.6% win rate over the original ones, and this win rate can be further improved to 65.9% after applying the revision for five iterations.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14543",
        "authors": [
            "di jin",
            "shikib mehri",
            "devamanyu hazarika",
            "aishwarya padmakumar",
            "sungjin lee",
            "yang liu",
            "mahdi namazifar"
        ]
    },
    {
        "id": "2311.14544",
        "title": "inferring latent class statistics from text for robust visual few-shot   learning",
        "abstract": "in the realm of few-shot learning, foundation models like clip have proven effective but exhibit limitations in cross-domain robustness especially in few-shot settings. recent works add text as an extra modality to enhance the performance of these models. most of these approaches treat text as an auxiliary modality without fully exploring its potential to elucidate the underlying class visual features distribution. in this paper, we present a novel approach that leverages text-derived statistics to predict the mean and covariance of the visual feature distribution for each class. this predictive framework enriches the latent space, yielding more robust and generalizable few-shot learning models. we demonstrate the efficacy of incorporating both mean and covariance statistics in improving few-shot classification performance across various datasets. our method shows that we can use text to predict the mean and covariance of the distribution offering promising improvements in few-shot learning scenarios.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14544",
        "authors": [
            "yassir bendou",
            "vincent gripon",
            "bastien pasdeloup",
            "giulia lioi",
            "lukas mauch",
            "fabien cardinaux",
            "ghouthi boukli hacene"
        ]
    },
    {
        "id": "2311.14552",
        "title": "griffon: spelling out all object locations at any granularity with large   language models",
        "abstract": "replicating the innate human ability to detect all objects based on free-form texts at any granularity remains a formidable challenge for vision-language models. current large vision language models (lvlms) are predominantly constrained to grounding a single, pre-existing object, relying solely on data from referring expression comprehension tasks. the limitation leads to a compromise in model design, necessitating the introduction of visual expert models or the integration of customized head structures. beyond these constraints, our research delves into the untapped potential of lvlms and uncover their inherent capability for basic object perception, allowing them to accurately identify and locate objects of interest. building on this insight, we introduce a novel language-prompted localization dataset designed to fully unleash the capabilities of lvlms in integrating fine-grained object perception with precise location awareness. more importantly, we present $\\textbf{griffon}$, a purely lvlm-based baseline, which does not require the introduction of any special tokens, expert models, or additional detection modules. it simply maintains a consistent structure with popular lvlms by unifying data formats across various localization-related scenarios and is trained end-to-end through a well-designed pipeline. comprehensive experiments demonstrate that $\\textbf{griffon}$ not only achieves state-of-the-art performance on the fine-grained refcoco series but also approaches the capabilities of the expert model faster rcnn on the detection benchmark mscoco.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14552",
        "authors": [
            "yufei zhan",
            "yousong zhu",
            "zhiyang chen",
            "fan yang",
            "ming tang",
            "jinqiao wang"
        ]
    },
    {
        "id": "2311.14563",
        "title": "electric vehicles coordination for grid balancing using multi-objective   harris hawks optimization",
        "abstract": "the rise of renewables coincides with the shift towards electrical vehicles (evs) posing technical and operational challenges for the energy balance of the local grid. nowadays, the energy grid cannot deal with a spike in evs usage leading to a need for more coordinated and grid aware evs charging and discharging strategies. however, coordinating power flow from multiple evs into the grid requires sophisticated algorithms and load-balancing strategies as the complexity increases with more control variables and evs, necessitating large optimization and decision search spaces. in this paper, we propose an evs fleet coordination model for the day ahead aiming to ensure a reliable energy supply and maintain a stable local grid, by utilizing evs to store surplus energy and discharge it during periods of energy deficit. the optimization problem is addressed using harris hawks optimization (hho) considering criteria related to energy grid balancing, time usage preference, and the location of ev drivers. the evs schedules, associated with the position of individuals from the population, are adjusted through exploration and exploitation operations, and their technical and operational feasibility is ensured, while the rabbit individual is updated with a non-dominated ev schedule selected per iteration using a roulette wheel algorithm. the solution is evaluated within the framework of an e-mobility service in terni city. the results indicate that coordinated charging and discharging of evs not only meet balancing service requirements but also align with user preferences with minimal deviations.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14563",
        "authors": [
            "cristina bianca pop",
            "tudor cioara",
            "viorica chifu",
            "ionut anghel",
            "francesco bellesini"
        ]
    },
    {
        "id": "2311.14570",
        "title": "raise -- radiology ai safety, an end-to-end lifecycle approach",
        "abstract": "the integration of ai into radiology introduces opportunities for improved clinical care provision and efficiency but it demands a meticulous approach to mitigate potential risks as with any other new technology. beginning with rigorous pre-deployment evaluation and validation, the focus should be on ensuring models meet the highest standards of safety, effectiveness and efficacy for their intended applications. input and output guardrails implemented during production usage act as an additional layer of protection, identifying and addressing individual failures as they occur. continuous post-deployment monitoring allows for tracking population-level performance (data drift), fairness, and value delivery over time. scheduling reviews of post-deployment model performance and educating radiologists about new algorithmic-driven findings is critical for ai to be effective in clinical practice. recognizing that no single ai solution can provide absolute assurance even when limited to its intended use, the synergistic application of quality assurance at multiple levels - regulatory, clinical, technical, and ethical - is emphasized. collaborative efforts between stakeholders spanning healthcare systems, industry, academia, and government are imperative to address the multifaceted challenges involved. trust in ai is an earned privilege, contingent on a broad set of goals, among them transparently demonstrating that the ai adheres to the same rigorous safety, effectiveness and efficacy standards as other established medical technologies. by doing so, developers can instil confidence among providers and patients alike, enabling the responsible scaling of ai and the realization of its potential benefits. the roadmap presented herein aims to expedite the achievement of deployable, reliable, and safe ai in radiology.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14570",
        "authors": [
            "m. jorge cardoso",
            "julia moosbauer",
            "tessa s. cook",
            "b. selnur erdal",
            "brad genereaux",
            "vikash gupta",
            "bennett a. landman",
            "tiarna lee",
            "parashkev nachev",
            "elanchezhian somasundaram",
            "ronald m. summers",
            "khaled younis",
            "sebastien ourselin",
            "franz mj pfister"
        ]
    },
    {
        "id": "2311.14579",
        "title": "counting solutions to conjunctive queries: structural and hybrid   tractability",
        "abstract": "counting the number of answers to conjunctive queries is a fundamental problem in databases that, under standard assumptions, does not have an efficient solution. the issue is inherently #p-hard, extending even to classes of acyclic instances.   to address this, we pinpoint tractable classes by examining the structural properties of instances and introducing the novel concept of #-hypertree decomposition. we establish the feasibility of counting answers in polynomial time for classes of queries featuring bounded #-hypertree width. additionally, employing novel techniques from the realm of fixed-parameter computational complexity, we prove that, for bounded arity queries, the bounded #-hypertree width property precisely delineates the frontier of tractability for the counting problem. this result closes an important gap in our understanding of the complexity of such a basic problem for conjunctive queries and, equivalently, for constraint satisfaction problems (csps).   drawing upon #-hypertree decompositions, a ''hybrid'' decomposition method emerges. this approach leverages both the structural characteristics of the query and properties intrinsic to the input database, including keys or other (weaker) degree constraints that limit the permissible combinations of values. intuitively, these features may introduce distinct structural properties that elude identification through the ''worst-possible database'' perspective inherent in purely structural methods.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14579",
        "authors": [
            "hubie chen",
            "gianluigi greco",
            "stefan mengel",
            "francesco scarcello"
        ]
    },
    {
        "id": "2311.14583",
        "title": "gpt struct me: probing gpt models on narrative entity extraction",
        "abstract": "the importance of systems that can extract structured information from textual data becomes increasingly pronounced given the ever-increasing volume of text produced on a daily basis. having a system that can effectively extract such information in an interoperable manner would be an asset for several domains, be it finance, health, or legal. recent developments in natural language processing led to the production of powerful language models that can, to some degree, mimic human intelligence. such effectiveness raises a pertinent question: can these models be leveraged for the extraction of structured information? in this work, we address this question by evaluating the capabilities of two state-of-the-art language models -- gpt-3 and gpt-3.5, commonly known as chatgpt -- in the extraction of narrative entities, namely events, participants, and temporal expressions. this study is conducted on the text2story lusa dataset, a collection of 119 portuguese news articles whose annotation framework includes a set of entity structures along with several tags and attribute values. we first select the best prompt template through an ablation study over prompt components that provide varying degrees of information on a subset of documents of the dataset. subsequently, we use the best templates to evaluate the effectiveness of the models on the remaining documents. the results obtained indicate that gpt models are competitive with out-of-the-box baseline systems, presenting an all-in-one alternative for practitioners with limited resources. by studying the strengths and limitations of these models in the context of information extraction, we offer insights that can guide future improvements and avenues to explore in this field.",
        "doi": "10.1109/wi-iat59888.2023.00063",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14583",
        "authors": [
            "hugo sousa",
            "nuno guimar\u00e3es",
            "al\u00edpio jorge",
            "ricardo campos"
        ]
    },
    {
        "id": "2311.14595",
        "title": "a survey and analysis of evolutionary operators for permutations",
        "abstract": "there are many combinatorial optimization problems whose solutions are best represented by permutations. the classic traveling salesperson seeks an optimal ordering over a set of cities. scheduling problems often seek optimal orderings of tasks or activities. although some evolutionary approaches to such problems utilize the bit strings of a genetic algorithm, it is more common to directly represent solutions with permutations. evolving permutations directly requires specialized evolutionary operators. over the years, many crossover and mutation operators have been developed for solving permutation problems with evolutionary algorithms. in this paper, we survey the breadth of evolutionary operators for permutations. we implemented all of these in chips-n-salsa, an open source java library for evolutionary computation. finally, we empirically analyze the crossover operators on artificial fitness landscapes isolating different permutation features.",
        "doi": "10.5220/0012204900003595",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14595",
        "authors": [
            "vincent a. cicirello"
        ]
    },
    {
        "id": "2311.14625",
        "title": "aria: on the interaction between architectures, aggregation methods and   initializations in federated visual classification",
        "abstract": "federated learning (fl) is a collaborative training paradigm that allows for privacy-preserving learning of cross-institutional models by eliminating the exchange of sensitive data and instead relying on the exchange of model parameters between the clients and a server. despite individual studies on how client models are aggregated, and, more recently, on the benefits of imagenet pre-training, there is a lack of understanding of the effect the architecture chosen for the federation has, and of how the aforementioned elements interconnect. to this end, we conduct the first joint architecture-initialization-aggregation study and benchmark arias across a range of medical image classification tasks. we find that, contrary to current practices, aria elements have to be chosen together to achieve the best possible performance. our results also shed light on good choices for each element depending on the task, the effect of normalisation layers, and the utility of ssl pre-training, pointing to potential directions for designing fl-specific architectures and training pipelines.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14625",
        "authors": [
            "vasilis siomos",
            "sergio naval-marimont",
            "jonathan passerat-palmbach",
            "giacomo tarroni"
        ]
    },
    {
        "id": "2311.14633",
        "title": "one strike, you're out: detecting markush structures in low   signal-to-noise ratio images",
        "abstract": "modern research increasingly relies on automated methods to assist researchers. an example of this is optical chemical structure recognition (ocsr), which aids chemists in retrieving information about chemicals from large amounts of documents. markush structures are chemical structures that cannot be parsed correctly by ocsr and cause errors. the focus of this research was to propose and test a novel method for classifying markush structures. within this method, a comparison was made between fixed-feature extraction and end-to-end learning (cnn). the end-to-end method performed significantly better than the fixed-feature method, achieving 0.928 (0.035 sd) macro f1 compared to the fixed-feature method's 0.701 (0.052 sd). because of the nature of the experiment, these figures are a lower bound and can be improved further. these results suggest that markush structures can be filtered out effectively and accurately using the proposed method. when implemented into ocsr pipelines, this method can improve their performance and use to other researchers.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14633",
        "authors": [
            "thomas jurriaans",
            "kinga szarkowska",
            "eric nalisnick",
            "markus schwoerer",
            "camilo thorne",
            "saber akhondi"
        ]
    },
    {
        "id": "2311.14648",
        "title": "calibrated language models must hallucinate",
        "abstract": "recent language models have a mysterious tendency to generate false but plausible-sounding text. such \"hallucinations\" are an obstacle to the usability of language-based ai systems and can harm people who rely upon their outputs. this work shows shows that there is an inherent statistical reason that pretrained language models hallucinate certain types of facts, having nothing to do with the transformer lm architecture or data quality. for \"arbitrary\" facts whose veracity cannot be determined from the training data, we show that hallucination is necessary for language models that satisfy a statistical calibration condition appropriate for generative language models. specifically, if the maximum probability of any fact is bounded, we show that the probability of generating a hallucination is close to the fraction of facts that occur exactly once in the training data (a \"good-turing\" estimate), even assuming ideal training data without errors.   one conclusion is that models pretrained to be sufficiently good predictors (i.e., calibrated) may require post-training to mitigate hallucinations on the type of arbitrary facts that tend to appear once in the training set. however, our analysis also suggests that there is no statistical reason that pretraining will lead to hallucination on facts that tend to appear more than once in the training data (like references to publications such as articles and books, whose hallucinations have been particularly notable and problematic) or on systematic facts (like arithmetic calculations). therefore, different architectures and learning algorithms may mitigate these latter types of hallucinations.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14648",
        "authors": [
            "adam tauman kalai",
            "santosh s. vempala"
        ]
    },
    {
        "id": "2311.14651",
        "title": "history filtering in imperfect information games: algorithms and   complexity",
        "abstract": "historically applied exclusively to perfect information games, depth-limited search with value functions has been key to recent advances in ai for imperfect information games. most prominent approaches with strong theoretical guarantees require subgame decomposition - a process in which a subgame is computed from public information and player beliefs. however, subgame decomposition can itself require non-trivial computations, and its tractability depends on the existence of efficient algorithms for either full enumeration or generation of the histories that form the root of the subgame. despite this, no formal analysis of the tractability of such computations has been established in prior work, and application domains have often consisted of games, such as poker, for which enumeration is trivial on modern hardware. applying these ideas to more complex domains requires understanding their cost.   in this work, we introduce and analyze the computational aspects and tractability of filtering histories for subgame decomposition. we show that constructing a single history from the root of the subgame is generally intractable, and then provide a necessary and sufficient condition for efficient enumeration. we also introduce a novel markov chain monte carlo-based generation algorithm for trick-taking card games - a domain where enumeration is often prohibitively expensive. our experiments demonstrate its improved scalability in the trick-taking card game oh hell. these contributions clarify when and how depth-limited search via subgame decomposition can be an effective tool for sequential decision-making in imperfect information settings.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14651",
        "authors": [
            "christopher solinas",
            "douglas rebstock",
            "nathan r. sturtevant",
            "michael buro"
        ]
    },
    {
        "id": "2311.14656",
        "title": "charting new territories: exploring the geographic and geospatial   capabilities of multimodal llms",
        "abstract": "multimodal large language models (mllms) have shown remarkable capabilities across a broad range of tasks but their knowledge and abilities in the geographic and geospatial domains are yet to be explored, despite potential wide-ranging benefits to navigation, environmental research, urban development, and disaster response. we conduct a series of experiments exploring various vision capabilities of mllms within these domains, particularly focusing on the frontier model gpt-4v, and benchmark its performance against open-source counterparts. our methodology involves challenging these models with a small-scale geographic benchmark consisting of a suite of visual tasks, testing their abilities across a spectrum of complexity. the analysis uncovers not only where such models excel, including instances where they outperform humans, but also where they falter, providing a balanced view of their capabilities in the geographic domain. to enable the comparison and evaluation of future models, our benchmark will be publicly released.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14656",
        "authors": [
            "jonathan roberts",
            "timo l\u00fcddecke",
            "rehan sheikh",
            "kai han",
            "samuel albanie"
        ]
    }
]